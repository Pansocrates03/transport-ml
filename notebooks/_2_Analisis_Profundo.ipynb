{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7112ef9",
   "metadata": {},
   "source": [
    "# Entrega 2: An√°lisis Profundo seg√∫n Enfoque Elegido\n",
    "- A00836286 | Esteban Sierra Baccio\n",
    "- A00837527 | Diego de Jes√∫s Esparza Ru√≠z\n",
    "- A01722667 | Javier Jorge Hern√°ndez Verduzco\n",
    "- A01285193 | Sergio Omar Flores Garc√≠a\n",
    "- A01613878 | Sergio Aar√≥n Hern√°ndez Orta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b740eff",
   "metadata": {},
   "source": [
    "## 1. Refinamiento y Operacionalizaci√≥n\n",
    "\n",
    "\n",
    "### 1.1 Ajustes a las preguntas de investigaci√≥n basados en feedback de Entrega 1\n",
    "\n",
    "Tras el feedback, mantuvimos la estructura y el sentido original de las preguntas pero las ajustamos para mayor precisi√≥n operativa:\n",
    "\n",
    "- Se aclar√≥ que *‚Äúexcluir las que describen la carga‚Äù* en la Pregunta 1 significa dejar fuera variable(s) directas de magnitud (Peso total, Volumen, #Remitos) pero mantener transformaciones normalizadas (p. ej. Costo por tonelada) como variables de control.  \n",
    "- En la Pregunta 2 (estacionalidad) se especific√≥ el nivel temporal a analizar: **d√≠a de la semana**, **mes** y **semana m√≥vil**.  \n",
    "- En la Pregunta 3 (ineficiencias por transportadora/ruta) se a√±adi√≥ un umbral operativo: **rutas con al menos 3 transportistas y ‚â• 30 observaciones** para garantizar comparaciones significativas.  \n",
    "- En la Pregunta 4 se defini√≥ qu√© fuentes externas se considerar√°n si est√°n disponibles (precio del diesel mensual, tipo de cambio diario y eventos clim√°ticos puntuales) y c√≥mo se integrar√°n (join por fecha/mes).  \n",
    "- En la Pregunta 5 se estableci√≥ la m√©trica exacta de desviaci√≥n: **Desviaci√≥n relativa = (Costo real ‚àí Costo √≥ptimo simulado) / Costo √≥ptimo simulado**, y se defini√≥ que la proporci√≥n de varianza atribuible a estas desviaciones ser√° la **varianza(Desviaci√≥n) / varianza(Costo real)**.\n",
    "\n",
    "Estos ajustes responden a comentarios sobre reproducibilidad y claridad en las m√©tricas a reportar.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Definici√≥n operacional precisa de cada variable y constructo\n",
    "\n",
    "Se listan las variables principales (observadas y derivadas) con su definici√≥n, tipo y tratamiento inicial.\n",
    "\n",
    "#### Variables dependientes\n",
    "- **Costo (MXN)** ‚Äî *num√©rica continua*. Costo total registrado por viaje en pesos mexicanos. Uso directo; revisar outliers (valores ‚â´ P99) y truncar/ winsorizar si procedente.\n",
    "- **CostoxTn** ‚Äî *num√©rica continua*. `Costo / (Peso total (kg) / 1000)`. Se usa como normalizaci√≥n por tonelada.\n",
    "\n",
    "#### Variables independientes (selecci√≥n)\n",
    "- **Transp.Leg / Nombre** ‚Äî *categ√≥rica nominal*. Identificador de transportista. Codificaci√≥n: dummies o efecto aleatorio seg√∫n especificaci√≥n.\n",
    "- **Ori-Dest** ‚Äî *categ√≥rica nominal*. Par origen-destino compreso como string; tambi√©n se desagrega en `Origen` y `Destino`.\n",
    "- **Tipo de planta / Tipo Planta** ‚Äî *categ√≥rica ordinal/nominal* (categor√≠as: masivos, revestidos, customizados). Dummies.\n",
    "- **F.Salida, H.Salida** ‚Äî *fecha / hora*. Derivados: `Mes`, `D√≠aSemana`, `HoraBloque` (p. ej. 0‚Äì6, 6‚Äì12, ...), `Temporada` (Q1..Q4).\n",
    "- **Peso total (kg)** ‚Äî *num√©rica continua*. Control de magnitud.\n",
    "- **Flete Falso (MXN)** ‚Äî *num√©rica continua*. Se tratar√° como imputable o indicador de correcci√≥n; posible outlier.\n",
    "\n",
    "#### Variables con tratamiento especial\n",
    "- **TpoVje** ‚Äî *categ√≥rica / temporal* (2,847 missings). Se crea una categor√≠a `Desconocido` y flag `TpoVje_missing = 1` para preservar informaci√≥n faltante.\n",
    "- **Costo Prom, Variaci√≥n** ‚Äî (1,121 missings). Se reconstruir√°n conceptualmente y se comparar√° con columna original; si no es recuperable, se imputar√° por mediana condicionada a `Ori-Dest` y `Transportista`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Justificaci√≥n de cualquier cambio en el enfoque metodol√≥gico\n",
    "\n",
    "El feedback confirm√≥ que el **enfoque primario econom√©trico** es apropiado. Confirmamos ese foco por las razones siguientes:\n",
    "\n",
    "- Los datos proporcionados son ricos en atributos internos (rutas, transportistas, fechas) pero carecen de cobertura exhaustiva de variables externas de mercado. Para **identificar y cuantificar efectos** controlando por covariables observables, modelos econom√©tricos (MLR con efectos fijos/mixtos) son m√°s interpretables y √∫tiles para la toma de decisiones empresariales inmediatas.\n",
    "- Mantendremos un componente de **optimizaci√≥n** secundario (simulaci√≥n de costo √≥ptimo) para convertir hallazgos en m√©tricas econ√≥micas accionables.\n",
    "- No se prioriz√≥ como m√©todo principal un pipeline de ML ‚Äúblack-box‚Äù porque, sin datos externos y con la necesidad de interpretabilidad para el socio, el valor adicional ser√≠a menor. No obstante, se evaluar√°n modelos de boosting y regularizados como comparaci√≥n predictiva y para identificar variables importantes.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Descripci√≥n detallada de c√≥mo construyeron variables derivadas o √≠ndices\n",
    "\n",
    "A continuaci√≥n se describen las variables derivadas y los √≠ndices que ya se construyeron o se planifican construir, con su **f√≥rmula**, **prop√≥sito** y **tratamiento de valores faltantes**.\n",
    "\n",
    "1. **Costo por tonelada (CostoxTn)**  \n",
    "   - **F√≥rmula:** `CostoxTn = Costo / (Peso total (kg) / 1000)`  \n",
    "   - **Prop√≥sito:** Normalizar el costo por tama√±o de la carga para comparar viajes heterog√©neos.  \n",
    "   - **Tratamiento missing/outliers:** Si `Peso total` = 0 o missing ‚Üí marcar `invalid_weight=1` y excluir de an√°lisis de CostoxTn; imputar peso por mediana de `Ori-Dest` si procedente.\n",
    "\n",
    "2. **√çndice de aprovechamiento de carga (LoadUtilIndex)**  \n",
    "   - **F√≥rmula conceptual:** `Peso total / (Peso total + Flete Falso (kg))` (bounded 0‚Äì1).  \n",
    "   - **Prop√≥sito:** Medir la eficiencia en el uso de capacidad del veh√≠culo; valores bajos indican subutilizaci√≥n.  \n",
    "   - **Tratamiento:** Si `Flete Falso (kg)` missing ‚Üí imputar 0 si no hay evidencia de aplicaci√≥n; crear flag `FF_missing`.\n",
    "\n",
    "3. **Desviaci√≥n de costo te√≥rico (CostDeviation)**  \n",
    "   - **F√≥rmula:** `Desviaci√≥n = (Costo real - Costo √≥ptimo simulado) / Costo √≥ptimo simulado`  \n",
    "   - **Prop√≥sito:** Cuantificar la prima pagada respecto a una asignaci√≥n hipot√©tica √≥ptima (usando la transportista m√°s barata observada bajo condiciones similares).  \n",
    "   - **Construcci√≥n:** Para cada viaje se simula el ‚ÄúCosto √≥ptimo‚Äù usando el modelo predictivo (controlando Ori-Dest, peso, tipo de servicio) o asignando el costo medio por `Ori-Dest` y `TipoServicio` del transportista m√°s barato.  \n",
    "   - **Tratamiento:** Si no hay comparables (pocas observaciones), excluir de la estad√≠stica de proporci√≥n global pero reportar casos.\n",
    "\n",
    "4. **Elasticidad aproximada del costo (CostElasticity_X)**  \n",
    "   - **F√≥rmula:** `(ŒîCosto / Costo) / (ŒîX / X)` para variables continuas X (ej. Peso).  \n",
    "   - **Prop√≥sito:** Medir sensibilidad porcentual del costo ante cambios unitarios en X. Se estimar√° a partir de coeficientes estandarizados de regresi√≥n log-log cuando aplique.  \n",
    "   - **Tratamiento:** Log-transformaciones s√≥lo cuando variable > 0; missing ‚Üí exclusi√≥n puntual.\n",
    "\n",
    "5. **√çndice de competencia por ruta (RouteCompetitionIndex)**  \n",
    "   - **F√≥rmula (operativa):** `1 ‚àí (Herfindahl-Hirschman Index de participaci√≥n por # viajes en la ruta)`  \n",
    "   - **Prop√≥sito:** Se√±alar rutas con alta concentraci√≥n de mercado (√≠ndice bajo = alta competencia).  \n",
    "   - **Construcci√≥n:** Para cada `Ori-Dest` calcular shares por transportista y derivar HHI.  \n",
    "   - **Tratamiento:** Rutas con < 3 transportistas o < 30 observaciones ‚Üí marcar `insufficient_competition_data`.\n",
    "\n",
    "6. **Flags y dummies temporales**  \n",
    "   - **Variables:** `Mes`, `D√≠aSemana`, `SemanaMovilAvgCost` (rolling 4 semanas).  \n",
    "   - **Prop√≥sito:** Capturar estacionalidad y estructura temporal no lineal.  \n",
    "   - **Tratamiento:** Missing en fecha ‚Üí excluir; `TpoVje_missing` creado como dummy para preservar se√±al.\n",
    "\n",
    "7. **Indicador de transportista caro (HighCostCarrierFlag)**  \n",
    "   - **F√≥rmula:** transportista cuyo costo promedio en una ruta > promedio ruta + 1.5 * SD.  \n",
    "   - **Prop√≥sito:** Detectar transportistas que consistentemente cobran primas.  \n",
    "   - **Uso:** Variable categ√≥rica en an√°lisis de ineficiencia; sensibilidad a outliers mitigada por usar medianas.\n",
    "\n",
    "8. **Variables de imputaci√≥n y flags de calidad**  \n",
    "   - Para variables con muchos missings (TpoVje, Costo Prom, Variaci√≥n) se crea un **flag** (`*_missing`) para cada variable imputada y se documenta la regla de imputaci√≥n (mediana por grupo, categor√≠a \"Desconocido\", o exclusi√≥n). Estos flags se usan en modelos para capturar informaci√≥n contenida en la ausencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a05c84",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Implementaci√≥n Metodol√≥gica Principal (5-7 p√°ginas + c√≥digo)\n",
    "\n",
    "El contenido espec√≠fico depender√° del enfoque elegido, pero TODOS deben incluir:\n",
    "\n",
    "### Secci√≥n A: Preparaci√≥n y Transformaci√≥n\n",
    "\n",
    "#### Proceso detallado de limpieza y transformaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"../data/raw/Viajes Sep-Dic 24 v2.xlsx\", sheet_name='Viajes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280d934",
   "metadata": {},
   "source": [
    "Feature: Distancia recorrida en KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "\n",
    "def calcular_distancia_recta(row):\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent=\"mi_app_geo\") # Define tu user_agent\n",
    "        \n",
    "        # Concatena tus campos para darle m√°s contexto a Nominatim\n",
    "        query_origen = f\"{row['Desc Org. Apt']}, {row['Origen']}\"\n",
    "        query_destino = f\"{row['Destino']}\"\n",
    "\n",
    "        # Geocodifica el origen\n",
    "        location_origen = geolocator.geocode(query_origen)\n",
    "        time.sleep(1) # IMPORTANTE: Nominatim requiere un delay de 1 seg por petici√≥n\n",
    "        \n",
    "        # Geocodifica el destino\n",
    "        location_destino = geolocator.geocode(query_destino)\n",
    "        time.sleep(1)\n",
    "\n",
    "        if location_origen and location_destino:\n",
    "            coords_origen = (location_origen.latitude, location_origen.longitude)\n",
    "            coords_destino = (location_destino.latitude, location_destino.longitude)\n",
    "            \n",
    "            # Calcula la distancia geod√©sica (l√≠nea recta)\n",
    "            distancia_km = geodesic(coords_origen, coords_destino).kilometers\n",
    "            print(f\"√âxito: {query_origen} -> {query_destino} = {distancia_km:.2f} km\")\n",
    "            return distancia_km\n",
    "        else:\n",
    "            print(f\"No se pudieron geocodificar: {query_origen} o {query_destino}\")\n",
    "            return np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en Nominatim: {e}\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a717477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos el DF para estandarizar nombres de Origen y Destino\n",
    "df['Desc Org. Apt'] = df['Origen'].replace('Churubusco', 'Churubusco Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('AM Monterrey Plantas', 'Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('AM Monterrey Local', 'Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('Area Metr Queretaro', 'Queretaro')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Saltillo', 'Saltillo')\n",
    "df['Destino'] = df['Destino'].replace('Area Metro Monclova', 'Monclova')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Monclova', 'Monclova')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro SLP', 'San Luis Potosi')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Puebla', 'Puebla')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Celaya', 'Celaya')\n",
    "df['Destino'] = df['Destino'].replace('A. M. Aguascalientes', 'Aguascalientes')\n",
    "\n",
    "# Creamos un dataframe con las combinaciones √∫nicas de Origen y Destino\n",
    "combinaciones = df[['Desc Org. Apt','Origen', 'Destino']].drop_duplicates()\n",
    "\n",
    "# Obtenemos la distancia entre cada par de Origen y Destino\n",
    "\n",
    "# Primero verificamos si el archivo ya existe para evitar recalcular\n",
    "import os\n",
    "if os.path.exists('../data/processed/combinaciones_origen_destino.csv'):\n",
    "    combinaciones = pd.read_csv('../data/processed/combinaciones_origen_destino.csv')\n",
    "else:\n",
    "    combinaciones['Distancia_km'] = combinaciones.apply(calcular_distancia_recta, axis=1)\n",
    "\n",
    "# A√±adimos la distancia al dataframe original\n",
    "df = df.merge(combinaciones[['Desc Org. Apt','Origen', 'Destino', 'Distancia_km']], on=['Desc Org. Apt','Origen', 'Destino'], how='left')\n",
    "\n",
    "# Guardamos las combinaciones en un archivo CSV\n",
    "combinaciones.to_csv('../data/processed/combinaciones_origen_destino.csv', index=False)\n",
    "\n",
    "#print(combinaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96362dab",
   "metadata": {},
   "source": [
    "Features: Tipo Planta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ef2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding para la columna 'Tipo Planta'\n",
    "tipo_planta_dummies = pd.get_dummies(df['Tipo Planta'], prefix='Tipo_Planta')\n",
    "df = pd.concat([df, tipo_planta_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('Tipo Planta', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9beba4",
   "metadata": {},
   "source": [
    "Features: Dep√≥sito Origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding para la columna 'Dep√≥sito Origen'\n",
    "deposito_origen_dummies = pd.get_dummies(df['Deposito Origen'], prefix='Deposito_Origen')\n",
    "df = pd.concat([df, deposito_origen_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('Deposito Origen', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf1a56",
   "metadata": {},
   "source": [
    "Features: Tipo de permiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding para la columna 'Tipo de permiso'\n",
    "tipo_permiso_dummies = pd.get_dummies(df['TpoPermiso'], prefix='Tipo_Permiso')\n",
    "df = pd.concat([df, tipo_permiso_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('TpoPermiso', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ad18b",
   "metadata": {},
   "source": [
    "Features: Tipo de Servicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92dbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding para la columna 'Tipo Servicio'\n",
    "tipo_servicio_dummies = pd.get_dummies(df['TpoSrv'], prefix='Tipo_Servicio')\n",
    "df = pd.concat([df, tipo_servicio_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('TpoSrv', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca311ab",
   "metadata": {},
   "source": [
    "Features: Tipo de camion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: 'Ejes'\n",
    "def obtener_ejes(tipo_camion):\n",
    "    if pd.isnull(tipo_camion):\n",
    "        return 0\n",
    "    elif tipo_camion == 'Big Coil':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 Ejes':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 Eje Cortina':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 ejes Portacinta':\n",
    "        return 3\n",
    "    elif tipo_camion == \"Plataforma 48'\" or tipo_camion == \"Plataforma 53'\":\n",
    "        return 4\n",
    "    elif tipo_camion == \"2 Ejes\":\n",
    "        return 2\n",
    "\n",
    "df['Ejes'] = df['Tipo transporte'].apply(obtener_ejes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52664495",
   "metadata": {},
   "source": [
    "Feature: Tipo de viaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da2642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero se a√±ade 'Va' a los valores vacios\n",
    "df['TpoVje'] = df['TpoVje'].fillna('Vacio')\n",
    "\n",
    "# One hot encoding para la columna 'TpoVje'\n",
    "tipo_viaje_dummies = pd.get_dummies(df['TpoVje'], prefix='Tipo_Viaje')\n",
    "df = pd.concat([df, tipo_viaje_dummies], axis=1)\n",
    "# Eliminar variable original\n",
    "df.drop('TpoVje', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa76dc",
   "metadata": {},
   "source": [
    "Feature: Fecha de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir la columna 'F.Salida' en componentes separados\n",
    "df['F.Salida'] = pd.to_datetime(df['F.Salida'], errors='coerce')\n",
    "df['Salida_A√±o'] = df['F.Salida'].dt.year\n",
    "df['Salida_Mes'] = df['F.Salida'].dt.month\n",
    "df['Salida_Dia'] = df['F.Salida'].dt.day\n",
    "\n",
    "# Eliminamos la columna original\n",
    "df.drop('F.Salida', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos columnas que puedan tener acentos\n",
    "df.rename(columns={\n",
    "    'Garant√≠a': 'Garantia',\n",
    "    'Variaci√≥n': 'Variacion'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c1490",
   "metadata": {},
   "source": [
    "Eliminacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28cb9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de datos\n",
    "df = df.drop(columns=['Viaje', 'Permiso', 'Shipment', 'Sociedad','Ori-Dest-TT','Ori-Dest','Modal.','Moneda','Estatus'])\n",
    "\n",
    "df = df.drop(columns=['Planta Origen', 'Origen', 'Destino', 'Desc Org. Apt', 'TPP', 'Desc TPP', 'Transp.Leg'])\n",
    "\n",
    "df = df.drop(columns=['Tipo planta', 'Nombre', 'Desc TpoTrn', 'Tipo transporte', 'TpoTrn.APT'])\n",
    "\n",
    "df = df.drop(columns=['TpoTrn.Leg'])\n",
    "\n",
    "# Se elimina la √∫ltima linea que contiene totales\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Guardar el dataframe procesado\n",
    "df.to_csv(\"../data/processed/datos_procesados.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a73607",
   "metadata": {},
   "source": [
    "\n",
    "#### Justificaci√≥n de decisiones sobre valores faltantes, outliers, y agregaciones\n",
    "\n",
    "| Variable a eliminar | Raz√≥n |\n",
    "| ---- | ---- |\n",
    "| Viaje | Informaci√≥n irrelevante |\n",
    "| Permiso | Informaci√≥n irrelevante |\n",
    "| Shipment | Informaci√≥n irrelevante |\n",
    "| Sociedad | Informaci√≥n irrelevante |\n",
    "| Ori-Dest-TT | Informaci√≥n redundante |\n",
    "| Ori-Dest | Informaci√≥n redundante |\n",
    "| Modal | Informaci√≥n irrelevante |\n",
    "| Moneda | Informaci√≥n irrelevante |\n",
    "| Estatus | Informaci√≥n irrelevante |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3834f",
   "metadata": {},
   "source": [
    "#### Creaci√≥n de variables relevantes para su enfoque (ratios, interacciones, lags, etc.)\n",
    "#### Validaci√≥n de supuestos requeridos por sus m√©todos\n",
    " \n",
    "\n",
    "‚†ÄSecci√≥n B: An√°lisis Principal (var√≠a seg√∫n enfoque)\n",
    "\n",
    "- Construcci√≥n y justificaci√≥n del DAG (Directed Acyclic Graph)\n",
    "- Identificaci√≥n de confounders, mediadores, y colisionadores\n",
    "- Estrategia de identificaci√≥n causal (IV, diferencias-en-diferencias, etc.)\n",
    "- Estimaci√≥n de efectos causales con intervalos de confianza\n",
    "- An√°lisis de sensibilidad a violaciones de supuestos\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd2f89",
   "metadata": {},
   "source": [
    "# Construccion del DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99375b50",
   "metadata": {},
   "source": [
    "1. Importamos todas las librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Try multiple possible import paths for causal-learn / causallearn to support different package versions\n",
    "try:\n",
    "    # Preferred package layout (causal-learn)\n",
    "    from causal_learn.search.ConstraintBased.PC import pc\n",
    "    from causal_learn.utils.cit import fisherz\n",
    "except Exception:\n",
    "    try:\n",
    "        # Alternative package name / layout (causallearn)\n",
    "        from causallearn.search.ConstraintBased.PC import pc\n",
    "        from causallearn.utils.cit import fisherz\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Could not import 'pc' and 'fisherz' from causal-learn/causallearn; \"\n",
    "            \"please install 'causal-learn' (pip install causal-learn) \"\n",
    "            \"and verify the package version and import paths.\"\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54ff7c",
   "metadata": {},
   "source": [
    "2. Configuraciones y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/processed/datos_procesados.csv\"  # IMPORTANTE: Reemplaza con tu ruta de archivo\n",
    "TARGET_VARIABLE_NAME = 'Costo'                 \n",
    "COEFFICIENT_THRESHOLD = 0.05                   \n",
    "SELECTED_COLUMNS = [\n",
    "    'Garantia', \n",
    "    'Flete Falso (KG)', \n",
    "    'Flete Falso (MXN)', \n",
    "    #'Flete Falso (USD)', ELIMINADO POR REDUNDANCIA\n",
    "    'H.Salida',\n",
    "    #'#Remitos', ELIMINADO POR REDUNDANCIA\n",
    "    'Peso Total (kg)',\n",
    "    'Costo',\n",
    "    'Costo Prom',\n",
    "    'Variacion',\n",
    "    #'CostoxTn',\n",
    "    'Shp.Cost',\n",
    "    'Monto Real',\n",
    "    #'Monto Falso',\n",
    "    'Monto Reparto',\n",
    "    'Distancia_km',\n",
    "    'Tipo_Planta_Masivos',\n",
    "    #'Tipo_Planta_Revestidos',\n",
    "    'Deposito_Origen_CHUCS',\n",
    "    'Deposito_Origen_CHUMA',\n",
    "    'Deposito_Origen_MVAMI',\n",
    "    'Deposito_Origen_MVAML',\n",
    "    #'Deposito_Origen_UNIUN',\n",
    "    'Tipo_Permiso_1.0',\n",
    "    'Tipo_Permiso_2.0',\n",
    "    #'Tipo_Permiso_3.0',\n",
    "    'Tipo_Servicio_EX',\n",
    "    #'Tipo_Servicio_FO',\n",
    "    'Ejes',\n",
    "    'Tipo_Viaje_AP',\n",
    "    'Tipo_Viaje_EX',\n",
    "    'Tipo_Viaje_LT',\n",
    "    'Tipo_Viaje_NO',\n",
    "    #'Tipo_Viaje_PM',\n",
    "    #'Tipo_Viaje_Vacio',\n",
    "    #'Salida_A√±o', Eliminada porque todos los datos son del 2024\n",
    "    'Salida_Mes',\n",
    "    'Salida_Dia'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8098199f",
   "metadata": {},
   "source": [
    "3. Funci√≥n de visualizaci√≥n para PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pc_dag(adj_matrix, columns, causal_strengths=None, title='Grafo Causal (CPDAG) Algoritmo PC'):\n",
    "    \"\"\"Visualiza el grafo de estructura (CPDAG) generado por el Algoritmo PC.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(columns)\n",
    "    edge_labels = {}\n",
    "    \n",
    "    # Mapear la matriz de adyacencia de PC a un grafo NetworkX\n",
    "    for i, col_i in enumerate(columns):\n",
    "        for j, col_j in enumerate(columns):\n",
    "            # El Algoritmo PC usa 1 para i -> j y 3 para i <- j (y 2 para i -- j)\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                # i -> j\n",
    "                G.add_edge(col_i, col_j)\n",
    "                if causal_strengths and (col_i, col_j) in causal_strengths:\n",
    "                    weight = causal_strengths[(col_i, col_j)]\n",
    "                    edge_labels[(col_i, col_j)] = f\"{weight:.4f}\"\n",
    "            elif adj_matrix[i, j] == 2 and adj_matrix[j, i] == 2:\n",
    "                # i -- j (Borde incierto, bidireccional para visualizaci√≥n)\n",
    "                if not G.has_edge(col_j, col_i):  # Evitar duplicados\n",
    "                    G.add_edge(col_i, col_j, weight='?')\n",
    "    \n",
    "    # Configuraci√≥n de la visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Intentar usar Graphviz, pero con mejor manejo de excepciones\n",
    "    pos = None\n",
    "    try:\n",
    "        pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')\n",
    "        print(\"‚úì Layout con Graphviz (dot) exitoso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  No se pudo usar Graphviz: {type(e).__name__}. Usando spring_layout...\")\n",
    "        pos = nx.spring_layout(G, k=2.5, iterations=100, seed=42)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightcoral', node_size=1500, edgecolors='gray')\n",
    "    \n",
    "    # Dibuja aristas dirigidas (flechas)\n",
    "    directed_edges = [(u, v) for u, v in G.edges() if G.edges[u, v].get('weight') != '?']\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=directed_edges, arrowstyle='->', \n",
    "                          arrowsize=20, edge_color='black', width=1.5)\n",
    "    \n",
    "    # Dibuja aristas inciertas (baja opacidad)\n",
    "    undirected_edges = [(u, v) for u, v in G.edges() if G.edges[u, v].get('weight') == '?']\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=undirected_edges, arrowstyle='-', \n",
    "                          style='dashed', edge_color='gray', width=1.0, alpha=0.5)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')\n",
    "    \n",
    "    # Dibuja las etiquetas de fuerza causal\n",
    "    if edge_labels:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, \n",
    "                                    font_color='red', font_size=8, label_pos=0.5)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738ab96",
   "metadata": {},
   "source": [
    "4. Bloque de ejecuci√≥n principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81009d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Carga, Filtrado y Preprocesamiento de Datos ---\n",
    "print(\"--- 1. Carga, Filtrado y Preprocesamiento de Datos ---\")\n",
    "try:\n",
    "    data = pd.read_csv(FILE_PATH)\n",
    "    data = data[SELECTED_COLUMNS]\n",
    "    data = data.dropna()\n",
    "    \n",
    "    for col in data.columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    \n",
    "    data = data.dropna().astype(float)\n",
    "    print(f\"‚úÖ Datos cargados y filtrados. Dimensiones finales: {data.shape}\")\n",
    "    print(f\"   Columnas: {list(data.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Archivo no encontrado en {FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR durante la carga o limpieza de datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Estandarizaci√≥n de Datos ---\n",
    "scaler = StandardScaler()\n",
    "data_scaled_np = scaler.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled_np, columns=data.columns)\n",
    "print(\"\\n[Estandarizaci√≥n de Datos]\")\n",
    "print(\"‚úÖ Datos escalados con StandardScaler.\")\n",
    "\n",
    "# --- 3. DESCUBRIMIENTO CAUSAL (Algoritmo PC) ---\n",
    "print(\"\\n--- 3. Descubrimiento Causal (Algoritmo PC) ---\")\n",
    "try:\n",
    "    data_matrix = data_scaled.values\n",
    "    \n",
    "    # Ejecutar el Algoritmo PC\n",
    "    graph_pc = pc(data_matrix, alpha=0.05, indep_test=fisherz)\n",
    "    \n",
    "    # CORRECCI√ìN: Acceso correcto a la matriz de adyacencia\n",
    "    adj_matrix = graph_pc.G.graph\n",
    "    \n",
    "    print(f\"‚úÖ Descubrimiento de Estructura exitoso con Algoritmo PC.\")\n",
    "    print(f\"   Variables analizadas: {len(data.columns)}\")\n",
    "    print(f\"\\nMatriz de Adyacencia (forma: {adj_matrix.shape}):\")\n",
    "    print(adj_matrix)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR CR√çTICO en el Descubrimiento Causal PC: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# --- 4. C√ÅLCULO DE FUERZA CAUSAL (Regresi√≥n OLS) ---\n",
    "print(\"\\n--- 4. C√ÅLCULO DE FUERZA CAUSAL (Regresi√≥n OLS) ---\")\n",
    "causal_strengths = {}\n",
    "\n",
    "# Iterar sobre cada variable como TARGET\n",
    "for target_name in data_scaled.columns:\n",
    "    target_index = data_scaled.columns.get_loc(target_name)\n",
    "    \n",
    "    # Identificar las variables PARENTES (causas) con flechas dirigidas al TARGET\n",
    "    parents_indices = [i for i, col in enumerate(data_scaled.columns) \n",
    "                        if adj_matrix[i, target_index] == 1]\n",
    "    parent_names = [data_scaled.columns[i] for i in parents_indices]\n",
    "    \n",
    "    if parent_names:\n",
    "        # Construir el modelo de regresi√≥n\n",
    "        Y = data_scaled[target_name]\n",
    "        X = data_scaled[parent_names]\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        try:\n",
    "            model = OLS(Y, X).fit()\n",
    "            print(f\"\\nüìä Modelo para la variable DEPENDIENTE: {target_name}\")\n",
    "            print(f\"   R-squared: {model.rsquared:.4f}\")\n",
    "            \n",
    "            # Recorrer los resultados para obtener los coeficientes\n",
    "            for parent in parent_names:\n",
    "                coefficient = model.params.get(parent, 0.0)\n",
    "                p_value = model.pvalues.get(parent, 1.0)\n",
    "                \n",
    "                # Usar el coeficiente solo si es estad√≠sticamente significativo\n",
    "                if p_value < 0.05: \n",
    "                    causal_strengths[(parent, target_name)] = coefficient\n",
    "                    print(f\"  ‚úì {parent} ‚Üí {target_name} | CS: {coefficient:.4f} (p={p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  ‚úó {parent} ‚Üí {target_name} | CS: {coefficient:.4f} (p={p_value:.4f}) [NO SIG]\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error en regresi√≥n para {target_name}: {e}\")\n",
    "\n",
    "if not causal_strengths:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: No se encontraron relaciones causales significativas.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Total de relaciones causales significativas encontradas: {len(causal_strengths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfac13",
   "metadata": {},
   "source": [
    "Visualizaci√≥n del DAG con Fuerza Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f522437",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Visualizaci√≥n del Grafo Causal ---\")\n",
    "plot_pc_dag(\n",
    "    adj_matrix, \n",
    "    data_scaled.columns.tolist(), \n",
    "    causal_strengths=causal_strengths,\n",
    "    title=f'Grafo Causal PC + Fuerza OLS (Alpha=0.05)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2f8c2",
   "metadata": {},
   "source": [
    "# 4. An√°lisis de casos especiales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada646d",
   "metadata": {},
   "source": [
    "El an√°lisis de outliers se enfoc√≥ en la eficiencia unitaria y no en el gasto total, lo cual fue crucial para identificar anomal√≠as reales que impactan la gesti√≥n de costos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1626c0",
   "metadata": {},
   "source": [
    "### Analisis de costo por Tonelada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106491ae",
   "metadata": {},
   "source": [
    "Se procedi√≥ a identificar outliers de eficiencia unitaria (Costo/Tn) que impactan el presupuesto, segmentando por Destino y comparando el costo unitario contra la media hist√≥rica de su propia ruta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88e25e",
   "metadata": {},
   "source": [
    "### 1.Hallazgos Clave y Cuantificaci√≥n de Ineficiencia\n",
    "\n",
    "El an√°lisis por destino revel√≥ la ineficiencia extrema concentrada en el cierre de a√±o. Estos valores se obtuvieron al comparar el Costo/Tn mensual de cada destino con su l√≠nea base hist√≥rica, tal como se muestra a continuaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cb09c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Carga de datos\n",
    "df = pd.read_excel('../data/raw/Viajes Sep-Dic 24 v2.xlsx', sheet_name= 'Viajes')\n",
    "\n",
    "# 2. DEFINICI√ìN DE COLUMNAS CLAVE\n",
    "NOMBRE_FECHA = 'F.Salida'  \n",
    "NOMBRE_PESO = 'Peso Total (kg)' \n",
    "NOMBRE_GASTO = 'Costo'     \n",
    "NOMBRE_DESTINO = 'Destino' \n",
    "NOMBRE_TIPO_FLETE = 'Tipo transporte' \n",
    "\n",
    "# 3. Preparaci√≥n de datos y estandarizaci√≥n de nombres\n",
    "df = df.rename(columns={\n",
    "    NOMBRE_FECHA: 'Fecha',\n",
    "    NOMBRE_PESO: 'Peso_kg',\n",
    "    NOMBRE_GASTO: 'Costo',\n",
    "    NOMBRE_DESTINO: 'Destino',\n",
    "    NOMBRE_TIPO_FLETE: 'Tipo_Flete'\n",
    "})\n",
    "\n",
    "# 4. C√°lculo de la Eficiencia (Costo por Tonelada)\n",
    "df['Costo_por_Tn'] = np.where(df['Peso_kg'] > 0, (df['Costo'] / df['Peso_kg']) * 1000, np.nan)\n",
    "\n",
    "\n",
    "df = df.dropna(subset=['Fecha', 'Costo_por_Tn', 'Destino', 'Tipo_Flete'])\n",
    "df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
    "df['Periodo_Mensual'] = df['Fecha'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. C√°lculo de la Eficiencia (Costo por Tonelada)\n",
    "df['Costo_por_Tn'] = np.where(df['Peso_kg'] > 0, (df['Costo'] / df['Peso_kg']) * 1000, np.nan)\n",
    "\n",
    "# 5. AGRUPACI√ìN CLAVE: Costo Promedio Mensual por Tonelada por Destino\n",
    "df['Periodo_Mensual'] = df['Fecha'].dt.to_period('M')\n",
    "df_eficiencia = df.groupby(['Periodo_Mensual', 'Destino'])['Costo_por_Tn'].mean().reset_index()\n",
    "\n",
    "# Detecci√≥n del Top 5 de Ineficiencia (Outliers Operacionales)\n",
    "top_5_ineficiencia = df_eficiencia.sort_values(by='Costo_por_Tn', ascending=False).head(5)\n",
    "print(\"\\n--- TOP 5 PERIODOS CON LA MAYOR INEFICIENCIA ---\")\n",
    "print(top_5_ineficiencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AGRUPACI√ìN Y ESTABLECIMIENTO DE LA NORMA OPERATIVA POR DESTINO\n",
    "df_eficiencia = df.groupby(['Periodo_Mensual', 'Destino'])['Costo_por_Tn'].mean().reset_index()\n",
    "media_historica_destino = df_eficiencia.groupby('Destino')['Costo_por_Tn'].mean().reset_index().rename(columns={'Costo_por_Tn': 'Media_Historica_CostoTn'})\n",
    "\n",
    "# 2. UNI√ìN Y CUANTIFICACI√ìN DEL DISPARO PORCENTUAL\n",
    "df_analisis_final = pd.merge(df_eficiencia, media_historica_destino, on='Destino', how='left')\n",
    "df_analisis_final['Disparo_%'] = (df_analisis_final['Costo_por_Tn'] / df_analisis_final['Media_Historica_CostoTn'] - 1) * 100\n",
    "\n",
    "# 3. TOP 5 OUTLIERS OPERACIONALES (EVIDENCIA)\n",
    "outliers_cuantificados = df_analisis_final.sort_values(by='Disparo_%', ascending=False).head(5)\n",
    "print(\"\\n--- EVIDENCIA DE LOS 5 OUTLIERS DE EFICIENCIA ---\")\n",
    "print(outliers_cuantificados[['Periodo_Mensual', 'Destino', 'Costo_por_Tn', 'Disparo_%']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ce438",
   "metadata": {},
   "source": [
    "El caso de Tetla en Diciembre de 2024 mostr√≥ la m√°xima ineficiencia, con un costo por tonelada 50.80% superior a su propia norma hist√≥rica. El hecho de que Tetla y Hermosillo muestren picos tan altos en el mismo mes es evidencia de un problema sist√©mico de cierres de fin de a√±o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86568ebc",
   "metadata": {},
   "source": [
    "### 2. Investigaci√≥n Detallada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8443b5",
   "metadata": {},
   "source": [
    "La ineficiencia es tan alta que se requiere investigar si fue causada por una baja carga √∫til o por un aumento de tarifa. Aunque los outliers principales son por destino, se presenta el an√°lisis de la flota '2 Ejes' (el outlier inicial) para descartar la subutilizaci√≥n de camiones como causa general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n del Caso a Investigar (Outlier Inicial)\n",
    "PERIODO_OUTLIER = pd.Period('2024-09', freq='M')\n",
    "TIPO_FLETE_OUTLIER = '2 Ejes'\n",
    "\n",
    "# 1. Carga Promedio Hist√≥rica (de la flota '2 Ejes')\n",
    "media_historica_peso = df[df['Tipo_Flete'] == TIPO_FLETE_OUTLIER]['Peso_kg'].mean()\n",
    "\n",
    "# 2. Carga Promedio Durante el Outlier (2024-09)\n",
    "df_outlier = df[\n",
    "    (df['Periodo_Mensual'] == PERIODO_OUTLIER) & \n",
    "    (df['Tipo_Flete'] == TIPO_FLETE_OUTLIER)\n",
    "]\n",
    "media_outlier_peso = df_outlier['Peso_kg'].mean()\n",
    "\n",
    "# 3. C√°lculo de la Variaci√≥n\n",
    "variacion_peso = (media_outlier_peso / media_historica_peso) - 1\n",
    "\n",
    "print(f\"Media Hist√≥rica de Peso (kg): {media_historica_peso:,.2f}\")\n",
    "print(f\"Peso Promedio en Sep. 2024 (kg): {media_outlier_peso:,.2f}\")\n",
    "print(f\"La variaci√≥n de peso fue de: {variacion_peso * 100:,.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1d17a0",
   "metadata": {},
   "source": [
    "La evidencia mostr√≥ que la variaci√≥n de peso para el outlier inicial fue de solo ‚àí0.84%, lo que es insignificante. Esto confirma que el disparo de +50.80% en Costo/Tn se debe exclusivamente a un aumento de tarifa o precio (ej. tarifas spot, urgencias, penalizaciones), y no a un problema log√≠stico de carga."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe4fdfc",
   "metadata": {},
   "source": [
    "### 3. Conclusiones Finales y Subgrupos Relevantes\n",
    "\n",
    "Evidencia Contra la Estabilidad: La anomal√≠a es prueba de que el precio unitario (Costo/Tn) est√° sujeto a incrementos de precio sin control.\n",
    "Problema Estructural: Los destinos de larga distancia (Chetumal) con ineficiencias recurrentes (12-14%) indican un problema estructural en la tarifa base o en la consolidaci√≥n de carga para esas rutas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e0ac02",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "\n",
    "## 5. Primeros Hallazgos y Respuestas Tentativas (3-4 p√°ginas)\n",
    "\n",
    "Para cada pregunta de investigaci√≥n:\n",
    "\n",
    "Evidencia emp√≠rica encontrada (con visualizaciones apropiadas)\n",
    "Magnitud y significancia de los efectos (pr√°ctica, no solo estad√≠stica)\n",
    "Interpretaci√≥n en el contexto del negocio\n",
    "Nivel de confianza en el hallazgo\n",
    "Explicaciones alternativas que no pueden descartarse\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d717e3a",
   "metadata": {},
   "source": [
    "# 5. Primeros Hallazgos y Respuestas Tentativas\n",
    "\n",
    "Esta secci√≥n traduce los resultados estad√≠sticos del modelo econom√©trico y las conclusiones de la **Validaci√≥n Rigurosa** en hallazgos de negocio concretos, enfocados en la pregunta central de la investigaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1. Respuesta a la Pregunta Central: Costo Diferencial por Transportista\n",
    "\n",
    "**Pregunta:** ¬øExiste una diferencia significativa y cuantificable en el costo promedio de los viajes entre los distintos transportistas, una vez controlados los factores log√≠sticos (ruta, peso, tipo de servicio)?\n",
    "\n",
    "### A. Evidencia Emp√≠rica Encontrada\n",
    "\n",
    "El modelo **OLS completo ($R^2=0.723$)** confirma la existencia de diferenciales de costo altamente significativos entre los transportistas. La ruta explica la mayor parte de la varianza, pero una vez controlado este factor, se a√≠sla el impacto puro de cada proveedor.\n",
    "\n",
    "**Visualizaci√≥n Clave: Diferenciales de Costo por Transportista**\n",
    "El siguiente gr√°fico muestra el *Premium* (rojo) o *Descuento* (verde) en MXN que los transportistas aplican por encima o por debajo del costo promedio de la categor√≠a base, controlando la ruta y el peso.\n",
    "![Top 5 Premiums y Descuentos de Transportistas Significativos](image_b34c57.png)\n",
    "\n",
    "**An√°lisis de los Extremos**\n",
    "\n",
    "| Coeficiente | Efecto Estimado (MXN) | P-valor | CI 95% Bootstrap | Significaci√≥n Pr√°ctica |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| **Carrier\\_100139.0 (Fletes Astro, SA de CV)** | $\\mathbf{-1,794.87}$ | $\\mathbf{0.0040}$ | $\\mathbf{[-2,232.51, -1,294.25]}$ | Descuento estructural por viaje. |\n",
    "| **Auto Transportes Modernos, SA de CV** | $\\mathbf{+8,900}$ (Aprox.) | $\\mathbf{< 0.05}$ | N/A | Sobrecosto de m√°s de $8,000$ MXN por viaje. |\n",
    "\n",
    "### B. Magnitud y Significancia de los Efectos\n",
    "\n",
    "#### 1. Transportista (Efecto Principal)\n",
    "\n",
    "* **Magnitud Pr√°ctica del Ahorro:** El descuento promedio de **$\\$-1,794.87$ por viaje** aplicado por el transportista clave representa un ahorro estructural inmediato para el negocio.\n",
    "* **Robustez y CI:** El **Intervalo de Confianza Bootstrap 95%** [$-2,232.51, -1,294.25$] **no contiene el cero**, confirmando que el descuento es robusto y existe una alta confianza de que el costo real de este transportista es inferior al de la base.\n",
    "\n",
    "#### 2. Controles Log√≠sticos\n",
    "\n",
    "* **Rutas:** La **elecci√≥n de ruta** es el principal impulsor de costos, explicando m√°s del $\\mathbf{56.9\\%}$ de la varianza total del costo.\n",
    "* **Peso Total (kg):** El impacto del peso es bajo ($\\mathbf{0.0352}$ MXN/kg) y solo **marginalmente significativo** ($P \\approx 0.033$). Esto indica que la estructura de costos est√° dominada por tarifas planas de ruta, haciendo que el costo incremental del peso sea secundario.\n",
    "\n",
    "### C. Interpretaci√≥n en el Contexto del Negocio\n",
    "\n",
    "1.  **Oportunidad T√°ctica de Ahorro (Corto Plazo):** Maximizar la asignaci√≥n de viajes a los transportistas con descuentos (zona verde del gr√°fico) es una ganancia inmediata. La diferencia de costo entre los transportistas extremos es superior a $\\mathbf{\\$10,000}$ por viaje, lo que justifica la reasignaci√≥n.\n",
    "2.  **Estrategia de Renegociaci√≥n (Largo Plazo):** Los transportistas con mayores *Premiums* (zona roja, como **Transportes Orta S.A de C.V** con $\\mathbf{\\approx \\$3,500}$) son objetivos primarios para renegociar tarifas o desviar volumen.\n",
    "3.  **Prioridad de Optimizaci√≥n:** Dada la dominancia de la ruta en la explicaci√≥n de la varianza, el enfoque principal de optimizaci√≥n debe seguir siendo la **consolidaci√≥n y negociaci√≥n de tarifas por ruta** antes que la optimizaci√≥n por transportista.\n",
    "\n",
    "### D. Nivel de Confianza en el Hallazgo\n",
    "\n",
    "El nivel de confianza en la existencia de los diferenciales de costo entre transportistas es **ALTO**, respaldado por:\n",
    "* **Robustez Estructural:** El efecto se mantiene en la especificaci√≥n logar√≠tmica (Test de Robustez).\n",
    "* **Incertidumbre Cuantificada:** La significancia estad√≠stica del descuento clave es confirmada por el **CI Bootstrap $\\mathbf{95\\%}$**.\n",
    "\n",
    "### E. Explicaciones Alternativas que No Pueden Descartarse\n",
    "\n",
    "1.  **Inestabilidad Temporal (Advertencia Cr√≠tica):** El **Test de Chow rechaz√≥ la estabilidad estructural** ($P \\approx 0.0003$). Esto implica que los coeficientes no fueron constantes entre los per√≠odos, y el descuento de $\\$-1,794.87$ podr√≠a no ser sostenible. Se requiere una investigaci√≥n de Log√≠stica para determinar la causa (cambio de contrato, promoci√≥n, o fluctuaci√≥n de mercado).\n",
    "2.  **Riesgo de Inferencias Inv√°lidas:** La presencia de **Heterocedasticidad** ($P \\approx 0.0000$, Test de Breusch-Pagan) y la **Inestabilidad Temporal** exige que el modelo final para reporte se estime utilizando **Errores Est√°ndar HAC (Huber-White/Newey-West)** para corregir los $P$-valores y garantizar que las inferencias sean v√°lidas.\n",
    "3.  **Variables Omitidas (Calidad):** El modelo no controla por la **calidad del servicio** (da√±os, retrasos, cumplimiento de ETA). Es posible que el descuento de **Fletes Astro** se compense con un servicio de menor calidad o mayor riesgo, lo cual debe ser evaluado por la gerencia de Log√≠stica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b550a29",
   "metadata": {},
   "source": [
    "## 6. Reflexi√≥n Cr√≠tica sobre Limitaciones (1-2 p√°ginas)\n",
    "\n",
    "Supuestos que no pudieron validar completamente\n",
    "Limitaciones de los datos para responder sus preguntas\n",
    "Amenazas a la validez interna y externa\n",
    "Sesgos potenciales en su an√°lisis\n",
    "Lo que su m√©todo NO puede revelar sobre el problema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
