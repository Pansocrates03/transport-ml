{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8dc9dd-34c6-4c10-8263-935a2be81592",
   "metadata": {},
   "source": [
    "# Enfoque econom√©trico\n",
    "\n",
    "Para el enfoque econom√©trico usamos m√∫ltples modelos:\n",
    "\n",
    "- OLS\n",
    "- Fixed Effect\n",
    "- Random Effect\n",
    "\n",
    "Se van a probar ambos con la prueba Hausman, y la descomposici√≥n de varianza del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa7b55-cc9d-4c8d-842d-8c3cb6dac601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from linearmodels import PanelOLS, RandomEffects\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Raw Data\n",
    "df_raw = pd.read_excel(\"../data/raw/Viajes Sep-Dic 24 v2.xlsx\", sheet_name='Viajes')\n",
    "df_raw = df_raw[df_raw['Viaje'].notna()].copy()\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} trips\")\n",
    "print(f\"Date range: {df_raw['F.Salida'].min()} to {df_raw['F.Salida'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591891e-f23c-4b13-a626-a1837167ed7e",
   "metadata": {},
   "source": [
    "## Cargar y limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453ebe-bd24-4f4e-b76a-b2e186340fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Cleaning and Time Features\n",
    "df_raw['F.Salida'] = pd.to_datetime(df_raw['F.Salida'], errors='coerce')\n",
    "df_raw['Mes'] = df_raw['F.Salida'].dt.month\n",
    "df_raw['DiaSemana'] = df_raw['F.Salida'].dt.dayofweek\n",
    "df_raw['Semana'] = df_raw['F.Salida'].dt.isocalendar().week\n",
    "\n",
    "# Route statistics (proxy for distance/complexity)\n",
    "route_stats = df_raw.groupby('Ori-Dest').agg({\n",
    "    'Costo': ['mean', 'std', 'count'],\n",
    "    'Peso Total (kg)': 'mean'\n",
    "}).reset_index()\n",
    "route_stats.columns = ['Ori-Dest', 'Costo_Mean_Route', 'Costo_Std_Route', 'Route_Freq', 'Peso_Mean_Route']\n",
    "\n",
    "df = pd.merge(df_raw, route_stats, on='Ori-Dest', how='left')\n",
    "\n",
    "# Log transforms\n",
    "df['log_Costo'] = np.log(df['Costo'] + 1)\n",
    "df['log_Peso'] = np.log(df['Peso Total (kg)'] + 1)\n",
    "\n",
    "# Categorical features\n",
    "df['Es_Express'] = (df['TpoSrv'] == 'EX').astype(int)\n",
    "df['Es_Revestidos'] = (df['Tipo planta'] == 'Revestidos').astype(int)\n",
    "df['Es_Masivos'] = (df['Tipo planta'] == 'Masivos').astype(int)\n",
    "\n",
    "# IDs\n",
    "df['Ruta_ID'] = df['Ori-Dest']\n",
    "df['Carrier_ID'] = df['Transp.Leg']\n",
    "df['Carrier_Name'] = df['Nombre']\n",
    "\n",
    "print(\"Feature engineering completo\")\n",
    "print(f\"Unique routes: {df['Ruta_ID'].nunique()}\")\n",
    "print(f\"Unique carriers: {df['Carrier_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61239b7d-89b2-4ff0-9346-de3fc7f1dd7b",
   "metadata": {},
   "source": [
    "### Estad√≠sticas por transportista\n",
    "\n",
    "Ayudar√°n m√°s adelante a identificar cuales transportistas verdaderamente cobran m√°s por los mismos servicios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b19427-b5bb-4785-98b8-2474bff6b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrier Statistics\n",
    "carrier_stats = df.groupby('Carrier_ID').agg({\n",
    "    'Costo': ['mean', 'std', 'count'],\n",
    "    'Nombre': 'first'\n",
    "}).reset_index()\n",
    "carrier_stats.columns = ['Carrier_ID', 'Carrier_Cost_Mean', 'Carrier_Cost_Std', 'Carrier_Trips', 'Carrier_Name']\n",
    "\n",
    "df = pd.merge(df, carrier_stats, on='Carrier_ID', how='left')\n",
    "\n",
    "print(\"Top 10 transportistas:\")\n",
    "print(carrier_stats.nlargest(10, 'Carrier_Trips')[['Carrier_Name', 'Carrier_Trips', 'Carrier_Cost_Mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10a78f-3506-4c62-9ff7-05506e6e3478",
   "metadata": {},
   "source": [
    "## Dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf10e3b-2e07-4d2a-b485-3ab2e0e747f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Dataset\n",
    "df_clean = df.dropna(subset=['Costo', 'Peso Total (kg)', 'Carrier_ID', 'Ruta_ID']).copy()\n",
    "\n",
    "print(f\"Clean dataset: {len(df_clean)} trips ({100*len(df_clean)/len(df):.1f}% retained)\")\n",
    "\n",
    "# Create dummy variables\n",
    "top_carriers = df_clean['Carrier_ID'].value_counts().head(10).index\n",
    "df_clean['Carrier_Group'] = df_clean['Carrier_ID'].apply(\n",
    "    lambda x: x if x in top_carriers else 'Other'\n",
    ")\n",
    "carrier_dummies = pd.get_dummies(df_clean['Carrier_Group'], prefix='Carrier', drop_first=True)\n",
    "\n",
    "top_routes = df_clean['Ruta_ID'].value_counts().head(20).index\n",
    "df_clean['Route_Group'] = df_clean['Ruta_ID'].apply(\n",
    "    lambda x: x if x in top_routes else 'Other'\n",
    ")\n",
    "route_dummies = pd.get_dummies(df_clean['Route_Group'], prefix='Route', drop_first=True)\n",
    "\n",
    "print(f\"Created {len(carrier_dummies.columns)} carrier dummies\")\n",
    "print(f\"Created {len(route_dummies.columns)} route dummies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cb3da-49a7-4e86-9f71-9e0545ad1d14",
   "metadata": {},
   "source": [
    "# Modelo OLS\n",
    "\n",
    "Antes de hacer los modelos Fixed Effect y Random Effect, quiero comenzar con OLS, que est√° m√°s limitado pero llega bastante lejos con las variables correctas. Este me sirve como baseline para los otros modelos.\n",
    "Este modelo controla por el peso, las rutas, si el pedido es express, si es revestido (o masivo), los transportistas y las rutas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527138e7-7ad6-4669-821f-1c2cdcc77c4b",
   "metadata": {},
   "source": [
    "Antes de el modelo con todas las variables, hago modelos con menos variables para observar la diferencia entre sus $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97408b91-3239-4b5e-b083-ae2c9f051e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['Costo'].values\n",
    "X_base = df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']].values\n",
    "X_base = sm.add_constant(X_base)\n",
    "\n",
    "model_pooled = sm.OLS(y, X_base).fit()\n",
    "\n",
    "print(f\"\\nR^2 solo peso y servicio: {model_pooled.rsquared:.3f}\")\n",
    "\n",
    "X_route_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies\n",
    "], axis=1)\n",
    "\n",
    "X_route = X_route_data.astype(float).values\n",
    "X_route = sm.add_constant(X_route)\n",
    "\n",
    "model_route = sm.OLS(y, X_route).fit()\n",
    "\n",
    "print(f\"\\nR^2 peso, servicio y ruta: {model_route.rsquared:.3f}\")\n",
    "print(f\"Adjusted R-squared: {model_route.rsquared_adj:.3f}\")\n",
    "\n",
    "print(f\"\\nDiferencia de R^2 con solo servicio: {100*(model_route.rsquared - model_pooled.rsquared):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14800e91-2d1f-45ee-8bc2-87389a85c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1)\n",
    "\n",
    "X_full = X_full_data.astype(float).values\n",
    "X_full = sm.add_constant(X_full)\n",
    "\n",
    "model_full = sm.OLS(y, X_full).fit()\n",
    "\n",
    "print(f\"\\nR^2 completa: {model_full.rsquared:.3f}\")\n",
    "print(f\"R^2 ajustada: {model_full.rsquared_adj:.3f}\")\n",
    "print(f\"\\nKey Coefficients:\")\n",
    "print(f\"Weight: ${model_full.params[1]:.2f} per kg (p={model_full.pvalues[1]:.4f})\")\n",
    "print(f\"Express: ${model_full.params[2]:.2f} (p={model_full.pvalues[2]:.4f})\")\n",
    "print(f\"Revestidos: ${model_full.params[3]:.2f} (p={model_full.pvalues[3]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178354c-0a13-4303-a650-b31954447799",
   "metadata": {},
   "source": [
    "En esta ejecuci√≥n del modelo no muestro el efecto de los dummies de los transportistas, porque son muchos, pero debajo pongo una mejor manera de visualizar el efecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41808f0a-2335-4068-b669-cd697f7cdce5",
   "metadata": {},
   "source": [
    "### Efecto de los transportistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7c8e5-cdcd-4d08-8f64-445aa62ba0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_effects = []\n",
    "carrier_names_map = dict(zip(carrier_stats['Carrier_ID'], carrier_stats['Carrier_Name'])) # carrier_names_map[103079] da 'TRANSPORTES ORTA S.A DE C.V'\n",
    "\n",
    "# The carrier coefficients start after: const + 3 base vars + route dummies\n",
    "n_base = 4\n",
    "n_routes = len(route_dummies.columns)\n",
    "carrier_coef_start = n_base + n_routes\n",
    "\n",
    "for i, col in enumerate(carrier_dummies.columns):\n",
    "    coef_idx = carrier_coef_start + i\n",
    "    carrier_id = col.replace('Carrier_', '')\n",
    "    # El dummy es un float por alguna raz√≥n, hay que corregir eso para mapear al transportista con su id\n",
    "    try:\n",
    "        carrier_id_int = int(float(carrier_id))\n",
    "        carrier_name = carrier_names_map.get(carrier_id_int, 'Unknown')\n",
    "    except ValueError:\n",
    "        carrier_name = 'Unknown'  # Handle cases where conversion fails\n",
    "        \n",
    "    effect = model_full.params[coef_idx]\n",
    "    pval = model_full.pvalues[coef_idx]\n",
    "    \n",
    "    carrier_effects.append({\n",
    "        'Carrier_ID': carrier_id,\n",
    "        'Carrier_Name': carrier_name,\n",
    "        'Premium': effect,\n",
    "        'P_value': pval,\n",
    "        'Significant': 'Yes' if pval < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "carrier_effects_df = pd.DataFrame(carrier_effects).sort_values('Premium', ascending=False)\n",
    "print(carrier_effects_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279238d-7f93-412e-ae0a-e84e4587f26c",
   "metadata": {},
   "source": [
    "El efecto de cada transportista ahora se puede ordenar observando los 10 transportistas mas grandes entre los datos. Resalta especialemente que **TRANSPORTES ORTA** es el transportista m√°s grande para\n",
    "Ternium con mas de 1700 viajes, pero carga alrededor de 2800$ m√°s por le mismo viaje, en promedio. Eso se convierte en una diferencia de 4,900,000 entre todos los viajes que hace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db710c1-bf97-4582-8bfd-cf04b39e5c24",
   "metadata": {},
   "source": [
    "### Descomposici√≥n de la varianza (OLS)\n",
    "\n",
    "Tambi√©n como un tipo de baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9921e-99f1-4b1d-8911-7c13293166ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = df_clean['Costo'].var()\n",
    "\n",
    "# Between-route variance\n",
    "route_means = df_clean.groupby('Ruta_ID')['Costo'].mean()\n",
    "between_route_var = route_means.var()\n",
    "\n",
    "# Between-carrier variance\n",
    "carrier_means = df_clean.groupby('Carrier_ID')['Costo'].mean()\n",
    "between_carrier_var = carrier_means.var()\n",
    "\n",
    "# Explained variance from models\n",
    "explained_by_weight_service = model_pooled.rsquared * total_var\n",
    "explained_by_routes = (model_route.rsquared - model_pooled.rsquared) * total_var\n",
    "explained_by_carriers = (model_full.rsquared - model_route.rsquared) * total_var\n",
    "residual_var = (1 - model_full.rsquared) * total_var\n",
    "\n",
    "print(f\"Total Variance:          ${total_var:,.0f}\")\n",
    "print(f\"\\nDecomposition:\")\n",
    "print(f\"Weight + Service Type:   ${explained_by_weight_service:,.0f} ({100*model_pooled.rsquared:.1f}%)\")\n",
    "print(f\"Route Choice:            ${explained_by_routes:,.0f} ({100*(model_route.rsquared - model_pooled.rsquared):.1f}%)\")\n",
    "print(f\"Carrier Choice:          ${explained_by_carriers:,.0f} ({100*(model_full.rsquared - model_route.rsquared):.1f}%)\")\n",
    "print(f\"Unexplained:             ${residual_var:,.0f} ({100*(1-model_full.rsquared):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186f54d-45b7-4028-ae68-b4d232c730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Carrier Premiums\n",
    "ax1 = axes[0]\n",
    "carrier_sig = carrier_effects_df[carrier_effects_df['Significant'] == 'Yes'].head(10)\n",
    "if len(carrier_sig) > 0:\n",
    "    ax1.barh(carrier_sig['Carrier_Name'], carrier_sig['Premium'])\n",
    "    ax1.axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "    ax1.set_xlabel('Cost Premium (MXN)')\n",
    "    ax1.set_title('Carrier Cost Premium (Top 10, Significant Only)', fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "# 2. Model R-squared Comparison\n",
    "ax2 = axes[1]\n",
    "r2_values = [model_pooled.rsquared, model_route.rsquared, model_full.rsquared]\n",
    "model_names = ['Baseline', 'Route Controls', 'Route + Carrier']\n",
    "bars = ax2.bar(model_names, r2_values)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_ylabel('R-squared')\n",
    "ax2.set_title('Model Performance Comparison', fontweight='bold')\n",
    "for i, v in enumerate(r2_values):\n",
    "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f845e-23a0-4e0e-a265-ce361732f54e",
   "metadata": {},
   "source": [
    "## Conclusiones OLS\n",
    "\n",
    "Seg√∫n nuestro modelo OLS, la ruta determina la mayor√≠a del costo, seguido del peso y tipo de servicio, y finalmente los transportistas. El resto de la varianza no se explica en este modelo, pero prefiero no sobreajustar el modelo base.\n",
    "\n",
    "Lo que consideramos que propone este modelo es que la ruta misma ya explica gran parte de los costos en tiempo, combustible, etc., y sonbre este costo es que cada transportista pone encima su utilidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559e5d6",
   "metadata": {},
   "source": [
    "## Validacion Rigurosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec278a5",
   "metadata": {},
   "source": [
    "### 1. Validaci√≥n Cruzada Temporal (No Aleatoria) üìÖ\n",
    "Para evaluar la capacidad predictiva y la generalizaci√≥n de nuestro modelo a per√≠odos futuros, se utiliz√≥ una metodolog√≠a de Validaci√≥n Cruzada Temporal de Ventana Expandida (Expanding-Window). Esta t√©cnica es esencial en el an√°lisis de series de tiempo, ya que respeta la estructura cronol√≥gica de los datos y simula la predicci√≥n en un entorno real.\n",
    "\n",
    "Metodolog√≠a:\n",
    "\n",
    "Split 1: Entrenamiento con datos de Septiembre y Octubre; Prueba con los viajes de Noviembre.\n",
    "\n",
    "Split 2: Entrenamiento con Septiembre, Octubre y Noviembre; Prueba con los viajes de Diciembre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96067cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X_full_data = sm.add_constant(X_full_data, has_constant='add')\n",
    "X_full_data['Mes'] = df_clean['F.Salida'].dt.month \n",
    "\n",
    "# ======================================================================\n",
    "# *** 2. FUNCI√ìN DE VALIDACI√ìN TEMPORAL (VENTANA EXPANDIDA) ***\n",
    "# ======================================================================\n",
    "\n",
    "def temporal_cross_validation(X_data, y_data, train_months, test_months):\n",
    "    \"\"\"\n",
    "    Realiza un split temporal no aleatorio y eval√∫a el modelo OLS (o el elegido).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_mask = X_data['Mes'].isin(train_months)\n",
    "    test_mask = X_data['Mes'].isin(test_months)\n",
    "\n",
    "    X_train = X_data[train_mask].drop(columns=['Mes'])\n",
    "    X_test = X_data[test_mask].drop(columns=['Mes'])\n",
    "    y_train = y_data[train_mask]\n",
    "    y_test = y_data[test_mask]\n",
    "\n",
    "    if X_train.empty or X_test.empty:\n",
    "        return None, None\n",
    "\n",
    "    model = sm.OLS(y_train, X_train).fit() \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # C√°lculo de R^2 Out-of-Sample\n",
    "    ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "    ss_residual = np.sum((y_test - y_pred)**2)\n",
    "    r_squared_test = 1 - (ss_residual / ss_total)\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    return rmse, r_squared_test\n",
    "\n",
    "results = []\n",
    "rmse_1, r2_1 = temporal_cross_validation(X_full_data, y, [9, 10], [11])\n",
    "if rmse_1 is not None:\n",
    "    results.append({'Train Period': 'Sep-Oct', 'Test Period': 'Nov', 'RMSE': rmse_1, 'R-squared (Test)': r2_1})\n",
    "\n",
    "rmse_2, r2_2 = temporal_cross_validation(X_full_data, y, [9, 10, 11], [12])\n",
    "if rmse_2 is not None:\n",
    "    results.append({'Train Period': 'Sep-Nov', 'Test Period': 'Dic', 'RMSE': rmse_2, 'R-squared (Test)': r2_2})\n",
    "\n",
    "validation_df = pd.DataFrame(results)\n",
    "\n",
    "# Benchmarking In-Sample\n",
    "X_full_no_month = X_full_data.drop(columns=['Mes'])\n",
    "model_full_fit = sm.OLS(y, X_full_no_month).fit()\n",
    "r_squared_full_train = model_full_fit.rsquared\n",
    "\n",
    "print(\"\\n--- Resultados de Validaci√≥n Cruzada Temporal ---\")\n",
    "print(validation_df.to_string(index=False, float_format='%.4f'))\n",
    "print(f\"\\nR-squared (In-Sample, Full Data): {r_squared_full_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73960a14",
   "metadata": {},
   "source": [
    "Los resultados demuestran que el modelo tiene una capacidad explicativa fuerte, incluso fuera de la muestra, lo cual es un indicador de gran robustez.\n",
    "\n",
    "Alto Poder Predictivo: El R \n",
    "2\n",
    "  en la muestra (In-Sample R \n",
    "2\n",
    " ‚âà0.72) es alto y se mantiene s√≥lido en los per√≠odos de prueba (Out-of-Sample R >0.66). Esto es una se√±al excelente.\n",
    "\n",
    "Generalizaci√≥n Consistente: La peque√±a disminuci√≥n en el R \n",
    "2\n",
    "  de Noviembre (0.6989) a Diciembre (0.6662) sugiere una ligera ca√≠da en el poder predictivo a medida que nos acercamos al final del periodo (posiblemente debido a la estacionalidad de fin de a√±o o al aumento de la varianza del costo), pero el rendimiento se mantiene fuerte en general.\n",
    "\n",
    "Conclusi√≥n: La validaci√≥n cruzada temporal confirma que las relaciones estructurales encontradas en el modelo principal son estables y se generalizan bien a datos futuros. Esto refuerza la validez de las inferencias sobre el Premium de los transportistas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051934",
   "metadata": {},
   "source": [
    "### Test de robustez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96748715",
   "metadata": {},
   "source": [
    "Verificar si el signo y la significancia de las variables clave (Peso y Transportistas) se mantienen estables al cambiar la forma funcional del modelo de lineal a log-lineal (log-log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Creamos la matriz X usando log_Peso en lugar de Peso Total (kg)\n",
    "X_log_data = pd.concat([\n",
    "    df_clean['log_Peso'],  # Variable logar√≠tmica\n",
    "    df_clean[['Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "\n",
    "X_log = sm.add_constant(X_log_data, has_constant='add')\n",
    "y_log = df_clean['log_Costo']\n",
    "\n",
    "model_log = sm.OLS(y_log, X_log).fit()\n",
    "\n",
    "# --- 3. FIT MODELO LINEAL ORIGINAL (BASE) ---\n",
    "# Se necesita recrear el modelo lineal base para la comparaci√≥n directa.\n",
    "X_linear_data = pd.concat([\n",
    "    df_clean['Peso Total (kg)'],\n",
    "    df_clean[['Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X_linear = sm.add_constant(X_linear_data, has_constant='add')\n",
    "y_linear = df_clean['Costo']\n",
    "model_linear = sm.OLS(y_linear, X_linear).fit()\n",
    "\n",
    "carrier_name = carrier_dummies.columns[0] \n",
    "\n",
    "peso_idx = 1 \n",
    "carrier_idx = 1 + 3 + len(route_dummies.columns) + 0 \n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Modelo': ['Base (Lineal)', 'Robustez (Logar√≠tmico)'],\n",
    "    'R2 Ajustado': [model_linear.rsquared_adj, model_log.rsquared_adj],\n",
    "    'Coef. Peso/log_Peso': [model_linear.params[peso_idx], model_log.params[peso_idx]],\n",
    "    'P-valor Peso': [model_linear.pvalues[peso_idx], model_log.pvalues[peso_idx]],\n",
    "    f'Coef. {carrier_name}': [model_linear.params[carrier_idx], model_log.params[carrier_idx]],\n",
    "    f'P-valor {carrier_name}': [model_linear.pvalues[carrier_idx], model_log.pvalues[carrier_idx]]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Resultados de Robustez: Modelo Logar√≠tmico vs. Lineal ---\")\n",
    "print(results.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ce998",
   "metadata": {},
   "source": [
    "Los resultados demuestran una robustez s√≥lida en las principales inferencias del modelo:\n",
    "\n",
    "Robustez del Transportista (Hallazgo Clave):\n",
    "\n",
    "El signo del transportista clave (Carrier_100139.0) es robusto (negativo en ambos modelos, indicando un descuento constante).\n",
    "\n",
    "La significancia estad√≠stica es extremadamente alta (P‚â§0.0040 en el modelo Lineal y P=0.0000 en el Log-Log).\n",
    "\n",
    "Implicaci√≥n: La conclusi√≥n sobre el Premium/Descuento de este transportista se mantiene firme, independientemente de si la relaci√≥n Costo-Peso es lineal o logar√≠tmica.\n",
    "\n",
    "Mejora del Ajuste del Modelo: El R \n",
    "2\n",
    "  Ajustado aument√≥ significativamente (de 0.7210 a 0.8630) en la especificaci√≥n logar√≠tmica. Esto sugiere que la forma funcional log-log es estructuralmente superior para modelar la relaci√≥n entre el Costo y sus impulsores, probablemente por capturar mejor las econom√≠as de escala.\n",
    "\n",
    "Elasticidad del Peso: El coeficiente del peso perdi√≥ significancia en el modelo logar√≠tmico (P=0.1318), indicando que, si bien el costo marginal es positivo (elasticidad de 0.0180), su impacto no es estad√≠sticamente significativo cuando el modelo se ajusta a la forma Log-Log, lo que minimiza su rol frente a la elecci√≥n del transportista o la ruta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e9c56",
   "metadata": {},
   "source": [
    "### 3. An√°lisis de Estabilidad Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f93d89",
   "metadata": {},
   "source": [
    "El Test de Chow compara la suma de los errores cuadrados (RSS) del modelo estimado en todo el per√≠odo (Modelo Restringido) contra la suma de los errores cuadrados de los modelos estimados por separado en dos subper√≠odos (Modelos No Restringidos).\n",
    "\n",
    "Punto de Quiebre (t \n",
    "‚àó\n",
    " ): Utilizaremos la transici√≥n de Octubre a Noviembre (despu√©s del 31 de octubre) para dividir la muestra en dos mitades.\n",
    "\n",
    "Hip√≥tesis Nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ): Los coeficientes son iguales en ambos per√≠odos (Hay estabilidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f\n",
    "\n",
    "# Creamos la matriz X completa\n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X = sm.add_constant(X_full_data, has_constant='add')\n",
    "k = X.shape[1] # N√∫mero de coeficientes (incluye la constante)\n",
    "N_total = len(df_clean)\n",
    "\n",
    "# --- 2. MODELO RESTRINGIDO (R) - Full Data ---\n",
    "model_R = sm.OLS(y, X).fit()\n",
    "RSS_R = model_R.ssr\n",
    "\n",
    "# Punto de Quiebre: Despu√©s del 31 de Octubre\n",
    "split_date = pd.to_datetime('2024-10-31') \n",
    "\n",
    "# Per√≠odo 1: Septiembre y Octubre\n",
    "mask_1 = df_clean['F.Salida'] <= split_date\n",
    "X1 = X[mask_1]\n",
    "y1 = y[mask_1]\n",
    "model_1 = sm.OLS(y1, X1).fit()\n",
    "RSS_1 = model_1.ssr\n",
    "N1 = len(y1)\n",
    "\n",
    "# Per√≠odo 2: Noviembre y Diciembre\n",
    "mask_2 = df_clean['F.Salida'] > split_date\n",
    "X2 = X[mask_2]\n",
    "y2 = y[mask_2]\n",
    "N2 = len(y2)\n",
    "\n",
    "# Comprobaci√≥n de suficiencia de datos antes del c√°lculo\n",
    "if N1 <= k or N2 <= k:\n",
    "    print(f\"ERROR: No hay suficientes observaciones en uno de los subper√≠odos (N1={N1}, N2={N2}) para estimar {k} coeficientes.\")\n",
    "else:\n",
    "    model_2 = sm.OLS(y2, X2).fit()\n",
    "    RSS_2 = model_2.ssr\n",
    "    \n",
    "    # --- 4. C√ÅLCULO DEL ESTAD√çSTICO DE CHOW ---\n",
    "    \n",
    "    numerator = (RSS_R - (RSS_1 + RSS_2)) / k\n",
    "    denominator = (RSS_1 + RSS_2) / (N_total - 2 * k)\n",
    "    \n",
    "    F_statistic = numerator / denominator\n",
    "\n",
    "    # Valor p: usando la distribuci√≥n F\n",
    "    p_value = f.sf(F_statistic, k, N_total - 2 * k)\n",
    "\n",
    "    # --- 5. RESULTADOS ---\n",
    "    alpha = 0.05\n",
    "\n",
    "    print(\"--- Resultados del Test de Chow (Estabilidad Temporal) ---\")\n",
    "    print(f\"Punto de Quiebre: {split_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"N√∫mero de Coeficientes (k): {k}\")\n",
    "    print(f\"Estad√≠stico F: {F_statistic:.4f}\")\n",
    "    print(f\"Valor P: {p_value:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd761d1d",
   "metadata": {},
   "source": [
    "Inestabilidad Confirmada: El Valor P de 0.0003 es significativamente menor que el nivel de significancia de 0.05. Por lo tanto, rechazamos la hip√≥tesis nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ).\n",
    "\n",
    "Cambio Estructural: Hay evidencia estad√≠stica fuerte de inestabilidad estructural en el modelo. Esto implica que la relaci√≥n entre las variables de control (Peso, Servicio, Rutas) y el Costo, as√≠ como el Premium/Descuento de los Transportistas, cambiaron significativamente despu√©s del 31 de octubre.\n",
    "\n",
    "Discusi√≥n Requerida: Este hallazgo obliga a una discusi√≥n profunda en la interpretaci√≥n final:\n",
    "\n",
    "Causa Potencial: El cambio estructural puede estar impulsado por eventos estacionales (preparaci√≥n para la temporada alta de fin de a√±o en Noviembre/Diciembre), cambios en la pol√≠tica de precios de la empresa, o la renegociaci√≥n de tarifas con los transportistas clave.\n",
    "\n",
    "Modelo Final: Sugiere que un √∫nico modelo para todo el per√≠odo puede ser inadecuado. El an√°lisis deber√≠a enfocarse en el per√≠odo m√°s reciente (Nov-Dic) o considerar la modelaci√≥n con coeficientes variables en el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053978e",
   "metadata": {},
   "source": [
    "### 4.Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81741f",
   "metadata": {},
   "source": [
    "Estimar los intervalos de confianza del 95% para los coeficientes clave, como el Peso y el Transportista clave (Carrier_100139.0), mediante remuestreo para verificar si los resultados se mantienen significativos bajo un enfoque no param√©trico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Creamos la matriz X completa \n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X = sm.add_constant(X_full_data, has_constant='add')\n",
    "\n",
    "N = len(X)\n",
    "n_iterations = 500\n",
    "\n",
    "# Identificar Carrier Clave y su √≠ndice (asumiendo Carrier_100139.0)\n",
    "carrier_name = 'Carrier_100139.0'\n",
    "carrier_col_index = X_full_data.columns.get_loc(carrier_name)\n",
    "carrier_idx = 1 + carrier_col_index \n",
    "\n",
    "# √çndice de la variable Peso\n",
    "peso_idx = 1\n",
    "\n",
    "# --- 2. MODELO BASE (Para obtener CIs est√°ndar y coeficiente) ---\n",
    "model_base = sm.OLS(y, X).fit()\n",
    "\n",
    "# --- 3. EJECUCI√ìN DEL BOOTSTRAP DE PARES (C√ìDIGO CORREGIDO) ---\n",
    "boot_coeffs = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Remuestrear √≠ndices con reemplazo (Pairs Bootstrap)\n",
    "    sample_indices = np.random.choice(N, size=N, replace=True)\n",
    "    \n",
    "    X_sample = X.iloc[sample_indices]\n",
    "    y_sample = y[sample_indices] # <--- CORRECCI√ìN APLICADA AQU√ç\n",
    "\n",
    "    \n",
    "    try:\n",
    "        model_boot = sm.OLS(y_sample, X_sample).fit()\n",
    "        \n",
    "        # Almacenar los coeficientes de inter√©s\n",
    "        boot_coeffs.append({\n",
    "            'Peso_Coef': model_boot.params[peso_idx],\n",
    "            'Carrier_Coef': model_boot.params[carrier_idx]\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "boot_df = pd.DataFrame(boot_coeffs)\n",
    "\n",
    "# --- 4. C√ÅLCULO DE INTERVALOS DE CONFIANZA DEL BOOTSTRAP (Percentil) ---\n",
    "\n",
    "# CIs de Bootstrap (Percentil 2.5 y 97.5)\n",
    "peso_ci_boot = boot_df['Peso_Coef'].quantile([0.025, 0.975]).tolist()\n",
    "carrier_ci_boot = boot_df['Carrier_Coef'].quantile([0.025, 0.975]).tolist()\n",
    "\n",
    "# Resultados del modelo base para comparaci√≥n (CIs est√°ndar)\n",
    "# Convertir model_base.conf_int() a DataFrame si es necesario para usar .iloc\n",
    "ci_ols_df = model_base.conf_int()\n",
    "peso_ci_ols = ci_ols_df.iloc[peso_idx].tolist()\n",
    "carrier_ci_ols = ci_ols_df.iloc[carrier_idx].tolist()\n",
    "carrier_coef_ols = model_base.params[carrier_idx]\n",
    "\n",
    "# Crear tabla de resultados\n",
    "results_table = pd.DataFrame({\n",
    "    'Coeficiente': ['Peso Total (kg)', carrier_name],\n",
    "    'Estimaci√≥n OLS': [model_base.params[peso_idx], carrier_coef_ols],\n",
    "    'CI OLS (95%)': [f\"[{peso_ci_ols[0]:.2f}, {peso_ci_ols[1]:.2f}]\", \n",
    "                     f\"[{carrier_ci_ols[0]:.2f}, {carrier_ci_ols[1]:.2f}]\"],\n",
    "    'CI Bootstrap (95%)': [f\"[{peso_ci_boot[0]:.2f}, {peso_ci_boot[1]:.2f}]\", \n",
    "                           f\"[{carrier_ci_boot[0]:.2f}, {carrier_ci_boot[1]:.2f}]\"]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Resultados de Bootstrap (Intervalos de Confianza) ---\")\n",
    "print(f\"N√∫mero de iteraciones v√°lidas: {len(boot_df)}\")\n",
    "print(results_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90617efc",
   "metadata": {},
   "source": [
    "Robustez del Transportista (Hallazgo Clave):\n",
    "\n",
    "El Intervalo de Confianza del Bootstrap para el transportista clave es [‚àí2,232.51,‚àí1,294.25]. Este intervalo no contiene el cero, lo que confirma que el descuento promedio de $‚àí1,794.87 es estad√≠sticamente significativo al 95% bajo un enfoque de inferencia no param√©trica.\n",
    "\n",
    "El CI Bootstrap es m√°s estrecho que el CI OLS tradicional, indicando que el Premium es altamente preciso y robusto, incluso al considerar la variabilidad de la muestra.\n",
    "\n",
    "Incertidumbre del Peso:\n",
    "\n",
    "El coeficiente del Peso Total (kg) (0.03516) se encuentra en un √°rea de marginalidad. Mientras que el CI OLS apenas excluye el cero, el CI Bootstrap [‚àí0.00,0.07] apenas lo incluye, indicando que la contribuci√≥n del peso al costo no es estad√≠sticamente significativa con un nivel de confianza estricto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70bab13",
   "metadata": {},
   "source": [
    "### 5. Diagn√≥sticos Espec√≠ficos del M√©todo\n",
    "\n",
    "#### 5.1 Multicolinealidad (Factor de Inflaci√≥n de la Varianza - VIF)\n",
    "\n",
    "La multicolinealidad existe cuando las variables predictoras est√°n altamente correlacionadas entre s√≠. Un VIF alto (VIF>5 o 10) infla los errores est√°ndar, lo que dificulta determinar la contribuci√≥n individual de cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_vif(df):\n",
    "    # Convertir a float expl√≠citamente y manejar NaNs (CORRECCI√ìN CLAVE)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame({'Variable': ['N/A'], 'VIF': [np.nan]})\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = df.columns\n",
    "    \n",
    "    # A√±adir constante temporalmente para el c√°lculo de VIF (necesario para la f√≥rmula)\n",
    "    X_temp = sm.add_constant(df, prepend=True)\n",
    "    \n",
    "    # Calcular VIF para cada variable\n",
    "    vif_list = []\n",
    "    try:\n",
    "        for i in range(X_temp.shape[1]):\n",
    "            vif_val = variance_inflation_factor(X_temp.values, i)\n",
    "            vif_list.append(vif_val)\n",
    "        \n",
    "        vif_data[\"VIF\"] = vif_list[1:] # Excluir el VIF de la constante (√≠ndice 0)\n",
    "        vif_data[\"Variable\"] = df.columns\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: Error al calcular VIF. {e}\")\n",
    "        vif_data[\"VIF\"] = [np.nan] * len(df.columns)\n",
    "        \n",
    "    return vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "# Variables principales (Peso y Servicio)\n",
    "main_vars = df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']].astype(float)\n",
    "vif_main = calculate_vif(main_vars)\n",
    "\n",
    "carrier_sample_cols = list(carrier_dummies.columns[:2])\n",
    "route_sample_cols = list(route_dummies.columns[:3])\n",
    "\n",
    "X_dummy_sample = pd.concat([main_vars] + \n",
    "                           [carrier_dummies[col] for col in carrier_sample_cols if col in carrier_dummies.columns] + \n",
    "                           [route_dummies[col] for col in route_sample_cols if col in route_dummies.columns], \n",
    "                           axis=1).astype(float)\n",
    "vif_dummies = calculate_vif(X_dummy_sample)\n",
    "\n",
    "\n",
    "print(\"\\n--- Resultados de VIF (Variables Principales) ---\")\n",
    "print(vif_main.to_string(index=False, float_format='%.2f'))\n",
    "print(\"\\n--- Resultados de VIF (Muestra de Dummies) ---\")\n",
    "print(vif_dummies.to_string(index=False, float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7bea7",
   "metadata": {},
   "source": [
    "Variables Principales: Los valores VIF para todas las variables continuas, binarias y la muestra de dummies son extremadamente bajos (VIF<2).\n",
    "\n",
    "Conclusi√≥n: Esto indica que no existe un problema significativo de multicolinealidad. La contribuci√≥n individual de cada variable se puede medir de forma fiable, garantizando que los coeficientes del Peso y del Transportista no est√°n sesgados por la alta correlaci√≥n con otros predictores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244074c3",
   "metadata": {},
   "source": [
    "#### 5.2 Heterocedasticidad (Test de Breusch-Pagan)\n",
    "\n",
    "Verificar si la varianza de los residuos del modelo es constante (homocedasticidad). La heterocedasticidad (varianza no constante) es muy com√∫n en an√°lisis de costos, y si est√° presente, hace que los errores est√°ndar de OLS sean incorrectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import het_breuschpagan\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Obtener los residuos y las variables explicativas\n",
    "resid = model_base.resid\n",
    "exog = model_base.model.exog # Usar las variables explicativas que se usaron para ajustar el modelo\n",
    "\n",
    "# Ejecutar el test de Breusch-Pagan\n",
    "# El test de Breusch-Pagan prueba la hip√≥tesis nula de homocedasticidad.\n",
    "bp_test_results = het_breuschpagan(resid, exog)\n",
    "\n",
    "print(\"\\n--- Resultados del Test de Breusch-Pagan (Heterocedasticidad) ---\")\n",
    "print(f\"Estad√≠stico LM: {bp_test_results[0]:.4f}\")\n",
    "print(f\"P-valor LM: {bp_test_results[1]:.4f}\")\n",
    "print(f\"Estad√≠stico F: {bp_test_results[2]:.4f}\")\n",
    "print(f\"P-valor F: {bp_test_results[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f991f4",
   "metadata": {},
   "source": [
    "Hip√≥tesis Nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ): Homocedasticidad (varianza constante).\n",
    "\n",
    "Conclusi√≥n: Dado que el P-valor (tanto LM como F) es 0.0000 (menor a 0.05), rechazamos firmemente la hip√≥tesis nula.\n",
    "\n",
    "Implicaci√≥n: Se confirma la presencia de heterocedasticidad. Esto significa que los errores del modelo son mayores para ciertos niveles de costo o variables explicativas. Los errores est√°ndar tradicionales de OLS son incorrectos y, por lo tanto, los P-valores de tus coeficientes (incluyendo el del transportista) son inv√°lidos para la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105a54b",
   "metadata": {},
   "source": [
    "#### 5.3 Autocorrelaci√≥n (Test de Durbin-Watson)\n",
    "\n",
    "Verificar si los errores del modelo est√°n correlacionados en el tiempo (autocorrelaci√≥n), lo cual es crucial en datos de series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Se usa el modelo base (OLS) ajustado\n",
    "# El test de Durbin-Watson opera sobre los residuos del modelo\n",
    "dw_statistic = durbin_watson(model_base.resid)\n",
    "\n",
    "print(\"\\n--- Resultados del Test de Durbin-Watson (Autocorrelaci√≥n) ---\")\n",
    "print(f\"Estad√≠stico Durbin-Watson: {dw_statistic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68315c52",
   "metadata": {},
   "source": [
    "Interpretaci√≥n: El Estad√≠stico DW se encuentra muy cerca de 2.0, el valor ideal que indica la ausencia de autocorrelaci√≥n de primer orden en los residuos.\n",
    "\n",
    "Conclusi√≥n: Se concluye que la autocorrelaci√≥n serial no es una preocupaci√≥n principal para la inferencia estad√≠stica del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8febfd4",
   "metadata": {},
   "source": [
    "La confirmaci√≥n de la Heterocedasticidad y la Inestabilidad Temporal exigen una correcci√≥n para garantizar que los P-valores de los coeficientes sean v√°lidos y fiables.\n",
    "\n",
    "El modelo final para la inferencia debe ser estimado utilizando Errores Est√°ndar Robustos a la Heterocedasticidad (Huber-White). Dada la estructura de datos de panel y series de tiempo, la opci√≥n m√°s segura es utilizar Errores Est√°ndar HAC (Heteroskedasticity and Autocorrelation Consistent) (Newey-West), ya que corrigen la heterocedasticidad y cualquier potencial autocorrelaci√≥n que el Test DW pudo haber fallado en detectar completamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
