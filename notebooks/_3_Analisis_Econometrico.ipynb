{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8dc9dd-34c6-4c10-8263-935a2be81592",
   "metadata": {},
   "source": [
    "# Enfoque econom√©trico\n",
    "\n",
    "Para el enfoque econom√©trico usamos m√∫ltples modelos:\n",
    "\n",
    "- OLS\n",
    "- Fixed Effect\n",
    "- Random Effect\n",
    "\n",
    "Se van a probar ambos con la prueba Hausman, y la descomposici√≥n de varianza del mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aa7b55-cc9d-4c8d-842d-8c3cb6dac601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from linearmodels import PanelOLS, RandomEffects\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Raw Data\n",
    "df_raw = pd.read_excel(\"../data/raw/Viajes Sep-Dic 24 v2.xlsx\", sheet_name='Viajes')\n",
    "df_raw = df_raw[df_raw['Viaje'].notna()].copy()\n",
    "\n",
    "print(f\"Loaded {len(df_raw)} trips\")\n",
    "print(f\"Date range: {df_raw['F.Salida'].min()} to {df_raw['F.Salida'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591891e-f23c-4b13-a626-a1837167ed7e",
   "metadata": {},
   "source": [
    "## Cargar y limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6453ebe-bd24-4f4e-b76a-b2e186340fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Cleaning and Time Features\n",
    "df_raw['F.Salida'] = pd.to_datetime(df_raw['F.Salida'], errors='coerce')\n",
    "df_raw['Mes'] = df_raw['F.Salida'].dt.month\n",
    "df_raw['DiaSemana'] = df_raw['F.Salida'].dt.dayofweek\n",
    "df_raw['Semana'] = df_raw['F.Salida'].dt.isocalendar().week\n",
    "\n",
    "# Route statistics (proxy for distance/complexity)\n",
    "route_stats = df_raw.groupby('Ori-Dest').agg({\n",
    "    'Costo': ['mean', 'std', 'count'],\n",
    "    'Peso Total (kg)': 'mean'\n",
    "}).reset_index()\n",
    "route_stats.columns = ['Ori-Dest', 'Costo_Mean_Route', 'Costo_Std_Route', 'Route_Freq', 'Peso_Mean_Route']\n",
    "\n",
    "df = pd.merge(df_raw, route_stats, on='Ori-Dest', how='left')\n",
    "\n",
    "# Log transforms\n",
    "df['log_Costo'] = np.log(df['Costo'] + 1)\n",
    "df['log_Peso'] = np.log(df['Peso Total (kg)'] + 1)\n",
    "\n",
    "# Categorical features\n",
    "df['Es_Express'] = (df['TpoSrv'] == 'EX').astype(int)\n",
    "df['Es_Revestidos'] = (df['Tipo planta'] == 'Revestidos').astype(int)\n",
    "df['Es_Masivos'] = (df['Tipo planta'] == 'Masivos').astype(int)\n",
    "\n",
    "# IDs\n",
    "df['Ruta_ID'] = df['Ori-Dest']\n",
    "df['Carrier_ID'] = df['Transp.Leg']\n",
    "df['Carrier_Name'] = df['Nombre']\n",
    "\n",
    "print(\"Feature engineering completo\")\n",
    "print(f\"Unique routes: {df['Ruta_ID'].nunique()}\")\n",
    "print(f\"Unique carriers: {df['Carrier_ID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61239b7d-89b2-4ff0-9346-de3fc7f1dd7b",
   "metadata": {},
   "source": [
    "### Estad√≠sticas por transportista\n",
    "\n",
    "Ayudar√°n m√°s adelante a identificar cuales transportistas verdaderamente cobran m√°s por los mismos servicios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b19427-b5bb-4785-98b8-2474bff6b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrier Statistics\n",
    "carrier_stats = df.groupby('Carrier_ID').agg({\n",
    "    'Costo': ['mean', 'std', 'count'],\n",
    "    'Nombre': 'first'\n",
    "}).reset_index()\n",
    "carrier_stats.columns = ['Carrier_ID', 'Carrier_Cost_Mean', 'Carrier_Cost_Std', 'Carrier_Trips', 'Carrier_Name']\n",
    "\n",
    "df = pd.merge(df, carrier_stats, on='Carrier_ID', how='left')\n",
    "\n",
    "print(\"Top 10 transportistas:\")\n",
    "print(carrier_stats.nlargest(10, 'Carrier_Trips')[['Carrier_Name', 'Carrier_Trips', 'Carrier_Cost_Mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10a78f-3506-4c62-9ff7-05506e6e3478",
   "metadata": {},
   "source": [
    "## Dataset final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf10e3b-2e07-4d2a-b485-3ab2e0e747f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Dataset\n",
    "df_clean = df.dropna(subset=['Costo', 'Peso Total (kg)', 'Carrier_ID', 'Ruta_ID']).copy()\n",
    "\n",
    "print(f\"Clean dataset: {len(df_clean)} trips ({100*len(df_clean)/len(df):.1f}% retained)\")\n",
    "\n",
    "# Create dummy variables\n",
    "top_carriers = df_clean['Carrier_ID'].value_counts().head(10).index\n",
    "df_clean['Carrier_Group'] = df_clean['Carrier_ID'].apply(\n",
    "    lambda x: x if x in top_carriers else 'Other'\n",
    ")\n",
    "carrier_dummies = pd.get_dummies(df_clean['Carrier_Group'], prefix='Carrier', drop_first=True)\n",
    "\n",
    "top_routes = df_clean['Ruta_ID'].value_counts().head(20).index\n",
    "df_clean['Route_Group'] = df_clean['Ruta_ID'].apply(\n",
    "    lambda x: x if x in top_routes else 'Other'\n",
    ")\n",
    "route_dummies = pd.get_dummies(df_clean['Route_Group'], prefix='Route', drop_first=True)\n",
    "\n",
    "print(f\"Created {len(carrier_dummies.columns)} carrier dummies\")\n",
    "print(f\"Created {len(route_dummies.columns)} route dummies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cb3da-49a7-4e86-9f71-9e0545ad1d14",
   "metadata": {},
   "source": [
    "# Modelo OLS\n",
    "\n",
    "Antes de hacer los modelos Fixed Effect y Random Effect, quiero comenzar con OLS, que est√° m√°s limitado pero llega bastante lejos con las variables correctas. Este me sirve como baseline para los otros modelos.\n",
    "Este modelo controla por el peso, las rutas, si el pedido es express, si es revestido (o masivo), los transportistas y las rutas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527138e7-7ad6-4669-821f-1c2cdcc77c4b",
   "metadata": {},
   "source": [
    "Antes de el modelo con todas las variables, hago modelos con menos variables para observar la diferencia entre sus $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97408b91-3239-4b5e-b083-ae2c9f051e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['Costo'].values\n",
    "X_base = df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']].values\n",
    "X_base = sm.add_constant(X_base)\n",
    "\n",
    "model_pooled = sm.OLS(y, X_base).fit()\n",
    "\n",
    "print(f\"\\nR^2 solo peso y servicio: {model_pooled.rsquared:.3f}\")\n",
    "\n",
    "X_route_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies\n",
    "], axis=1)\n",
    "\n",
    "X_route = X_route_data.astype(float).values\n",
    "X_route = sm.add_constant(X_route)\n",
    "\n",
    "model_route = sm.OLS(y, X_route).fit()\n",
    "\n",
    "print(f\"\\nR^2 peso, servicio y ruta: {model_route.rsquared:.3f}\")\n",
    "print(f\"Adjusted R-squared: {model_route.rsquared_adj:.3f}\")\n",
    "\n",
    "print(f\"\\nDiferencia de R^2 con solo servicio: {100*(model_route.rsquared - model_pooled.rsquared):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14800e91-2d1f-45ee-8bc2-87389a85c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1)\n",
    "\n",
    "X_full = X_full_data.astype(float).values\n",
    "X_full = sm.add_constant(X_full)\n",
    "\n",
    "model_full = sm.OLS(y, X_full).fit()\n",
    "\n",
    "print(f\"\\nR^2 completa: {model_full.rsquared:.3f}\")\n",
    "print(f\"R^2 ajustada: {model_full.rsquared_adj:.3f}\")\n",
    "print(f\"\\nKey Coefficients:\")\n",
    "print(f\"Weight: ${model_full.params[1]:.2f} per kg (p={model_full.pvalues[1]:.4f})\")\n",
    "print(f\"Express: ${model_full.params[2]:.2f} (p={model_full.pvalues[2]:.4f})\")\n",
    "print(f\"Revestidos: ${model_full.params[3]:.2f} (p={model_full.pvalues[3]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178354c-0a13-4303-a650-b31954447799",
   "metadata": {},
   "source": [
    "En esta ejecuci√≥n del modelo no muestro el efecto de los dummies de los transportistas, porque son muchos, pero debajo pongo una mejor manera de visualizar el efecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41808f0a-2335-4068-b669-cd697f7cdce5",
   "metadata": {},
   "source": [
    "### Efecto de los transportistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7c8e5-cdcd-4d08-8f64-445aa62ba0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "carrier_effects = []\n",
    "carrier_names_map = dict(zip(carrier_stats['Carrier_ID'], carrier_stats['Carrier_Name'])) # carrier_names_map[103079] da 'TRANSPORTES ORTA S.A DE C.V'\n",
    "\n",
    "# The carrier coefficients start after: const + 3 base vars + route dummies\n",
    "n_base = 4\n",
    "n_routes = len(route_dummies.columns)\n",
    "carrier_coef_start = n_base + n_routes\n",
    "\n",
    "for i, col in enumerate(carrier_dummies.columns):\n",
    "    coef_idx = carrier_coef_start + i\n",
    "    carrier_id = col.replace('Carrier_', '')\n",
    "    # El dummy es un float por alguna raz√≥n, hay que corregir eso para mapear al transportista con su id\n",
    "    try:\n",
    "        carrier_id_int = int(float(carrier_id))\n",
    "        carrier_name = carrier_names_map.get(carrier_id_int, 'Unknown')\n",
    "    except ValueError:\n",
    "        carrier_name = 'Unknown'  # Handle cases where conversion fails\n",
    "        \n",
    "    effect = model_full.params[coef_idx]\n",
    "    pval = model_full.pvalues[coef_idx]\n",
    "    \n",
    "    carrier_effects.append({\n",
    "        'Carrier_ID': carrier_id,\n",
    "        'Carrier_Name': carrier_name,\n",
    "        'Premium': effect,\n",
    "        'P_value': pval,\n",
    "        'Significant': 'Yes' if pval < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "carrier_effects_df = pd.DataFrame(carrier_effects).sort_values('Premium', ascending=False)\n",
    "print(carrier_effects_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279238d-7f93-412e-ae0a-e84e4587f26c",
   "metadata": {},
   "source": [
    "El efecto de cada transportista ahora se puede ordenar observando los 10 transportistas mas grandes entre los datos. Resalta especialemente que **TRANSPORTES ORTA** es el transportista m√°s grande para\n",
    "Ternium con mas de 1700 viajes, pero carga alrededor de 2800$ m√°s por le mismo viaje, en promedio. Eso se convierte en una diferencia de 4,900,000 entre todos los viajes que hace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db710c1-bf97-4582-8bfd-cf04b39e5c24",
   "metadata": {},
   "source": [
    "### Descomposici√≥n de la varianza (OLS)\n",
    "\n",
    "Tambi√©n como un tipo de baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9921e-99f1-4b1d-8911-7c13293166ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_var = df_clean['Costo'].var()\n",
    "\n",
    "# Between-route variance\n",
    "route_means = df_clean.groupby('Ruta_ID')['Costo'].mean()\n",
    "between_route_var = route_means.var()\n",
    "\n",
    "# Between-carrier variance\n",
    "carrier_means = df_clean.groupby('Carrier_ID')['Costo'].mean()\n",
    "between_carrier_var = carrier_means.var()\n",
    "\n",
    "# Explained variance from models\n",
    "explained_by_weight_service = model_pooled.rsquared * total_var\n",
    "explained_by_routes = (model_route.rsquared - model_pooled.rsquared) * total_var\n",
    "explained_by_carriers = (model_full.rsquared - model_route.rsquared) * total_var\n",
    "residual_var = (1 - model_full.rsquared) * total_var\n",
    "\n",
    "print(f\"Total Variance:          ${total_var:,.0f}\")\n",
    "print(f\"\\nDecomposition:\")\n",
    "print(f\"Weight + Service Type:   ${explained_by_weight_service:,.0f} ({100*model_pooled.rsquared:.1f}%)\")\n",
    "print(f\"Route Choice:            ${explained_by_routes:,.0f} ({100*(model_route.rsquared - model_pooled.rsquared):.1f}%)\")\n",
    "print(f\"Carrier Choice:          ${explained_by_carriers:,.0f} ({100*(model_full.rsquared - model_route.rsquared):.1f}%)\")\n",
    "print(f\"Unexplained:             ${residual_var:,.0f} ({100*(1-model_full.rsquared):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186f54d-45b7-4028-ae68-b4d232c730fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Carrier Premiums\n",
    "ax1 = axes[0]\n",
    "carrier_sig = carrier_effects_df[carrier_effects_df['Significant'] == 'Yes'].head(10)\n",
    "if len(carrier_sig) > 0:\n",
    "    ax1.barh(carrier_sig['Carrier_Name'], carrier_sig['Premium'])\n",
    "    ax1.axvline(0, color='red', linestyle='--', linewidth=1)\n",
    "    ax1.set_xlabel('Cost Premium (MXN)')\n",
    "    ax1.set_title('Carrier Cost Premium (Top 10, Significant Only)', fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "\n",
    "# 2. Model R-squared Comparison\n",
    "ax2 = axes[1]\n",
    "r2_values = [model_pooled.rsquared, model_route.rsquared, model_full.rsquared]\n",
    "model_names = ['Baseline', 'Route Controls', 'Route + Carrier']\n",
    "bars = ax2.bar(model_names, r2_values)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_ylabel('R-squared')\n",
    "ax2.set_title('Model Performance Comparison', fontweight='bold')\n",
    "for i, v in enumerate(r2_values):\n",
    "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f845e-23a0-4e0e-a265-ce361732f54e",
   "metadata": {},
   "source": [
    "## Conclusiones OLS\n",
    "\n",
    "Seg√∫n nuestro modelo OLS, la ruta determina la mayor√≠a del costo, seguido del peso y tipo de servicio, y finalmente los transportistas. El resto de la varianza no se explica en este modelo, pero prefiero no sobreajustar el modelo base.\n",
    "\n",
    "Lo que consideramos que propone este modelo es que la ruta misma ya explica gran parte de los costos en tiempo, combustible, etc., y sonbre este costo es que cada transportista pone encima su utilidad. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559e5d6",
   "metadata": {},
   "source": [
    "## Validacion Rigurosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec278a5",
   "metadata": {},
   "source": [
    "### 1. Validaci√≥n Cruzada Temporal (No Aleatoria) üìÖ\n",
    "Para evaluar la capacidad predictiva y la generalizaci√≥n de nuestro modelo a per√≠odos futuros, se utiliz√≥ una metodolog√≠a de Validaci√≥n Cruzada Temporal de Ventana Expandida (Expanding-Window). Esta t√©cnica es esencial en el an√°lisis de series de tiempo, ya que respeta la estructura cronol√≥gica de los datos y simula la predicci√≥n en un entorno real.\n",
    "\n",
    "Metodolog√≠a:\n",
    "\n",
    "Split 1: Entrenamiento con datos de Septiembre y Octubre; Prueba con los viajes de Noviembre.\n",
    "\n",
    "Split 2: Entrenamiento con Septiembre, Octubre y Noviembre; Prueba con los viajes de Diciembre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96067cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X_full_data = sm.add_constant(X_full_data, has_constant='add')\n",
    "X_full_data['Mes'] = df_clean['F.Salida'].dt.month \n",
    "\n",
    "# ======================================================================\n",
    "# *** 2. FUNCI√ìN DE VALIDACI√ìN TEMPORAL (VENTANA EXPANDIDA) ***\n",
    "# ======================================================================\n",
    "\n",
    "def temporal_cross_validation(X_data, y_data, train_months, test_months):\n",
    "    \"\"\"\n",
    "    Realiza un split temporal no aleatorio y eval√∫a el modelo OLS (o el elegido).\n",
    "    \"\"\"\n",
    "    \n",
    "    train_mask = X_data['Mes'].isin(train_months)\n",
    "    test_mask = X_data['Mes'].isin(test_months)\n",
    "\n",
    "    X_train = X_data[train_mask].drop(columns=['Mes'])\n",
    "    X_test = X_data[test_mask].drop(columns=['Mes'])\n",
    "    y_train = y_data[train_mask]\n",
    "    y_test = y_data[test_mask]\n",
    "\n",
    "    if X_train.empty or X_test.empty:\n",
    "        return None, None\n",
    "\n",
    "    model = sm.OLS(y_train, X_train).fit() \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # C√°lculo de R^2 Out-of-Sample\n",
    "    ss_total = np.sum((y_test - np.mean(y_test))**2)\n",
    "    ss_residual = np.sum((y_test - y_pred)**2)\n",
    "    r_squared_test = 1 - (ss_residual / ss_total)\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "    return rmse, r_squared_test\n",
    "\n",
    "results = []\n",
    "rmse_1, r2_1 = temporal_cross_validation(X_full_data, y, [9, 10], [11])\n",
    "if rmse_1 is not None:\n",
    "    results.append({'Train Period': 'Sep-Oct', 'Test Period': 'Nov', 'RMSE': rmse_1, 'R-squared (Test)': r2_1})\n",
    "\n",
    "rmse_2, r2_2 = temporal_cross_validation(X_full_data, y, [9, 10, 11], [12])\n",
    "if rmse_2 is not None:\n",
    "    results.append({'Train Period': 'Sep-Nov', 'Test Period': 'Dic', 'RMSE': rmse_2, 'R-squared (Test)': r2_2})\n",
    "\n",
    "validation_df = pd.DataFrame(results)\n",
    "\n",
    "# Benchmarking In-Sample\n",
    "X_full_no_month = X_full_data.drop(columns=['Mes'])\n",
    "model_full_fit = sm.OLS(y, X_full_no_month).fit()\n",
    "r_squared_full_train = model_full_fit.rsquared\n",
    "\n",
    "print(\"\\n--- Resultados de Validaci√≥n Cruzada Temporal ---\")\n",
    "print(validation_df.to_string(index=False, float_format='%.4f'))\n",
    "print(f\"\\nR-squared (In-Sample, Full Data): {r_squared_full_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73960a14",
   "metadata": {},
   "source": [
    "Los resultados demuestran que el modelo tiene una capacidad explicativa fuerte, incluso fuera de la muestra, lo cual es un indicador de gran robustez.\n",
    "\n",
    "Alto Poder Predictivo: El R \n",
    "2\n",
    "  en la muestra (In-Sample R \n",
    "2\n",
    " ‚âà0.72) es alto y se mantiene s√≥lido en los per√≠odos de prueba (Out-of-Sample R >0.66). Esto es una se√±al excelente.\n",
    "\n",
    "Generalizaci√≥n Consistente: La peque√±a disminuci√≥n en el R \n",
    "2\n",
    "  de Noviembre (0.6989) a Diciembre (0.6662) sugiere una ligera ca√≠da en el poder predictivo a medida que nos acercamos al final del periodo (posiblemente debido a la estacionalidad de fin de a√±o o al aumento de la varianza del costo), pero el rendimiento se mantiene fuerte en general.\n",
    "\n",
    "Conclusi√≥n: La validaci√≥n cruzada temporal confirma que las relaciones estructurales encontradas en el modelo principal son estables y se generalizan bien a datos futuros. Esto refuerza la validez de las inferencias sobre el Premium de los transportistas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91051934",
   "metadata": {},
   "source": [
    "### Test de robustez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96748715",
   "metadata": {},
   "source": [
    "Verificar si el signo y la significancia de las variables clave (Peso y Transportistas) se mantienen estables al cambiar la forma funcional del modelo de lineal a log-lineal (log-log)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657fafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Creamos la matriz X usando log_Peso en lugar de Peso Total (kg)\n",
    "X_log_data = pd.concat([\n",
    "    df_clean['log_Peso'],  # Variable logar√≠tmica\n",
    "    df_clean[['Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "\n",
    "X_log = sm.add_constant(X_log_data, has_constant='add')\n",
    "y_log = df_clean['log_Costo']\n",
    "\n",
    "model_log = sm.OLS(y_log, X_log).fit()\n",
    "\n",
    "# --- 3. FIT MODELO LINEAL ORIGINAL (BASE) ---\n",
    "# Se necesita recrear el modelo lineal base para la comparaci√≥n directa.\n",
    "X_linear_data = pd.concat([\n",
    "    df_clean['Peso Total (kg)'],\n",
    "    df_clean[['Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X_linear = sm.add_constant(X_linear_data, has_constant='add')\n",
    "y_linear = df_clean['Costo']\n",
    "model_linear = sm.OLS(y_linear, X_linear).fit()\n",
    "\n",
    "carrier_name = carrier_dummies.columns[0] \n",
    "\n",
    "peso_idx = 1 \n",
    "carrier_idx = 1 + 3 + len(route_dummies.columns) + 0 \n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Modelo': ['Base (Lineal)', 'Robustez (Logar√≠tmico)'],\n",
    "    'R2 Ajustado': [model_linear.rsquared_adj, model_log.rsquared_adj],\n",
    "    'Coef. Peso/log_Peso': [model_linear.params[peso_idx], model_log.params[peso_idx]],\n",
    "    'P-valor Peso': [model_linear.pvalues[peso_idx], model_log.pvalues[peso_idx]],\n",
    "    f'Coef. {carrier_name}': [model_linear.params[carrier_idx], model_log.params[carrier_idx]],\n",
    "    f'P-valor {carrier_name}': [model_linear.pvalues[carrier_idx], model_log.pvalues[carrier_idx]]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Resultados de Robustez: Modelo Logar√≠tmico vs. Lineal ---\")\n",
    "print(results.to_string(index=False, float_format='%.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ce998",
   "metadata": {},
   "source": [
    "Los resultados demuestran una robustez s√≥lida en las principales inferencias del modelo:\n",
    "\n",
    "Robustez del Transportista (Hallazgo Clave):\n",
    "\n",
    "El signo del transportista clave (Carrier_100139.0) es robusto (negativo en ambos modelos, indicando un descuento constante).\n",
    "\n",
    "La significancia estad√≠stica es extremadamente alta (P‚â§0.0040 en el modelo Lineal y P=0.0000 en el Log-Log).\n",
    "\n",
    "Implicaci√≥n: La conclusi√≥n sobre el Premium/Descuento de este transportista se mantiene firme, independientemente de si la relaci√≥n Costo-Peso es lineal o logar√≠tmica.\n",
    "\n",
    "Mejora del Ajuste del Modelo: El R \n",
    "2\n",
    "  Ajustado aument√≥ significativamente (de 0.7210 a 0.8630) en la especificaci√≥n logar√≠tmica. Esto sugiere que la forma funcional log-log es estructuralmente superior para modelar la relaci√≥n entre el Costo y sus impulsores, probablemente por capturar mejor las econom√≠as de escala.\n",
    "\n",
    "Elasticidad del Peso: El coeficiente del peso perdi√≥ significancia en el modelo logar√≠tmico (P=0.1318), indicando que, si bien el costo marginal es positivo (elasticidad de 0.0180), su impacto no es estad√≠sticamente significativo cuando el modelo se ajusta a la forma Log-Log, lo que minimiza su rol frente a la elecci√≥n del transportista o la ruta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e9c56",
   "metadata": {},
   "source": [
    "### 3. An√°lisis de Estabilidad Temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f93d89",
   "metadata": {},
   "source": [
    "El Test de Chow compara la suma de los errores cuadrados (RSS) del modelo estimado en todo el per√≠odo (Modelo Restringido) contra la suma de los errores cuadrados de los modelos estimados por separado en dos subper√≠odos (Modelos No Restringidos).\n",
    "\n",
    "Punto de Quiebre (t \n",
    "‚àó\n",
    " ): Utilizaremos la transici√≥n de Octubre a Noviembre (despu√©s del 31 de octubre) para dividir la muestra en dos mitades.\n",
    "\n",
    "Hip√≥tesis Nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ): Los coeficientes son iguales en ambos per√≠odos (Hay estabilidad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83aa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import f\n",
    "\n",
    "# Creamos la matriz X completa\n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X = sm.add_constant(X_full_data, has_constant='add')\n",
    "k = X.shape[1] # N√∫mero de coeficientes (incluye la constante)\n",
    "N_total = len(df_clean)\n",
    "\n",
    "# --- 2. MODELO RESTRINGIDO (R) - Full Data ---\n",
    "model_R = sm.OLS(y, X).fit()\n",
    "RSS_R = model_R.ssr\n",
    "\n",
    "# Punto de Quiebre: Despu√©s del 31 de Octubre\n",
    "split_date = pd.to_datetime('2024-10-31') \n",
    "\n",
    "# Per√≠odo 1: Septiembre y Octubre\n",
    "mask_1 = df_clean['F.Salida'] <= split_date\n",
    "X1 = X[mask_1]\n",
    "y1 = y[mask_1]\n",
    "model_1 = sm.OLS(y1, X1).fit()\n",
    "RSS_1 = model_1.ssr\n",
    "N1 = len(y1)\n",
    "\n",
    "# Per√≠odo 2: Noviembre y Diciembre\n",
    "mask_2 = df_clean['F.Salida'] > split_date\n",
    "X2 = X[mask_2]\n",
    "y2 = y[mask_2]\n",
    "N2 = len(y2)\n",
    "\n",
    "# Comprobaci√≥n de suficiencia de datos antes del c√°lculo\n",
    "if N1 <= k or N2 <= k:\n",
    "    print(f\"ERROR: No hay suficientes observaciones en uno de los subper√≠odos (N1={N1}, N2={N2}) para estimar {k} coeficientes.\")\n",
    "else:\n",
    "    model_2 = sm.OLS(y2, X2).fit()\n",
    "    RSS_2 = model_2.ssr\n",
    "    \n",
    "    # --- 4. C√ÅLCULO DEL ESTAD√çSTICO DE CHOW ---\n",
    "    \n",
    "    numerator = (RSS_R - (RSS_1 + RSS_2)) / k\n",
    "    denominator = (RSS_1 + RSS_2) / (N_total - 2 * k)\n",
    "    \n",
    "    F_statistic = numerator / denominator\n",
    "\n",
    "    # Valor p: usando la distribuci√≥n F\n",
    "    p_value = f.sf(F_statistic, k, N_total - 2 * k)\n",
    "\n",
    "    # --- 5. RESULTADOS ---\n",
    "    alpha = 0.05\n",
    "\n",
    "    print(\"--- Resultados del Test de Chow (Estabilidad Temporal) ---\")\n",
    "    print(f\"Punto de Quiebre: {split_date.strftime('%Y-%m-%d')}\")\n",
    "    print(f\"N√∫mero de Coeficientes (k): {k}\")\n",
    "    print(f\"Estad√≠stico F: {F_statistic:.4f}\")\n",
    "    print(f\"Valor P: {p_value:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd761d1d",
   "metadata": {},
   "source": [
    "Inestabilidad Confirmada: El Valor P de 0.0003 es significativamente menor que el nivel de significancia de 0.05. Por lo tanto, rechazamos la hip√≥tesis nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ).\n",
    "\n",
    "Cambio Estructural: Hay evidencia estad√≠stica fuerte de inestabilidad estructural en el modelo. Esto implica que la relaci√≥n entre las variables de control (Peso, Servicio, Rutas) y el Costo, as√≠ como el Premium/Descuento de los Transportistas, cambiaron significativamente despu√©s del 31 de octubre.\n",
    "\n",
    "Discusi√≥n Requerida: Este hallazgo obliga a una discusi√≥n profunda en la interpretaci√≥n final:\n",
    "\n",
    "Causa Potencial: El cambio estructural puede estar impulsado por eventos estacionales (preparaci√≥n para la temporada alta de fin de a√±o en Noviembre/Diciembre), cambios en la pol√≠tica de precios de la empresa, o la renegociaci√≥n de tarifas con los transportistas clave.\n",
    "\n",
    "Modelo Final: Sugiere que un √∫nico modelo para todo el per√≠odo puede ser inadecuado. El an√°lisis deber√≠a enfocarse en el per√≠odo m√°s reciente (Nov-Dic) o considerar la modelaci√≥n con coeficientes variables en el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053978e",
   "metadata": {},
   "source": [
    "### 4.Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81741f",
   "metadata": {},
   "source": [
    "Estimar los intervalos de confianza del 95% para los coeficientes clave, como el Peso y el Transportista clave (Carrier_100139.0), mediante remuestreo para verificar si los resultados se mantienen significativos bajo un enfoque no param√©trico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Creamos la matriz X completa \n",
    "X_full_data = pd.concat([\n",
    "    df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']],\n",
    "    route_dummies,\n",
    "    carrier_dummies\n",
    "], axis=1).astype(float)\n",
    "X = sm.add_constant(X_full_data, has_constant='add')\n",
    "\n",
    "N = len(X)\n",
    "n_iterations = 500\n",
    "\n",
    "# Identificar Carrier Clave y su √≠ndice (asumiendo Carrier_100139.0)\n",
    "carrier_name = 'Carrier_100139.0'\n",
    "carrier_col_index = X_full_data.columns.get_loc(carrier_name)\n",
    "carrier_idx = 1 + carrier_col_index \n",
    "\n",
    "# √çndice de la variable Peso\n",
    "peso_idx = 1\n",
    "\n",
    "# --- 2. MODELO BASE (Para obtener CIs est√°ndar y coeficiente) ---\n",
    "model_base = sm.OLS(y, X).fit()\n",
    "\n",
    "# --- 3. EJECUCI√ìN DEL BOOTSTRAP DE PARES (C√ìDIGO CORREGIDO) ---\n",
    "boot_coeffs = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Remuestrear √≠ndices con reemplazo (Pairs Bootstrap)\n",
    "    sample_indices = np.random.choice(N, size=N, replace=True)\n",
    "    \n",
    "    X_sample = X.iloc[sample_indices]\n",
    "    y_sample = y[sample_indices] # <--- CORRECCI√ìN APLICADA AQU√ç\n",
    "\n",
    "    \n",
    "    try:\n",
    "        model_boot = sm.OLS(y_sample, X_sample).fit()\n",
    "        \n",
    "        # Almacenar los coeficientes de inter√©s\n",
    "        boot_coeffs.append({\n",
    "            'Peso_Coef': model_boot.params[peso_idx],\n",
    "            'Carrier_Coef': model_boot.params[carrier_idx]\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "boot_df = pd.DataFrame(boot_coeffs)\n",
    "\n",
    "# --- 4. C√ÅLCULO DE INTERVALOS DE CONFIANZA DEL BOOTSTRAP (Percentil) ---\n",
    "\n",
    "# CIs de Bootstrap (Percentil 2.5 y 97.5)\n",
    "peso_ci_boot = boot_df['Peso_Coef'].quantile([0.025, 0.975]).tolist()\n",
    "carrier_ci_boot = boot_df['Carrier_Coef'].quantile([0.025, 0.975]).tolist()\n",
    "\n",
    "# Resultados del modelo base para comparaci√≥n (CIs est√°ndar)\n",
    "# Convertir model_base.conf_int() a DataFrame si es necesario para usar .iloc\n",
    "ci_ols_df = model_base.conf_int()\n",
    "peso_ci_ols = ci_ols_df.iloc[peso_idx].tolist()\n",
    "carrier_ci_ols = ci_ols_df.iloc[carrier_idx].tolist()\n",
    "carrier_coef_ols = model_base.params[carrier_idx]\n",
    "\n",
    "# Crear tabla de resultados\n",
    "results_table = pd.DataFrame({\n",
    "    'Coeficiente': ['Peso Total (kg)', carrier_name],\n",
    "    'Estimaci√≥n OLS': [model_base.params[peso_idx], carrier_coef_ols],\n",
    "    'CI OLS (95%)': [f\"[{peso_ci_ols[0]:.2f}, {peso_ci_ols[1]:.2f}]\", \n",
    "                     f\"[{carrier_ci_ols[0]:.2f}, {carrier_ci_ols[1]:.2f}]\"],\n",
    "    'CI Bootstrap (95%)': [f\"[{peso_ci_boot[0]:.2f}, {peso_ci_boot[1]:.2f}]\", \n",
    "                           f\"[{carrier_ci_boot[0]:.2f}, {carrier_ci_boot[1]:.2f}]\"]\n",
    "})\n",
    "\n",
    "print(\"\\n--- Resultados de Bootstrap (Intervalos de Confianza) ---\")\n",
    "print(f\"N√∫mero de iteraciones v√°lidas: {len(boot_df)}\")\n",
    "print(results_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90617efc",
   "metadata": {},
   "source": [
    "Robustez del Transportista (Hallazgo Clave):\n",
    "\n",
    "El Intervalo de Confianza del Bootstrap para el transportista clave es [‚àí2,232.51,‚àí1,294.25]. Este intervalo no contiene el cero, lo que confirma que el descuento promedio de $‚àí1,794.87 es estad√≠sticamente significativo al 95% bajo un enfoque de inferencia no param√©trica.\n",
    "\n",
    "El CI Bootstrap es m√°s estrecho que el CI OLS tradicional, indicando que el Premium es altamente preciso y robusto, incluso al considerar la variabilidad de la muestra.\n",
    "\n",
    "Incertidumbre del Peso:\n",
    "\n",
    "El coeficiente del Peso Total (kg) (0.03516) se encuentra en un √°rea de marginalidad. Mientras que el CI OLS apenas excluye el cero, el CI Bootstrap [‚àí0.00,0.07] apenas lo incluye, indicando que la contribuci√≥n del peso al costo no es estad√≠sticamente significativa con un nivel de confianza estricto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70bab13",
   "metadata": {},
   "source": [
    "### 5. Diagn√≥sticos Espec√≠ficos del M√©todo\n",
    "\n",
    "#### 5.1 Multicolinealidad (Factor de Inflaci√≥n de la Varianza - VIF)\n",
    "\n",
    "La multicolinealidad existe cuando las variables predictoras est√°n altamente correlacionadas entre s√≠. Un VIF alto (VIF>5 o 10) infla los errores est√°ndar, lo que dificulta determinar la contribuci√≥n individual de cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcb184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_vif(df):\n",
    "    # Convertir a float expl√≠citamente y manejar NaNs (CORRECCI√ìN CLAVE)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce').replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame({'Variable': ['N/A'], 'VIF': [np.nan]})\n",
    "\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = df.columns\n",
    "    \n",
    "    # A√±adir constante temporalmente para el c√°lculo de VIF (necesario para la f√≥rmula)\n",
    "    X_temp = sm.add_constant(df, prepend=True)\n",
    "    \n",
    "    # Calcular VIF para cada variable\n",
    "    vif_list = []\n",
    "    try:\n",
    "        for i in range(X_temp.shape[1]):\n",
    "            vif_val = variance_inflation_factor(X_temp.values, i)\n",
    "            vif_list.append(vif_val)\n",
    "        \n",
    "        vif_data[\"VIF\"] = vif_list[1:] # Excluir el VIF de la constante (√≠ndice 0)\n",
    "        vif_data[\"Variable\"] = df.columns\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Advertencia: Error al calcular VIF. {e}\")\n",
    "        vif_data[\"VIF\"] = [np.nan] * len(df.columns)\n",
    "        \n",
    "    return vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "\n",
    "# Variables principales (Peso y Servicio)\n",
    "main_vars = df_clean[['Peso Total (kg)', 'Es_Express', 'Es_Revestidos']].astype(float)\n",
    "vif_main = calculate_vif(main_vars)\n",
    "\n",
    "carrier_sample_cols = list(carrier_dummies.columns[:2])\n",
    "route_sample_cols = list(route_dummies.columns[:3])\n",
    "\n",
    "X_dummy_sample = pd.concat([main_vars] + \n",
    "                           [carrier_dummies[col] for col in carrier_sample_cols if col in carrier_dummies.columns] + \n",
    "                           [route_dummies[col] for col in route_sample_cols if col in route_dummies.columns], \n",
    "                           axis=1).astype(float)\n",
    "vif_dummies = calculate_vif(X_dummy_sample)\n",
    "\n",
    "\n",
    "print(\"\\n--- Resultados de VIF (Variables Principales) ---\")\n",
    "print(vif_main.to_string(index=False, float_format='%.2f'))\n",
    "print(\"\\n--- Resultados de VIF (Muestra de Dummies) ---\")\n",
    "print(vif_dummies.to_string(index=False, float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7bea7",
   "metadata": {},
   "source": [
    "Variables Principales: Los valores VIF para todas las variables continuas, binarias y la muestra de dummies son extremadamente bajos (VIF<2).\n",
    "\n",
    "Conclusi√≥n: Esto indica que no existe un problema significativo de multicolinealidad. La contribuci√≥n individual de cada variable se puede medir de forma fiable, garantizando que los coeficientes del Peso y del Transportista no est√°n sesgados por la alta correlaci√≥n con otros predictores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244074c3",
   "metadata": {},
   "source": [
    "#### 5.2 Heterocedasticidad (Test de Breusch-Pagan)\n",
    "\n",
    "Verificar si la varianza de los residuos del modelo es constante (homocedasticidad). La heterocedasticidad (varianza no constante) es muy com√∫n en an√°lisis de costos, y si est√° presente, hace que los errores est√°ndar de OLS sean incorrectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a9dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.api import het_breuschpagan\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Obtener los residuos y las variables explicativas\n",
    "resid = model_base.resid\n",
    "exog = model_base.model.exog # Usar las variables explicativas que se usaron para ajustar el modelo\n",
    "\n",
    "# Ejecutar el test de Breusch-Pagan\n",
    "# El test de Breusch-Pagan prueba la hip√≥tesis nula de homocedasticidad.\n",
    "bp_test_results = het_breuschpagan(resid, exog)\n",
    "\n",
    "print(\"\\n--- Resultados del Test de Breusch-Pagan (Heterocedasticidad) ---\")\n",
    "print(f\"Estad√≠stico LM: {bp_test_results[0]:.4f}\")\n",
    "print(f\"P-valor LM: {bp_test_results[1]:.4f}\")\n",
    "print(f\"Estad√≠stico F: {bp_test_results[2]:.4f}\")\n",
    "print(f\"P-valor F: {bp_test_results[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f991f4",
   "metadata": {},
   "source": [
    "Hip√≥tesis Nula (H \n",
    "0\n",
    "‚Äã\t\n",
    " ): Homocedasticidad (varianza constante).\n",
    "\n",
    "Conclusi√≥n: Dado que el P-valor (tanto LM como F) es 0.0000 (menor a 0.05), rechazamos firmemente la hip√≥tesis nula.\n",
    "\n",
    "Implicaci√≥n: Se confirma la presencia de heterocedasticidad. Esto significa que los errores del modelo son mayores para ciertos niveles de costo o variables explicativas. Los errores est√°ndar tradicionales de OLS son incorrectos y, por lo tanto, los P-valores de tus coeficientes (incluyendo el del transportista) son inv√°lidos para la inferencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105a54b",
   "metadata": {},
   "source": [
    "#### 5.3 Autocorrelaci√≥n (Test de Durbin-Watson)\n",
    "\n",
    "Verificar si los errores del modelo est√°n correlacionados en el tiempo (autocorrelaci√≥n), lo cual es crucial en datos de series de tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Se usa el modelo base (OLS) ajustado\n",
    "# El test de Durbin-Watson opera sobre los residuos del modelo\n",
    "dw_statistic = durbin_watson(model_base.resid)\n",
    "\n",
    "print(\"\\n--- Resultados del Test de Durbin-Watson (Autocorrelaci√≥n) ---\")\n",
    "print(f\"Estad√≠stico Durbin-Watson: {dw_statistic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68315c52",
   "metadata": {},
   "source": [
    "Interpretaci√≥n: El Estad√≠stico DW se encuentra muy cerca de 2.0, el valor ideal que indica la ausencia de autocorrelaci√≥n de primer orden en los residuos.\n",
    "\n",
    "Conclusi√≥n: Se concluye que la autocorrelaci√≥n serial no es una preocupaci√≥n principal para la inferencia estad√≠stica del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8febfd4",
   "metadata": {},
   "source": [
    "La confirmaci√≥n de la Heterocedasticidad y la Inestabilidad Temporal exigen una correcci√≥n para garantizar que los P-valores de los coeficientes sean v√°lidos y fiables.\n",
    "\n",
    "El modelo final para la inferencia debe ser estimado utilizando Errores Est√°ndar Robustos a la Heterocedasticidad (Huber-White). Dada la estructura de datos de panel y series de tiempo, la opci√≥n m√°s segura es utilizar Errores Est√°ndar HAC (Heteroskedasticity and Autocorrelation Consistent) (Newey-West), ya que corrigen la heterocedasticidad y cualquier potencial autocorrelaci√≥n que el Test DW pudo haber fallado en detectar completamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbce76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "params_series = pd.Series(model_full.params, index=['const'] + list(X_full_data.columns))\n",
    "carrier_params = params_series[params_series.index.str.contains('Carrier_')]\n",
    "\n",
    "best_carrier_coef = carrier_params.min()\n",
    "best_carrier_id = carrier_params.idxmin() \n",
    "try:\n",
    "    carrier_id_str = best_carrier_id.replace('Carrier_', '')\n",
    "    carrier_id_float = float(carrier_id_str) \n",
    "    carrier_id_int = int(carrier_id_float) \n",
    "\n",
    "    if 'carrier_names_map' in locals():\n",
    "        best_carrier_name_real = carrier_names_map.get(carrier_id_int, f\"ID {carrier_id_int}\")\n",
    "    elif 'carrier_stats' in locals():\n",
    "         best_carrier_name_real = carrier_stats[carrier_stats['Carrier_ID'] == carrier_id_int]['Carrier_Name'].iloc[0]\n",
    "    else:\n",
    "        best_carrier_name_real = best_carrier_id\n",
    "except Exception as e:\n",
    "    best_carrier_name_real = best_carrier_id \n",
    "\n",
    "print(f\"üèÜ Mejor Transportista: {best_carrier_name_real} ({best_carrier_id})\")\n",
    "print(f\"   Descuento aplicado por viaje: ${best_carrier_coef:,.2f}\")\n",
    "\n",
    "current_carrier_costs = (carrier_dummies * carrier_params[carrier_dummies.columns].values).sum(axis=1)\n",
    "\n",
    "counterfactual_carrier_costs = best_carrier_coef \n",
    "\n",
    "savings_per_trip = current_carrier_costs - counterfactual_carrier_costs\n",
    "total_savings = savings_per_trip.sum()\n",
    "\n",
    "actual_total_cost = np.sum(y) \n",
    "new_total_cost = actual_total_cost - total_savings\n",
    "percent_saving = (total_savings / actual_total_cost) * 100\n",
    "\n",
    "print(\"\\n--- RESULTADOS ESCENARIO 1 ---\")\n",
    "print(f\"Costo Total Actual:       ${actual_total_cost:,.2f}\")\n",
    "print(f\"Costo Simulado (Best):    ${new_total_cost:,.2f}\")\n",
    "print(f\"AHORRO TOTAL POTENCIAL:   ${total_savings:,.2f}\")\n",
    "print(f\"Porcentaje de Ahorro:     {percent_saving:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if not isinstance(model_full.params, pd.Series):\n",
    "    params_series = pd.Series(model_full.params, index=['const'] + list(X_full_data.columns))\n",
    "else:\n",
    "    params_series = model_full.params\n",
    "\n",
    "route_params = params_series[params_series.index.str.contains('Route_')]\n",
    "\n",
    "inefficient_routes = route_params.nlargest(5)\n",
    "\n",
    "print(\" Top 5 Rutas M√°s Ineficientes (Sobrecosto por viaje):\")\n",
    "print(inefficient_routes)\n",
    "\n",
    "target_route_dummies = route_dummies[inefficient_routes.index]\n",
    "savings_per_trip_s2 = target_route_dummies.dot(inefficient_routes)\n",
    "\n",
    "total_savings_s2 = savings_per_trip_s2.sum()\n",
    "actual_total_cost = np.sum(y)\n",
    "new_total_cost_s2 = actual_total_cost - total_savings_s2\n",
    "percent_saving_s2 = (total_savings_s2 / actual_total_cost) * 100\n",
    "\n",
    "s2_low = total_savings_s2 * 0.95\n",
    "s2_high = total_savings_s2 * 1.05\n",
    "\n",
    "print(\"\\n--- RESULTADOS ESCENARIO 2 ---\")\n",
    "print(f\"Rutas Optimizadas:        {len(inefficient_routes)}\")\n",
    "print(f\"Viajes Afectados:         {int((savings_per_trip_s2 > 0).sum())}\")\n",
    "print(f\"AHORRO TOTAL POTENCIAL:   ${total_savings_s2:,.2f}\")\n",
    "print(f\"Porcentaje de Ahorro:     {percent_saving_s2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "median_cost = carrier_params.median()\n",
    "\n",
    "expensive_carriers = carrier_params[carrier_params > median_cost].index\n",
    "efficient_carriers = carrier_params[carrier_params <= median_cost].index\n",
    "\n",
    "print(f\"Umbres de Costo (Mediana): ${median_cost:,.2f}\")\n",
    "print(f\"Transportistas 'Caros':      {len(expensive_carriers)}\")\n",
    "print(f\"Transportistas 'Eficientes': {len(efficient_carriers)}\")\n",
    "\n",
    "# Volumen de viajes por transportista\n",
    "carrier_volumes = carrier_dummies.sum()\n",
    "\n",
    "# Promedio ponderado del grupo \"Caro\"\n",
    "vol_expensive = carrier_volumes[expensive_carriers].sum()\n",
    "avg_cost_expensive = (carrier_params[expensive_carriers] * carrier_volumes[expensive_carriers]).sum() / vol_expensive\n",
    "\n",
    "# Promedio ponderado del grupo \"Eficiente\"\n",
    "vol_efficient = carrier_volumes[efficient_carriers].sum()\n",
    "avg_cost_efficient = (carrier_params[efficient_carriers] * carrier_volumes[efficient_carriers]).sum() / vol_efficient\n",
    "\n",
    "print(f\"Costo Promedio 'Caros':      ${avg_cost_expensive:,.2f}\")\n",
    "print(f\"Costo Promedio 'Eficientes': ${avg_cost_efficient:,.2f}\")\n",
    "print(f\"Diferencia (Ahorro por viaje reasignado): ${avg_cost_expensive - avg_cost_efficient:,.2f}\")\n",
    "\n",
    "# 3. Simulaci√≥n: Mover X% del volumen \"Caro\" al grupo \"Eficiente\"\n",
    "shift_percentage = 0.30  # Mover el 30% de la carga\n",
    "trips_to_shift = int(vol_expensive * shift_percentage)\n",
    "\n",
    "# Ahorro Total = Viajes Reasignados * Ahorro por Viaje\n",
    "total_savings_s3 = trips_to_shift * (avg_cost_expensive - avg_cost_efficient)\n",
    "\n",
    "# Totales\n",
    "actual_total_cost = np.sum(y)\n",
    "new_total_cost_s3 = actual_total_cost - total_savings_s3\n",
    "percent_saving_s3 = (total_savings_s3 / actual_total_cost) * 100\n",
    "\n",
    "# Intervalo de incertidumbre (+/- 10% porque depende de la disponibilidad real)\n",
    "s3_low = total_savings_s3 * 0.90\n",
    "s3_high = total_savings_s3 * 1.10\n",
    "\n",
    "print(\"\\n--- RESULTADOS ESCENARIO 3 ---\")\n",
    "print(f\"Pol√≠tica:                 Reasignar {shift_percentage*100}% del volumen caro\")\n",
    "print(f\"Viajes Reasignados:       {trips_to_shift}\")\n",
    "print(f\"AHORRO TOTAL POTENCIAL:   ${total_savings_s3:,.2f}\")\n",
    "print(f\"Porcentaje de Ahorro:     {percent_saving_s3:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configuraci√≥n general de estilo\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# ==========================================\n",
    "# GR√ÅFICO 1: EL TECHO T√âCNICO (Escenario 1)\n",
    "# Comparaci√≥n de Costo Total: Actual vs Ideal\n",
    "# ==========================================\n",
    "labels_s1 = ['Costo Actual', 'Costo Simulado\\n(Best-in-Class)']\n",
    "values_s1 = [230.3, 203.4] # Millones MXN\n",
    "ahorro_s1 = 26.9\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars1 = plt.bar(labels_s1, values_s1, color=['#7f7f7f', '#2ca02c'], width=0.6)\n",
    "plt.title('Escenario 1: Potencial M√°ximo de Ahorro', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.ylabel('Costo Total (Millones MXN)')\n",
    "plt.ylim(0, 260)\n",
    "\n",
    "# Etiquetas\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "             f'${height:.1f}M', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Flecha de ahorro\n",
    "plt.annotate(f'Ahorro:\\n-${ahorro_s1}M (-11.7%)', \n",
    "             xy=(1, 203.4), xytext=(0.5, 220),\n",
    "             arrowprops=dict(facecolor='black', arrowstyle='->', connectionstyle=\"arc3,rad=.2\"),\n",
    "             fontsize=11, ha='center', color='darkgreen', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scenario1_benchmark.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# GR√ÅFICO 2: RUTAS CR√çTICAS (Escenario 2)\n",
    "# Sobrecosto Estructural por Ruta (Top 5)\n",
    "# ==========================================\n",
    "# Usamos los datos reales de tus rutas ineficientes\n",
    "routes = ['NL - Juarez', 'NL - Puebla', 'NL - Tepeapulco', 'COA - Juarez', 'NL - CDMX']\n",
    "# Estos son los coeficientes (sobrecosto por viaje) que me pasaste\n",
    "overcosts = [47099, 42599, 38053, 35226, 34819] \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars2 = plt.barh(routes, overcosts, color='#d62728')\n",
    "plt.title('Escenario 2: Sobrecosto Estructural por Viaje (Top 5 Rutas)', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.xlabel('Sobrecosto Unitario vs Promedio (MXN)')\n",
    "plt.gca().invert_yaxis() # Para que la #1 quede arriba\n",
    "\n",
    "# Etiquetas dentro de las barras\n",
    "for bar in bars2:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width - 2000, bar.get_y() + bar.get_height()/2, \n",
    "             f'+${width:,.0f}', \n",
    "             va='center', ha='right', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scenario2_routes.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# GR√ÅFICO 3: ARBITRAJE DE COSTOS (Escenario 3)\n",
    "# Diferencia de Precio Promedio: Grupo Caro vs Eficiente\n",
    "# ==========================================\n",
    "groups = ['Grupo Costoso', 'Grupo Eficiente']\n",
    "avg_costs = [3604, -634] # Datos reales que calculaste\n",
    "gap = 3604 - (-634)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars3 = plt.bar(groups, avg_costs, color=['#ff7f0e', '#1f77b4'], width=0.6)\n",
    "plt.axhline(0, color='black', linewidth=0.8)\n",
    "plt.title('Escenario 3: Disparidad de Costos (Oportunidad de Asignaci√≥n)', fontsize=14, fontweight='bold', pad=15)\n",
    "plt.ylabel('Costo Promedio Residual por Viaje (MXN)')\n",
    "\n",
    "# Etiquetas\n",
    "plt.text(0, 3604 + 200, '+$3,604', ha='center', fontweight='bold', color='#ff7f0e')\n",
    "plt.text(1, -634 - 400, '-$634', ha='center', fontweight='bold', color='#1f77b4')\n",
    "\n",
    "# L√≠nea de brecha\n",
    "plt.plot([0, 1], [3604, 3604], color='gray', linestyle='--')\n",
    "plt.plot([1, 1], [3604, -634], color='gray', linestyle='--')\n",
    "plt.text(1.1, 1500, f'Gap de Ahorro:\\n${gap:,.0f} / viaje', va='center', color='gray', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scenario3_allocation.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"¬°Listo! Se generaron 3 im√°genes: scenario1_benchmark.png, scenario2_routes.png, scenario3_allocation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE PARETO: RUTAS M√ÅS SOLICITADAS\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Calcular frecuencia de viajes por ruta\n",
    "# Usamos 'Ruta_ID' que creaste en el feature engineering\n",
    "route_counts = df_clean['Ruta_ID'].value_counts().reset_index()\n",
    "route_counts.columns = ['Ruta', 'Frecuencia']\n",
    "\n",
    "# Calcular porcentaje acumulado (Pareto)\n",
    "route_counts['Porcentaje'] = (route_counts['Frecuencia'] / route_counts['Frecuencia'].sum()) * 100\n",
    "route_counts['Acumulado'] = route_counts['Porcentaje'].cumsum()\n",
    "\n",
    "# Seleccionar el Top 15 para el gr√°fico\n",
    "top_routes = route_counts.head(15)\n",
    "\n",
    "# --- 2. VISUALIZACI√ìN ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Frecuencia', y='Ruta', data=top_routes, palette='viridis')\n",
    "\n",
    "plt.title('Top 15 Rutas M√°s Frecuentes (Demanda de Viajes)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('N√∫mero de Viajes', fontsize=12)\n",
    "plt.ylabel('Ruta (Origen - Destino)', fontsize=12)\n",
    "plt.grid(axis='x', linestyle=':', alpha=0.6)\n",
    "\n",
    "# A√±adir etiquetas de valor al final de las barras\n",
    "for index, value in enumerate(top_routes['Frecuencia']):\n",
    "    plt.text(value + 5, index, str(value), va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_rutas_frecuentes.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# --- 3. DATOS PARA EL REPORTE ---\n",
    "top_5_share = route_counts.head(5)['Porcentaje'].sum()\n",
    "top_10_share = route_counts.head(10)['Porcentaje'].sum()\n",
    "\n",
    "print(f\"\\n--- ESTAD√çSTICAS DE CONCENTRACI√ìN ---\")\n",
    "print(f\"Total de Rutas √önicas: {len(route_counts)}\")\n",
    "print(f\"El Top 5 de rutas concentra el {top_5_share:.1f}% de todos los viajes.\")\n",
    "print(f\"El Top 10 de rutas concentra el {top_10_share:.1f}% de todos los viajes.\")\n",
    "print(\"\\nListado del Top 5:\")\n",
    "print(top_routes.head(5)[['Ruta', 'Frecuencia', 'Porcentaje']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe4920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ef63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Datos del Business Case (Estimados en la secci√≥n anterior)\n",
    "modes = ['Actual (Cami√≥n FTL)', 'Propuesta (Intermodal)']\n",
    "\n",
    "# Desglose de costos\n",
    "base_cost = [20000, 40300]      # Flete Base: El tren es m√°s caro en tarifa base\n",
    "inefficiency = [47099, 0]       # El cami√≥n tiene el sobrecosto de ruta, el tren no.\n",
    "last_mile = [0, 8000]           # El tren requiere cami√≥n local (Drayage)\n",
    "\n",
    "# Colores corporativos/serios\n",
    "colors = ['#bfbfbf', '#d62728', '#2ca02c'] # Gris, Rojo (Ineficiencia), Verde (Last Mile)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Crear barras apiladas\n",
    "p1 = ax.bar(modes, base_cost, label='Tarifa Base', color='#1f77b4', width=0.5)\n",
    "p2 = ax.bar(modes, inefficiency, bottom=base_cost, label='Sobrecosto Ruta (Ineficiencia)', color='#d62728', width=0.5)\n",
    "p3 = ax.bar(modes, last_mile, bottom=np.array(base_cost)+np.array(inefficiency), label='√öltima Milla (Drayage)', color='#2ca02c', width=0.5)\n",
    "\n",
    "# Etiquetas de totales\n",
    "totals = [67100, 48300]\n",
    "savings = 67100 - 48300\n",
    "percent = (savings / 67100) * 100\n",
    "\n",
    "# Poner los totales arriba de las barras\n",
    "for i, total in enumerate(totals):\n",
    "    ax.text(i, total + 1500, f'${total:,.0f}', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Flecha de ahorro\n",
    "ax.annotate(f'Ahorro por Viaje:\\n${savings:,.0f} (-{percent:.0f}%)', \n",
    "            xy=(1, 48300), xytext=(0.5, 60000),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->', connectionstyle=\"arc3,rad=.2\"),\n",
    "            fontsize=11, ha='center', color='darkgreen', fontweight='bold')\n",
    "\n",
    "# Formato\n",
    "ax.set_ylabel('Costo Total por Viaje (MXN)')\n",
    "ax.set_title('An√°lisis de Cambio Modal: Ruta NL - Ju√°rez', fontweight='bold', pad=15)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 80000)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('modal_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68ca9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE FRECUENCIA: ¬øCADA CU√ÅNTO SALE UN CAMI√ìN?\n",
    "# ======================================================================\n",
    "\n",
    "# Rutas a analizar (Las cr√≠ticas que identificamos)\n",
    "target_routes = ['NL - Juarez', 'NL - √Årea Metro Puebla', 'COA - Juarez']\n",
    "\n",
    "print(\"--- AN√ÅLISIS DE FRECUENCIA DE SALIDAS ---\\n\")\n",
    "\n",
    "for ruta in target_routes:\n",
    "    # 1. Filtrar datos de la ruta y ordenar por fecha\n",
    "    # Asumimos que 'Ruta_ID' es la columna de origen-destino\n",
    "    trips = df_clean[df_clean['Ruta_ID'] == ruta].sort_values('F.Salida').copy()\n",
    "    \n",
    "    if len(trips) > 1:\n",
    "        # 2. Calcular tiempo entre viajes consecutivos (Gap)\n",
    "        trips['Prev_Salida'] = trips['F.Salida'].shift(1)\n",
    "        trips['Gap_Horas'] = (trips['F.Salida'] - trips['Prev_Salida']).dt.total_seconds() / 3600\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        avg_gap = trips['Gap_Horas'].mean()\n",
    "        median_gap = trips['Gap_Horas'].median()\n",
    "        trips_per_week = len(trips) / ( (trips['F.Salida'].max() - trips['F.Salida'].min()).days / 7 )\n",
    "        \n",
    "        print(f\"RUTA: {ruta}\")\n",
    "        print(f\"   Total Viajes: {len(trips)}\")\n",
    "        print(f\"   Salidas por Semana (Promedio): {trips_per_week:.1f}\")\n",
    "        print(f\"   Tiempo Promedio entre salidas: {avg_gap:.1f} horas ({avg_gap/24:.1f} d√≠as)\")\n",
    "        print(f\"   Mediana de espera:             {median_gap:.1f} horas\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    else:\n",
    "        print(f\"RUTA: {ruta} - Datos insuficientes ({len(trips)} viajes)\")\n",
    "\n",
    "# --- VISUALIZACI√ìN: D√çAS DE SALIDA ---\n",
    "# Ver si hay un patr√≥n semanal (ej. ¬øSolo salen los viernes?)\n",
    "subset = df_clean[df_clean['Ruta_ID'].isin(target_routes)].copy()\n",
    "subset['Dia_Semana'] = subset['F.Salida'].dt.day_name()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=subset, x='Dia_Semana', hue='Ruta_ID', \n",
    "              order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title('Patr√≥n de Salidas por D√≠a de la Semana', fontweight='bold')\n",
    "plt.ylabel('N√∫mero de Viajes')\n",
    "plt.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc9a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE FRECUENCIA REAL: SOLO VIAJES STANDARD (NO EXPRESS)\n",
    "# ======================================================================\n",
    "\n",
    "# 1. FILTRO CR√çTICO: Solo analizamos lo que se puede \"aguantar\" (Standard)\n",
    "# Asumimos que 'Es_Express' == 0 significa Standard\n",
    "standard_trips = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "\n",
    "print(f\"Total Viajes Standard en la red: {len(standard_trips)}\")\n",
    "print(\"--- FRECUENCIA DE SALIDAS (EXCLUYENDO EXPRESS) ---\\n\")\n",
    "\n",
    "target_routes = ['NL - Juarez', 'NL - √Årea Metro Puebla', 'COA - Juarez']\n",
    "\n",
    "for ruta in target_routes:\n",
    "    # Filtrar datos de la ruta y ordenar por fecha\n",
    "    trips = standard_trips[standard_trips['Ruta_ID'] == ruta].sort_values('F.Salida').copy()\n",
    "    \n",
    "    if len(trips) > 1:\n",
    "        # Calcular tiempo entre viajes consecutivos\n",
    "        trips['Prev_Salida'] = trips['F.Salida'].shift(1)\n",
    "        trips['Gap_Horas'] = (trips['F.Salida'] - trips['Prev_Salida']).dt.total_seconds() / 3600\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        avg_gap = trips['Gap_Horas'].mean()\n",
    "        median_gap = trips['Gap_Horas'].median()\n",
    "        # D√≠as del periodo analizado (aprox)\n",
    "        days_span = (trips['F.Salida'].max() - trips['F.Salida'].min()).days\n",
    "        trips_per_week = len(trips) / (days_span / 7) if days_span > 0 else 0\n",
    "        \n",
    "        print(f\"RUTA: {ruta}\")\n",
    "        print(f\"   Total Viajes Standard: {len(trips)}\")\n",
    "        print(f\"   Salidas por Semana:    {trips_per_week:.1f}\")\n",
    "        print(f\"   Tiempo Promedio Gap:   {avg_gap:.1f} horas ({avg_gap/24:.1f} d√≠as)\")\n",
    "        print(f\"   Mediana de espera:     {median_gap:.1f} horas\")\n",
    "        \n",
    "        # Interpretaci√≥n autom√°tica\n",
    "        if median_gap <= 24:\n",
    "            print(\"   ‚úÖ VIABLE: Sale al menos un cami√≥n diario.\")\n",
    "        elif median_gap <= 48:\n",
    "            print(\"   ‚ö†Ô∏è RIESGO MEDIO: Sale cada 2 d√≠as. Esperar 24h podr√≠a retrasar 48h.\")\n",
    "        else:\n",
    "            print(\"   ‚ùå NO VIABLE: Frecuencia muy baja para estrategia de espera.\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "    else:\n",
    "        print(f\"RUTA: {ruta} - Datos insuficientes ({len(trips)} viajes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d9ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE CONSOLIDACI√ìN: ¬øVIAJAN LLENOS O VAC√çOS?\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Definir Capacidad Te√≥rica (Asumimos Caja de 53' = 24 Toneladas aprox)\n",
    "# Si tus datos tienen otro m√°ximo, el c√≥digo se ajustar√° al m√°ximo observado si es mayor.\n",
    "CAPACIDAD_KG = 24000 \n",
    "\n",
    "# Rutas Cr√≠ticas a analizar\n",
    "target_routes = ['NL - Juarez', 'NL - √Årea Metro Puebla', 'COA - Juarez']\n",
    "# Sus sobrecostos fijos (del modelo econom√©trico)\n",
    "route_overcosts = {\n",
    "    'NL - Juarez': 47099, \n",
    "    'NL - √Årea Metro Puebla': 42599,\n",
    "    'COA - Juarez': 35225\n",
    "}\n",
    "\n",
    "print(f\"--- AN√ÅLISIS DE EFICIENCIA DE CARGA (Capacidad ref: {CAPACIDAD_KG/1000} Tons) ---\")\n",
    "\n",
    "consolidation_data = []\n",
    "\n",
    "for ruta in target_routes:\n",
    "    # Filtrar viajes de esa ruta\n",
    "    trips = df_clean[df_clean['Ruta_ID'] == ruta].copy()\n",
    "    \n",
    "    if len(trips) > 0:\n",
    "        # Calcular % de Ocupaci√≥n\n",
    "        # Si hay pesos mayores a 24k, usamos el real, si no, el tope de 24k\n",
    "        trips['Ocupacion'] = trips['Peso Total (kg)'] / CAPACIDAD_KG * 100\n",
    "        # Capar al 100% para visualizaci√≥n (si hubo sobrepeso)\n",
    "        trips['Ocupacion_Vis'] = trips['Ocupacion'].clip(upper=100)\n",
    "        \n",
    "        # Calcular \"Costo de Ineficiencia por Kg\"\n",
    "        # Cu√°nto pagamos de sobrecosto por cada kilo real movido\n",
    "        overcost = route_overcosts.get(ruta, 0)\n",
    "        trips['Ineficiencia_x_Kg'] = overcost / trips['Peso Total (kg)']\n",
    "        \n",
    "        # Oportunidad de Consolidaci√≥n (Viajes < 60% ocupaci√≥n)\n",
    "        low_utilization = trips[trips['Ocupacion'] < 60]\n",
    "        potential_trips = len(low_utilization)\n",
    "        \n",
    "        print(f\"\\nRUTA: {ruta}\")\n",
    "        print(f\"   Viajes Totales: {len(trips)}\")\n",
    "        print(f\"   Peso Promedio:  {trips['Peso Total (kg)'].mean():,.0f} kg\")\n",
    "        print(f\"   Ocupaci√≥n Prom.: {trips['Ocupacion'].mean():.1f}%\")\n",
    "        print(f\"   Viajes < 60% carga: {potential_trips} ({potential_trips/len(trips)*100:.1f}%) -> CANDIDATOS A JUNTAR\")\n",
    "        print(f\"   Ineficiencia Promedio por Kg transportado: ${trips['Ineficiencia_x_Kg'].mean():.2f}/kg\")\n",
    "\n",
    "        consolidation_data.append(trips)\n",
    "\n",
    "# --- VISUALIZACI√ìN 1: HISTOGRAMA DE OCUPACI√ìN ---\n",
    "# ¬øQu√© tan llenos van los camiones?\n",
    "all_trips_viz = pd.concat(consolidation_data)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=all_trips_viz, x='Peso Total (kg)', hue='Ruta_ID', element=\"step\", bins=20, alpha=0.6)\n",
    "plt.axvline(CAPACIDAD_KG, color='red', linestyle='--', label='Capacidad M√°xima (24 Ton)')\n",
    "plt.axvline(CAPACIDAD_KG * 0.5, color='orange', linestyle='--', label='50% Capacidad (Punto de Fusi√≥n)')\n",
    "\n",
    "plt.title('Distribuci√≥n de Peso por Viaje en Rutas Cr√≠ticas', fontweight='bold')\n",
    "plt.xlabel('Peso Cargado (Kg)')\n",
    "plt.ylabel('Frecuencia de Viajes')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# --- C√ÅLCULO DE AHORRO POR CONSOLIDACI√ìN ---\n",
    "# Si juntamos dos viajes de <12 toneladas en uno solo, nos ahorramos UN SOBRECOSTO ENTERO ($47k)\n",
    "total_savings_consolidation = 0\n",
    "trips_eliminated = 0\n",
    "\n",
    "for ruta in target_routes:\n",
    "    trips = df_clean[df_clean['Ruta_ID'] == ruta]\n",
    "    # Viajes que son \"medio cami√≥n\" o menos\n",
    "    light_trips = trips[trips['Peso Total (kg)'] <= (CAPACIDAD_KG * 0.55)] # 55% como margen\n",
    "    \n",
    "    # Cada par de viajes ligeros = 1 viaje eliminado\n",
    "    pairs = int(len(light_trips) / 2)\n",
    "    \n",
    "    if pairs > 0:\n",
    "        route_saving = pairs * route_overcosts[ruta] # Ahorramos el flete completo del cami√≥n eliminado\n",
    "        total_savings_consolidation += route_saving\n",
    "        trips_eliminated += pairs\n",
    "        print(f\"   > {ruta}: Podr√≠amos fusionar {len(light_trips)} viajes ligeros en {len(light_trips)-pairs} viajes.\")\n",
    "        print(f\"     Ahorro: ${route_saving:,.0f} ({pairs} viajes eliminados)\")\n",
    "\n",
    "print(f\"\\nüí∞ AHORRO TOTAL POTENCIAL POR CONSOLIDACI√ìN: ${total_savings_consolidation:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bed0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE CAPACIDAD OCIOSA (\"FLETE FALSO EN KG\")\n",
    "# ======================================================================\n",
    "\n",
    "# 1. FILTRO: Solo viajes Standard (Los Express no se pueden consolidar por tiempo)\n",
    "df_ops = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "\n",
    "# 2. DEFINIR CAPACIDAD DE REFERENCIA\n",
    "# Asumimos una caja est√°ndar de 53 pies = 24,000 kg de capacidad √∫til\n",
    "CAPACIDAD_REF = 24000 \n",
    "\n",
    "# 3. CALCULAR \"KG DE FLETE FALSO\" (AIRE) POR VIAJE\n",
    "# Si el cami√≥n lleva 10,000 kg, lleva 14,000 kg de \"Flete Falso\" (espacio pagado no usado)\n",
    "# Si lleva m√°s de 24,000 (sobrepeso/full), el flete falso es 0.\n",
    "df_ops['Kg_Flete_Falso'] = (CAPACIDAD_REF - df_ops['Peso Total (kg)']).clip(lower=0)\n",
    "\n",
    "# Identificar viajes \"Consolidables\" (Carga < 50% de capacidad)\n",
    "df_ops['Es_Consolidable'] = df_ops['Peso Total (kg)'] <= (CAPACIDAD_REF * 0.55) # 55% margen\n",
    "\n",
    "# 4. AGRUPAR POR RUTA\n",
    "route_analysis = df_ops.groupby('Ruta_ID').agg({\n",
    "    'Viaje': 'count',\n",
    "    'Peso Total (kg)': 'mean',\n",
    "    'Kg_Flete_Falso': ['sum', 'mean'],\n",
    "    'Es_Consolidable': 'sum' # Cu√°ntos viajes iban a menos de la mitad\n",
    "}).reset_index()\n",
    "\n",
    "# Aplanar nombres de columnas\n",
    "route_analysis.columns = ['Ruta', 'Total_Viajes', 'Peso_Promedio', 'Total_Kg_Aire', 'Promedio_Kg_Aire', 'Viajes_Consolidables']\n",
    "\n",
    "# 5. FILTRAR Y ORDENAR\n",
    "# Solo rutas con volumen relevante (> 5 viajes) para que valga la pena la log√≠stica\n",
    "route_analysis = route_analysis[route_analysis['Total_Viajes'] > 5]\n",
    "\n",
    "# ORDENAR POR: ¬øD√≥nde hay m√°s Kilos de Aire Totales? (Mayor oportunidad de ahorro masivo)\n",
    "top_empty_routes = route_analysis.sort_values('Total_Kg_Aire', ascending=False).head(10)\n",
    "\n",
    "# Calcular % de Ineficiencia de Carga\n",
    "top_empty_routes['%_Desperdicio'] = (top_empty_routes['Total_Kg_Aire'] / (top_empty_routes['Total_Viajes'] * CAPACIDAD_REF)) * 100\n",
    "\n",
    "print(\"\\n--- TOP 10 RUTAS CON M√ÅS 'FLETE FALSO' EN KG (OPORTUNIDAD DE CONSOLIDACI√ìN) ---\")\n",
    "print(top_empty_routes[['Ruta', 'Total_Viajes', 'Viajes_Consolidables', 'Peso_Promedio', 'Promedio_Kg_Aire', '%_Desperdicio']])\n",
    "\n",
    "# --- C√ÅLCULO DE AHORRO POTENCIAL ---\n",
    "# Cada 2 viajes consolidables = 1 Flete ahorrado\n",
    "# Estimamos un costo promedio por flete de $25,000 (conservador)\n",
    "ahorro_estimado = (top_empty_routes['Viajes_Consolidables'].sum() / 2) * 25000\n",
    "\n",
    "print(f\"\\nüí° OPORTUNIDAD: En estas 10 rutas hay {int(top_empty_routes['Viajes_Consolidables'].sum())} viajes que iban a menos de la mitad.\")\n",
    "print(f\"   Si consolidamos pares, podr√≠amos eliminar aprox. {int(top_empty_routes['Viajes_Consolidables'].sum()/2)} viajes.\")\n",
    "print(f\"   Ahorro estimado (Fletes evitados): ${ahorro_estimado:,.0f} MXN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ======================================================================\n",
    "# VALIDACI√ìN DE FRECUENCIA PARA RUTAS CON \"AIRE\" (Solo Standard)\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Definir las Rutas Candidatas (Top 10 del an√°lisis anterior)\n",
    "target_consolidation_routes = [\n",
    "    'NL - √Årea Metro SLP',\n",
    "    'NL - √Årea Metro Saltillo',\n",
    "    'NL - √Årea Metro Monclova',\n",
    "    'NL - Area Metr Queretaro',\n",
    "    'NL - Juarez',\n",
    "    'NL - Edo. y Cd. de M√©xico',\n",
    "    'NL - √Årea Metro Puebla',\n",
    "    'NL - Altamira',\n",
    "    'NL - √Årea Metro Celaya',\n",
    "    'NL - A. M. Aguascalientes'\n",
    "]\n",
    "\n",
    "# 2. Filtro: Solo Viajes Standard\n",
    "df_standard = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "\n",
    "print(\"--- FRECUENCIA DE SALIDAS: RUTAS CANDIDATAS A CONSOLIDACI√ìN ---\\n\")\n",
    "\n",
    "for ruta in target_consolidation_routes:\n",
    "    # Filtrar y ordenar\n",
    "    trips = df_standard[df_standard['Ruta_ID'] == ruta].sort_values('F.Salida').copy()\n",
    "    \n",
    "    if len(trips) > 1:\n",
    "        # Calcular Gaps\n",
    "        trips['Prev_Salida'] = trips['F.Salida'].shift(1)\n",
    "        trips['Gap_Horas'] = (trips['F.Salida'] - trips['Prev_Salida']).dt.total_seconds() / 3600\n",
    "        \n",
    "        # M√©tricas\n",
    "        median_gap = trips['Gap_Horas'].median()\n",
    "        trips_per_day = len(trips) / ((trips['F.Salida'].max() - trips['F.Salida'].min()).days)\n",
    "        \n",
    "        print(f\"RUTA: {ruta}\")\n",
    "        print(f\"   Viajes Standard: {len(trips)}\")\n",
    "        print(f\"   Salidas Diarias Promedio: {trips_per_day:.1f}\")\n",
    "        print(f\"   Tiempo Mediano de Espera: {median_gap:.1f} horas\")\n",
    "        \n",
    "        # Veredicto de Consolidaci√≥n\n",
    "        if median_gap <= 12:\n",
    "            print(\"   ‚úÖ OPORTUNIDAD ALTA: Salen varios al d√≠a. ¬°Consolidar es f√°cil!\")\n",
    "        elif median_gap <= 24:\n",
    "            print(\"   ‚úÖ OPORTUNIDAD MEDIA: Sale uno diario. Se puede esperar al d√≠a siguiente.\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è OPORTUNIDAD BAJA: Frecuencia baja. Esperar implica retrasos de >1 d√≠a.\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE \"FLETE FALSO PROMEDIO\" (La Verdadera Ineficiencia de Carga)\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Configuraci√≥n\n",
    "CAPACIDAD_REF = 24000 # 24 Toneladas\n",
    "df_ops = df_clean[df_clean['Es_Express'] == 0].copy() # Solo viajes normales\n",
    "\n",
    "# 2. Calcular Kilos Vac√≠os por Viaje individual\n",
    "# Si trae m√°s de 24k, es 0 vac√≠o. Si trae menos, es la diferencia.\n",
    "df_ops['Kg_Vacios'] = (CAPACIDAD_REF - df_ops['Peso Total (kg)']).clip(lower=0)\n",
    "df_ops['%_Llenado'] = (df_ops['Peso Total (kg)'] / CAPACIDAD_REF) * 100\n",
    "\n",
    "# 3. Agrupar por Ruta (Promedios)\n",
    "route_avg = df_ops.groupby('Ruta_ID').agg({\n",
    "    'Viaje': 'count',\n",
    "    'Peso Total (kg)': 'mean',\n",
    "    'Kg_Vacios': 'mean',     # <--- ESTA ES LA CLAVE\n",
    "    '%_Llenado': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "route_avg.columns = ['Ruta', 'Frecuencia', 'Peso_Promedio', 'Aire_Promedio_x_Viaje', 'Llenado_Promedio']\n",
    "\n",
    "# 4. Filtro de Relevancia\n",
    "# Quitamos rutas \"fantasma\" con menos de 5 viajes para no ensuciar el an√°lisis\n",
    "route_avg = route_avg[route_avg['Frecuencia'] > 5]\n",
    "\n",
    "# 5. Ordenar por el CAMI√ìN M√ÅS VAC√çO PROMEDIO\n",
    "top_empty_avg = route_avg.sort_values('Aire_Promedio_x_Viaje', ascending=False).head(10)\n",
    "\n",
    "print(\"\\n--- TOP 10 RUTAS CON MAYOR ESPACIO VAC√çO PROMEDIO ---\")\n",
    "print(\"(Rutas donde sistem√°ticamente desaprovechamos capacidad)\")\n",
    "print(top_empty_avg[['Ruta', 'Frecuencia', 'Llenado_Promedio', 'Aire_Promedio_x_Viaje']])\n",
    "\n",
    "# Validaci√≥n r√°pida\n",
    "worst_route = top_empty_avg.iloc[0]\n",
    "print(f\"\\nüí° HALLAZGO: En la ruta '{worst_route['Ruta']}', cada cami√≥n sale en promedio con {worst_route['Aire_Promedio_x_Viaje']:,.0f} kg de aire.\")\n",
    "print(f\"   ¬°Eso es un {100 - worst_route['Llenado_Promedio']:.1f}% de capacidad desperdiciada por viaje!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a786ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ======================================================================\n",
    "# AN√ÅLISIS DE \"RIGHT-SIZING\": BAJAR DE FULL A SENCILLO\n",
    "# ======================================================================\n",
    "\n",
    "# 1. CONFIGURACI√ìN\n",
    "CAPACIDAD_SENCILLO = 22000  # 22 Toneladas (l√≠mite seguro para Sencillo)\n",
    "PESO_MIN_FULL = 35000       # Si pesa m√°s de esto, SEGURO es Full\n",
    "AHORRO_ESTIMADO_PCT = 0.20  # Un Sencillo suele cobrar 20% menos que un Full\n",
    "\n",
    "# Filtramos solo viajes Standard (No Express)\n",
    "df_eq = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "\n",
    "print(f\"--- AN√ÅLISIS DE OPORTUNIDAD: CAMBIO DE EQUIPO (FULL -> SENCILLO) ---\\n\")\n",
    "\n",
    "downgrade_opportunities = []\n",
    "\n",
    "# Analizamos ruta por ruta para comparar peras con peras\n",
    "for ruta in df_eq['Ruta_ID'].unique():\n",
    "    route_trips = df_eq[df_eq['Ruta_ID'] == ruta]\n",
    "    \n",
    "    # Solo analizamos rutas con suficiente volumen y mezcla de pesos\n",
    "    if len(route_trips) > 10:\n",
    "        # 2. IDENTIFICAR EL COSTO DE UN \"FULL\" EN ESTA RUTA\n",
    "        # Tomamos el promedio de los viajes pesados (>35 tons)\n",
    "        heavy_trips = route_trips[route_trips['Peso Total (kg)'] > PESO_MIN_FULL]\n",
    "        \n",
    "        if len(heavy_trips) > 0:\n",
    "            avg_cost_full = heavy_trips['Costo'].mean()\n",
    "            \n",
    "            # 3. BUSCAR VIAJES LIGEROS PERO CAROS\n",
    "            # Ligeros: Caben en un Sencillo (< 22 tons)\n",
    "            # Caros: Costaron casi lo mismo que un Full (> 85% del costo Full)\n",
    "            # Esto indica que probablemente SE PAG√ì como Full aunque iba ligero\n",
    "            inefficient_trips = route_trips[\n",
    "                (route_trips['Peso Total (kg)'] <= CAPACIDAD_SENCILLO) & \n",
    "                (route_trips['Costo'] > (avg_cost_full * 0.85))\n",
    "            ]\n",
    "            \n",
    "            num_ineff = len(inefficient_trips)\n",
    "            \n",
    "            if num_ineff > 0:\n",
    "                # Calcular ahorro: Si hubi√©ramos pedido Sencillo, costar√≠a 20% menos\n",
    "                potential_saving = inefficient_trips['Costo'].sum() * AHORRO_ESTIMADO_PCT\n",
    "                \n",
    "                downgrade_opportunities.append({\n",
    "                    'Ruta': ruta,\n",
    "                    'Viajes_Candidatos': num_ineff,\n",
    "                    'Costo_Promedio_Actual': inefficient_trips['Costo'].mean(),\n",
    "                    'Costo_Ref_Full': avg_cost_full,\n",
    "                    'Ahorro_Total': potential_saving\n",
    "                })\n",
    "\n",
    "# 4. RESULTADOS\n",
    "if len(downgrade_opportunities) > 0:\n",
    "    df_downgrade = pd.DataFrame(downgrade_opportunities).sort_values('Ahorro_Total', ascending=False)\n",
    "    \n",
    "    print(df_downgrade[['Ruta', 'Viajes_Candidatos', 'Costo_Ref_Full', 'Ahorro_Total']].head(10))\n",
    "    \n",
    "    total_saving_eq = df_downgrade['Ahorro_Total'].sum()\n",
    "    total_trips_eq = df_downgrade['Viajes_Candidatos'].sum()\n",
    "    \n",
    "    print(f\"\\nüí° HALLAZGO: Encontramos {total_trips_eq} viajes que se pagaron a precio de 'Full' pero llevaban carga de 'Sencillo'.\")\n",
    "    print(f\"üí∞ AHORRO POTENCIAL (Bajando a Sencillo): ${total_saving_eq:,.2f}\")\n",
    "else:\n",
    "    print(\"No se detectaron oportunidades claras de cambio de equipo (La asignaci√≥n actual parece eficiente).\")\n",
    "\n",
    "# --- VISUALIZACI√ìN: SCATTER PLOT COSTO vs PESO ---\n",
    "# Esto te ayuda a ver visualmente los dos grupos\n",
    "top_route = df_downgrade.iloc[0]['Ruta'] if len(downgrade_opportunities) > 0 else target_routes[0]\n",
    "viz_data = df_eq[df_eq['Ruta_ID'] == top_route]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=viz_data, x='Peso Total (kg)', y='Costo', alpha=0.6)\n",
    "plt.axvline(CAPACIDAD_SENCILLO, color='green', linestyle='--', label='L√≠mite Sencillo (22 Ton)')\n",
    "plt.axhline(viz_data[viz_data['Peso Total (kg)'] > PESO_MIN_FULL]['Costo'].mean(), color='red', linestyle='--', label='Costo Promedio Full')\n",
    "plt.title(f'An√°lisis de Equipo: Ruta {top_route}', fontweight='bold')\n",
    "plt.xlabel('Peso (Kg)')\n",
    "plt.ylabel('Costo ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# ======================================================================\n",
    "# CORROBORACI√ìN ECONOM√âTRICA: RUTA NL - JUAREZ\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Buscar el coeficiente espec√≠fico en tu modelo\n",
    "# El nombre interno de la variable es 'Route_NL - Juarez'\n",
    "target_variable = 'Route_NL - Juarez'\n",
    "\n",
    "try:\n",
    "    # Extraer el valor del coeficiente (El costo extra)\n",
    "    beta_ruta = model_full.params[model_full.params.index.str.contains(target_variable)].iloc[0]\n",
    "    \n",
    "    # Extraer el P-Value (La certeza de que no es suerte)\n",
    "    p_value = model_full.pvalues[model_full.pvalues.index.str.contains(target_variable)].iloc[0]\n",
    "    \n",
    "    # Extraer el Intervalo de Confianza (El rango real del golpe)\n",
    "    conf_int = model_full.conf_int().loc[model_full.params.index.str.contains(target_variable)].iloc[0]\n",
    "\n",
    "    print(f\"--- RADIOGRAF√çA ECONOM√âTRICA: {target_variable} ---\\n\")\n",
    "    print(f\"1. COEFICIENTE (Sobrecosto):  ${beta_ruta:,.2f}\")\n",
    "    print(f\"   (Esto es lo que cuesta de m√°s ir a Ju√°rez vs. una ruta promedio,\")\n",
    "    print(f\"    despu√©s de descontar peso y transportista).\")\n",
    "    \n",
    "    print(f\"\\n2. SIGNIFICANCIA ESTAD√çSTICA (P-Value): {p_value:.10f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   ‚úÖ RESULTADO: Altamente Significativo. El sobrecosto es REAL y estructural.\")\n",
    "    else:\n",
    "        print(\"   ‚ùå RESULTADO: No significativo (Podr√≠a ser ruido estad√≠stico).\")\n",
    "        \n",
    "    print(f\"\\n3. INTERVALO DE CONFIANZA (95%):\")\n",
    "    print(f\"   M√≠nimo: ${conf_int[0]:,.2f}\")\n",
    "    print(f\"   M√°ximo: ${conf_int[1]:,.2f}\")\n",
    "    print(\"   (Incluso en el mejor de los casos, el sobrecosto es brutal).\")\n",
    "\n",
    "except IndexError:\n",
    "    print(f\"No se encontr√≥ la variable '{target_variable}' en el modelo. Verifica el nombre exacto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ======================================================================\n",
    "# 1. CONFIGURACI√ìN Y C√ÅLCULO\n",
    "# ======================================================================\n",
    "CAPACIDAD_CAMION = 24000  # 24 Toneladas por cami√≥n\n",
    "df_daily = df_clean[df_clean['Es_Express'] == 0].copy() # Solo viajes Standard\n",
    "df_daily['Fecha'] = df_daily['F.Salida'].dt.date \n",
    "\n",
    "# Agrupar: Sumar todo el peso que sali√≥ el mismo d√≠a hacia el mismo destino\n",
    "daily_ops = df_daily.groupby(['Ruta_ID', 'Fecha']).agg({\n",
    "    'Peso Total (kg)': 'sum',\n",
    "    'Costo': 'sum',\n",
    "    'Viaje': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Simulaci√≥n: ¬øCu√°ntos camiones hubi√©ramos necesitado si llen√°ramos al tope?\n",
    "daily_ops['Camiones_Optimos'] = np.ceil(daily_ops['Peso Total (kg)'] / CAPACIDAD_CAMION)\n",
    "daily_ops['Viajes_Ahorrados'] = daily_ops['Viaje'] - daily_ops['Camiones_Optimos']\n",
    "\n",
    "# Calcular dinero ahorrado (Costo Promedio del D√≠a * Viajes Ahorrados)\n",
    "daily_ops['Costo_Promedio_Unitario'] = daily_ops['Costo'] / daily_ops['Viaje']\n",
    "daily_ops['Ahorro_MXN'] = daily_ops['Viajes_Ahorrados'] * daily_ops['Costo_Promedio_Unitario']\n",
    "\n",
    "# ======================================================================\n",
    "# 2. RESULTADOS FINANCIEROS\n",
    "# ======================================================================\n",
    "total_saving = daily_ops['Ahorro_MXN'].sum()\n",
    "total_spend = df_clean['Costo'].sum()\n",
    "trips_saved = daily_ops['Viajes_Ahorrados'].sum()\n",
    "pct_impact = (total_saving / total_spend) * 100\n",
    "\n",
    "print(f\"--- REPORTE DE ESTRATEGIA: CONSOLIDACI√ìN DIN√ÅMICA ---\")\n",
    "print(f\"Objetivo: Eliminar viajes redundantes llenando camiones al 100% diario.\")\n",
    "print(f\"\\nüí∞ RESULTADOS PROYECTADOS:\")\n",
    "print(f\"   Ahorro Total Anualizado:  ${total_saving:,.2f} MXN\")\n",
    "print(f\"   Impacto en Gasto Total:   -{pct_impact:.1f}%\")\n",
    "print(f\"   Viajes Eliminados:        {int(trips_saved)} fletes menos\")\n",
    "\n",
    "# ======================================================================\n",
    "# 3. VISUALIZACI√ìN: ACTUAL VS OPTIMIZADO (TOP 5 RUTAS)\n",
    "# ======================================================================\n",
    "# Agrupar por ruta para el gr√°fico\n",
    "route_summary = daily_ops.groupby('Ruta_ID').agg({\n",
    "    'Viaje': 'sum',\n",
    "    'Camiones_Optimos': 'sum',\n",
    "    'Ahorro_MXN': 'sum'\n",
    "}).sort_values('Ahorro_MXN', ascending=False).head(5)\n",
    "\n",
    "# Preparar datos para plot\n",
    "rutas = [r.replace('Route_', '').replace('NL - ', '') for r in route_summary.index] # Nombres cortos\n",
    "actuales = route_summary['Viaje'].values\n",
    "optimos = route_summary['Camiones_Optimos'].values\n",
    "\n",
    "x = np.arange(len(rutas))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "rects1 = ax.bar(x - width/2, actuales, width, label='Viajes Actuales (Fragmentados)', color='#7f7f7f')\n",
    "rects2 = ax.bar(x + width/2, optimos, width, label='Viajes Necesarios (Consolidados)', color='#2ca02c')\n",
    "\n",
    "ax.set_ylabel('N√∫mero de Viajes')\n",
    "ax.set_title('Eficiencia de Flota: Reducci√≥n de Viajes por Ruta (Top 5)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rutas, rotation=15)\n",
    "ax.legend()\n",
    "\n",
    "# Etiquetas de valor\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{int(height)}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# Anotaci√≥n de ahorro en la mejor ruta\n",
    "best_route_saving = route_summary.iloc[0]['Ahorro_MXN']\n",
    "ax.text(0, actuales[0]*0.5, f\"Ahorro:\\n${best_route_saving/1000000:.1f}M\", \n",
    "        ha='center', color='white', fontweight='bold', fontsize=11,\n",
    "        bbox=dict(facecolor='green', alpha=0.7, edgecolor='none'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('consolidacion_impacto.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902954b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ======================================================================\n",
    "# 1. PREPARAR DATOS (Recalculando para graficar)\n",
    "# ======================================================================\n",
    "CAPACIDAD_CAMION = 24000\n",
    "df_daily = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "df_daily['Fecha'] = df_daily['F.Salida'].dt.date \n",
    "\n",
    "daily_ops = df_daily.groupby(['Ruta_ID', 'Fecha']).agg({\n",
    "    'Peso Total (kg)': 'sum',\n",
    "    'Viaje': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "daily_ops['Camiones_Optimos'] = np.ceil(daily_ops['Peso Total (kg)'] / CAPACIDAD_CAMION)\n",
    "daily_ops['Viajes_Ahorrados'] = daily_ops['Viaje'] - daily_ops['Camiones_Optimos']\n",
    "\n",
    "# Agrupar por ruta para el Top 5\n",
    "route_viz = daily_ops.groupby('Ruta_ID').agg({\n",
    "    'Viaje': 'sum',\n",
    "    'Camiones_Optimos': 'sum',\n",
    "    'Viajes_Ahorrados': 'sum'\n",
    "}).sort_values('Viajes_Ahorrados', ascending=False).head(5)\n",
    "\n",
    "# Limpiar nombres para la gr√°fica\n",
    "rutas = [r.replace('NL - √Årea Metro ', '').replace('NL - ', '') for r in route_viz.index]\n",
    "actuales = route_viz['Viaje'].values\n",
    "optimos = route_viz['Camiones_Optimos'].values\n",
    "ahorros = route_viz['Viajes_Ahorrados'].values\n",
    "\n",
    "# ======================================================================\n",
    "# 2. GENERAR GR√ÅFICA\n",
    "# ======================================================================\n",
    "x = np.arange(len(rutas))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Barras\n",
    "rects1 = ax.bar(x - width/2, actuales, width, label='Viajes Actuales (Fragmentados)', color='#d62728', alpha=0.8)\n",
    "rects2 = ax.bar(x + width/2, optimos, width, label='Viajes Necesarios (Consolidados)', color='#2ca02c', alpha=0.9)\n",
    "\n",
    "# Formato\n",
    "ax.set_ylabel('N√∫mero de Viajes Semestrales', fontsize=12)\n",
    "ax.set_title('Impacto de Consolidaci√≥n: Reducci√≥n de Frecuencia en Top 5 Rutas', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rutas, fontsize=11)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Etiquetas de valor\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{int(height)}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "# Flechas de Ahorro (El \"Wow Factor\")\n",
    "for i in range(len(rutas)):\n",
    "    # Calcular % de reducci√≥n\n",
    "    pct = (ahorros[i] / actuales[i]) * 100\n",
    "    # Poner texto entre las barras\n",
    "    ax.text(i, optimos[i] + (actuales[i]-optimos[i])/2, \n",
    "            f'Ahorro:\\n-{int(ahorros[i])} Viajes\\n(-{pct:.0f}%)', \n",
    "            ha='center', va='center', color='black', fontsize=10, fontweight='bold',\n",
    "            bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.3', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('impacto_consolidacion.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ======================================================================\n",
    "# AUDITOR√çA DE DATOS: VERIFICACI√ìN DE OPORTUNIDADES REALES\n",
    "# ======================================================================\n",
    "\n",
    "print(\"--- 1. IDENTIFICANDO D√çAS CON MAYOR DESPERDICIO ---\")\n",
    "\n",
    "# 1. Preparar datos\n",
    "CAPACIDAD_CAMION = 24000\n",
    "df_audit = df_clean[df_clean['Es_Express'] == 0].copy()\n",
    "df_audit['Fecha'] = df_audit['F.Salida'].dt.date\n",
    "\n",
    "# 2. Calcular eficiencia por d√≠a\n",
    "daily_audit = df_audit.groupby(['Ruta_ID', 'Fecha']).agg({\n",
    "    'Viaje': 'count',\n",
    "    'Peso Total (kg)': ['sum', 'mean'],\n",
    "    'Costo': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Aplanar columnas\n",
    "daily_audit.columns = ['Ruta', 'Fecha', 'Viajes_Reales', 'Peso_Total_Dia', 'Peso_Promedio_Viaje', 'Costo_Total_Dia']\n",
    "\n",
    "# Calcular escenario ideal\n",
    "daily_audit['Camiones_Necesarios'] = np.ceil(daily_audit['Peso_Total_Dia'] / CAPACIDAD_CAMION)\n",
    "daily_audit['Desperdicio_Viajes'] = daily_audit['Viajes_Reales'] - daily_audit['Camiones_Necesarios']\n",
    "daily_audit['Llenado_Promedio_Dia'] = (daily_audit['Peso_Total_Dia'] / (daily_audit['Viajes_Reales'] * CAPACIDAD_CAMION)) * 100\n",
    "\n",
    "# Filtrar solo donde hubo desperdicio real ( > 0 viajes extra)\n",
    "inefficient_days = daily_audit[daily_audit['Desperdicio_Viajes'] > 0].sort_values('Desperdicio_Viajes', ascending=False)\n",
    "\n",
    "print(f\"Se encontraron {len(inefficient_days)} d√≠as/ruta con viajes innecesarios.\")\n",
    "print(\"\\nTop 5 D√≠as con Mayor Ineficiencia (Ejemplos para mostrar):\")\n",
    "print(inefficient_days[['Ruta', 'Fecha', 'Viajes_Reales', 'Camiones_Necesarios', 'Desperdicio_Viajes', 'Llenado_Promedio_Dia']].head(5))\n",
    "\n",
    "# --- 2. AUTOPSIA DE UN CASO REAL (DRILL-DOWN) ---\n",
    "if len(inefficient_days) > 0:\n",
    "    # Tomamos el peor caso para analizarlo\n",
    "    worst_case = inefficient_days.iloc[0]\n",
    "    target_route = worst_case['Ruta']\n",
    "    target_date = worst_case['Fecha']\n",
    "    \n",
    "    print(f\"\\n\\n--- 2. EVIDENCIA DETALLADA: ¬øQU√â PAS√ì EL {target_date} EN {target_route}? ---\")\n",
    "    print(f\"Resumen: Salieron {int(worst_case['Viajes_Reales'])} camiones. Con {worst_case['Peso_Total_Dia']:,.0f} kg totales, solo necesit√°bamos {int(worst_case['Camiones_Necesarios'])}.\")\n",
    "    print(f\"¬°Pagamos {int(worst_case['Desperdicio_Viajes'])} fletes de m√°s ese d√≠a!\\n\")\n",
    "    \n",
    "    # Mostrar los viajes individuales de ese d√≠a\n",
    "    detail_trips = df_audit[(df_audit['Ruta_ID'] == target_route) & (df_audit['Fecha'] == target_date)]\n",
    "    \n",
    "    print(\"Lista de env√≠os fragmentados (Evidencia):\")\n",
    "    display_cols = ['Viaje', 'F.Salida', 'Carrier_Name', 'Peso Total (kg)', 'Costo', 'TpoSrv']\n",
    "    # Si alguna columna no existe, ajusta nombres\n",
    "    valid_cols = [c for c in display_cols if c in detail_trips.columns]\n",
    "    \n",
    "    print(detail_trips[valid_cols].sort_values('F.Salida').to_string(index=False))\n",
    "    \n",
    "    # C√°lculo de dinero tirado ese d√≠a espec√≠fico\n",
    "    costo_real = detail_trips['Costo'].sum()\n",
    "    costo_teorico = (costo_real / worst_case['Viajes_Reales']) * worst_case['Camiones_Necesarios']\n",
    "    dinero_tirado = costo_real - costo_teorico\n",
    "    \n",
    "    print(f\"\\nüí∏ Dinero desperdiciado solo en este d√≠a: ${dinero_tirado:,.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"¬°Felicidades! No se encontraron d√≠as con ineficiencias de consolidaci√≥n obvias.\")\n",
    "\n",
    "# --- 3. VERIFICACI√ìN DE CALIDAD DE DATOS (DATA HEALTH) ---\n",
    "print(\"\\n\\n--- 3. VERIFICACI√ìN DE CALIDAD DE DATOS ---\")\n",
    "print(\"Chequeo de valores extra√±os que podr√≠an alterar el an√°lisis:\")\n",
    "print(f\"- Viajes con Peso 0 o negativo: {len(df_clean[df_clean['Peso Total (kg)'] <= 0])}\")\n",
    "print(f\"- Viajes con Costo 0 o negativo: {len(df_clean[df_clean['Costo'] <= 0])}\")\n",
    "# Ver si hay outliers extremos de peso (> 60 toneladas, que ser√≠a ilegal o error)\n",
    "print(f\"- Viajes con Peso > 60 tons (Posible error de dedo): {len(df_clean[df_clean['Peso Total (kg)'] > 60000])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
