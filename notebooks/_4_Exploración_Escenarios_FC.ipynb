{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b940674",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Implementaci√≥n Metodol√≥gica Principal (5-7 p√°ginas + c√≥digo)\n",
    "\n",
    "El contenido espec√≠fico depender√° del enfoque elegido, pero TODOS deben incluir:\n",
    "\n",
    "### Secci√≥n A: Preparaci√≥n y Transformaci√≥n\n",
    "\n",
    "#### Proceso detallado de limpieza y transformaci√≥n de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846c206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel(\"../data/raw/Viajes Sep-Dic 24 v2.xlsx\", sheet_name='Viajes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b84b6",
   "metadata": {},
   "source": [
    "Feature: Distancia recorrida en KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51786c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "\n",
    "def calcular_distancia_recta(row):\n",
    "    try:\n",
    "        geolocator = Nominatim(user_agent=\"mi_app_geo\") # Define tu user_agent\n",
    "        \n",
    "        # Concatena tus campos para darle m√°s contexto a Nominatim\n",
    "        query_origen = f\"{row['Desc Org. Apt']}, {row['Origen']}\"\n",
    "        query_destino = f\"{row['Destino']}\"\n",
    "\n",
    "        # Geocodifica el origen\n",
    "        location_origen = geolocator.geocode(query_origen)\n",
    "        time.sleep(1) # IMPORTANTE: Nominatim requiere un delay de 1 seg por petici√≥n\n",
    "        \n",
    "        # Geocodifica el destino\n",
    "        location_destino = geolocator.geocode(query_destino)\n",
    "        time.sleep(1)\n",
    "\n",
    "        if location_origen and location_destino:\n",
    "            coords_origen = (location_origen.latitude, location_origen.longitude)\n",
    "            coords_destino = (location_destino.latitude, location_destino.longitude)\n",
    "            \n",
    "            # Calcula la distancia geod√©sica (l√≠nea recta)\n",
    "            distancia_km = geodesic(coords_origen, coords_destino).kilometers\n",
    "            print(f\"√âxito: {query_origen} -> {query_destino} = {distancia_km:.2f} km\")\n",
    "            return distancia_km\n",
    "        else:\n",
    "            print(f\"No se pudieron geocodificar: {query_origen} o {query_destino}\")\n",
    "            return np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error en Nominatim: {e}\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos el DF para estandarizar nombres de Origen y Destino\n",
    "df['Desc Org. Apt'] = df['Origen'].replace('Churubusco', 'Churubusco Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('AM Monterrey Plantas', 'Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('AM Monterrey Local', 'Monterrey')\n",
    "df['Destino'] = df['Destino'].replace('Area Metr Queretaro', 'Queretaro')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Saltillo', 'Saltillo')\n",
    "df['Destino'] = df['Destino'].replace('Area Metro Monclova', 'Monclova')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Monclova', 'Monclova')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro SLP', 'San Luis Potosi')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Puebla', 'Puebla')\n",
    "df['Destino'] = df['Destino'].replace('√Årea Metro Celaya', 'Celaya')\n",
    "df['Destino'] = df['Destino'].replace('A. M. Aguascalientes', 'Aguascalientes')\n",
    "\n",
    "# Creamos un dataframe con las combinaciones √∫nicas de Origen y Destino\n",
    "combinaciones = df[['Desc Org. Apt','Origen', 'Destino']].drop_duplicates()\n",
    "\n",
    "# Obtenemos la distancia entre cada par de Origen y Destino\n",
    "\n",
    "# Primero verificamos si el archivo ya existe para evitar recalcular\n",
    "import os\n",
    "if os.path.exists('../data/processed/combinaciones_origen_destino.csv'):\n",
    "    combinaciones = pd.read_csv('../data/processed/combinaciones_origen_destino.csv')\n",
    "else:\n",
    "    combinaciones['Distancia_km'] = combinaciones.apply(calcular_distancia_recta, axis=1)\n",
    "\n",
    "# A√±adimos la distancia al dataframe original\n",
    "df = df.merge(combinaciones[['Desc Org. Apt','Origen', 'Destino', 'Distancia_km']], on=['Desc Org. Apt','Origen', 'Destino'], how='left')\n",
    "\n",
    "# Guardamos las combinaciones en un archivo CSV\n",
    "combinaciones.to_csv('../data/processed/combinaciones_origen_destino.csv', index=False)\n",
    "\n",
    "#print(combinaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5587027",
   "metadata": {},
   "source": [
    "Features: Tipo Planta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73008f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding para la columna 'Tipo Planta'\n",
    "tipo_planta_dummies = pd.get_dummies(df['Tipo Planta'], prefix='Tipo_Planta')\n",
    "df = pd.concat([df, tipo_planta_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('Tipo Planta', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5f106",
   "metadata": {},
   "source": [
    "Features: Dep√≥sito Origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding para la columna 'Dep√≥sito Origen'\n",
    "deposito_origen_dummies = pd.get_dummies(df['Deposito Origen'], prefix='Deposito_Origen')\n",
    "df = pd.concat([df, deposito_origen_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('Deposito Origen', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edfcd3c",
   "metadata": {},
   "source": [
    "Features: Tipo de permiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding para la columna 'Tipo de permiso'\n",
    "tipo_permiso_dummies = pd.get_dummies(df['TpoPermiso'], prefix='Tipo_Permiso')\n",
    "df = pd.concat([df, tipo_permiso_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('TpoPermiso', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01f34b",
   "metadata": {},
   "source": [
    "Features: Tipo de Servicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcf04eb",
   "metadata": {},
   "source": [
    "# One hot encoding para la columna 'Tipo Servicio'\n",
    "tipo_servicio_dummies = pd.get_dummies(df['TpoSrv'], prefix='Tipo_Servicio')\n",
    "df = pd.concat([df, tipo_servicio_dummies], axis=1)\n",
    "\n",
    "# Eliminar variable original\n",
    "df.drop('TpoSrv', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b76d7d",
   "metadata": {},
   "source": [
    "Features: Tipo de camion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5df1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: 'Ejes'\n",
    "def obtener_ejes(tipo_camion):\n",
    "    if pd.isnull(tipo_camion):\n",
    "        return 0\n",
    "    elif tipo_camion == 'Big Coil':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 Ejes':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 Eje Cortina':\n",
    "        return 3\n",
    "    elif tipo_camion == '3 ejes Portacinta':\n",
    "        return 3\n",
    "    elif tipo_camion == \"Plataforma 48'\" or tipo_camion == \"Plataforma 53'\":\n",
    "        return 4\n",
    "    elif tipo_camion == \"2 Ejes\":\n",
    "        return 2\n",
    "\n",
    "df['Ejes'] = df['Tipo transporte'].apply(obtener_ejes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc919d",
   "metadata": {},
   "source": [
    "Feature: Tipo de viaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero se a√±ade 'Va' a los valores vacios\n",
    "df['TpoVje'] = df['TpoVje'].fillna('Vacio')\n",
    "\n",
    "# One hot encoding para la columna 'TpoVje'\n",
    "tipo_viaje_dummies = pd.get_dummies(df['TpoVje'], prefix='Tipo_Viaje')\n",
    "df = pd.concat([df, tipo_viaje_dummies], axis=1)\n",
    "# Eliminar variable original\n",
    "df.drop('TpoVje', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd517402",
   "metadata": {},
   "source": [
    "Feature: Fecha de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7000c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir la columna 'F.Salida' en componentes separados\n",
    "df['F.Salida'] = pd.to_datetime(df['F.Salida'], errors='coerce')\n",
    "df['Salida_A√±o'] = df['F.Salida'].dt.year\n",
    "df['Salida_Mes'] = df['F.Salida'].dt.month\n",
    "df['Salida_Dia'] = df['F.Salida'].dt.day\n",
    "\n",
    "# Eliminamos la columna original\n",
    "df.drop('F.Salida', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modificamos columnas que puedan tener acentos\n",
    "df.rename(columns={\n",
    "    'Garant√≠a': 'Garantia',\n",
    "    'Variaci√≥n': 'Variacion'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd764bc7",
   "metadata": {},
   "source": [
    "Eliminacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de datos\n",
    "df = df.drop(columns=['Viaje', 'Permiso', 'Shipment', 'Sociedad','Ori-Dest-TT','Ori-Dest','Modal.','Moneda','Estatus'])\n",
    "\n",
    "df = df.drop(columns=['Planta Origen', 'Origen', 'Destino', 'Desc Org. Apt', 'TPP', 'Desc TPP', 'Transp.Leg'])\n",
    "\n",
    "df = df.drop(columns=['Tipo planta', 'Nombre', 'Desc TpoTrn', 'Tipo transporte', 'TpoTrn.APT'])\n",
    "\n",
    "df = df.drop(columns=['TpoTrn.Leg'])\n",
    "\n",
    "# Se elimina la √∫ltima linea que contiene totales\n",
    "df = df.iloc[:-1]\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Guardar el dataframe procesado\n",
    "df.to_csv(\"../data/processed/datos_procesados.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb17b0",
   "metadata": {},
   "source": [
    "\n",
    "#### Justificaci√≥n de decisiones sobre valores faltantes, outliers, y agregaciones\n",
    "\n",
    "| Variable a eliminar | Raz√≥n |\n",
    "| ---- | ---- |\n",
    "| Viaje | Informaci√≥n irrelevante |\n",
    "| Permiso | Informaci√≥n irrelevante |\n",
    "| Shipment | Informaci√≥n irrelevante |\n",
    "| Sociedad | Informaci√≥n irrelevante |\n",
    "| Ori-Dest-TT | Informaci√≥n redundante |\n",
    "| Ori-Dest | Informaci√≥n redundante |\n",
    "| Modal | Informaci√≥n irrelevante |\n",
    "| Moneda | Informaci√≥n irrelevante |\n",
    "| Estatus | Informaci√≥n irrelevante |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100d4d7",
   "metadata": {},
   "source": [
    "#### Creaci√≥n de variables relevantes para su enfoque (ratios, interacciones, lags, etc.)\n",
    "#### Validaci√≥n de supuestos requeridos por sus m√©todos\n",
    " \n",
    "\n",
    "‚†ÄSecci√≥n B: An√°lisis Principal (var√≠a seg√∫n enfoque)\n",
    "\n",
    "- Construcci√≥n y justificaci√≥n del DAG (Directed Acyclic Graph)\n",
    "- Identificaci√≥n de confounders, mediadores, y colisionadores\n",
    "- Estrategia de identificaci√≥n causal (IV, diferencias-en-diferencias, etc.)\n",
    "- Estimaci√≥n de efectos causales con intervalos de confianza\n",
    "- An√°lisis de sensibilidad a violaciones de supuestos\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c93682",
   "metadata": {},
   "source": [
    "# Construccion del DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31330f94",
   "metadata": {},
   "source": [
    "1. Importamos todas las librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Try multiple possible import paths for causal-learn / causallearn to support different package versions\n",
    "try:\n",
    "    # Preferred package layout (causal-learn)\n",
    "    from causal_learn.search.ConstraintBased.PC import pc\n",
    "    from causal_learn.utils.cit import fisherz\n",
    "except Exception:\n",
    "    try:\n",
    "        # Alternative package name / layout (causallearn)\n",
    "        from causallearn.search.ConstraintBased.PC import pc\n",
    "        from causallearn.utils.cit import fisherz\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Could not import 'pc' and 'fisherz' from causal-learn/causallearn; \"\n",
    "            \"please install 'causal-learn' (pip install causal-learn) \"\n",
    "            \"and verify the package version and import paths.\"\n",
    "        ) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dc535e",
   "metadata": {},
   "source": [
    "2. Configuraciones y constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500df4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../data/processed/datos_procesados.csv\"  # IMPORTANTE: Reemplaza con tu ruta de archivo\n",
    "TARGET_VARIABLE_NAME = 'Costo'                 \n",
    "COEFFICIENT_THRESHOLD = 0.05                   \n",
    "SELECTED_COLUMNS = [\n",
    "    'Garantia', \n",
    "    'Flete Falso (KG)', \n",
    "    'Flete Falso (MXN)', \n",
    "    #'Flete Falso (USD)', ELIMINADO POR REDUNDANCIA\n",
    "    'H.Salida',\n",
    "    #'#Remitos', ELIMINADO POR REDUNDANCIA\n",
    "    'Peso Total (kg)',\n",
    "    'Costo',\n",
    "    'Costo Prom',\n",
    "    'Variacion',\n",
    "    #'CostoxTn',\n",
    "    'Shp.Cost',\n",
    "    'Monto Real',\n",
    "    #'Monto Falso',\n",
    "    'Monto Reparto',\n",
    "    'Distancia_km',\n",
    "    'Tipo_Planta_Masivos',\n",
    "    #'Tipo_Planta_Revestidos',\n",
    "    'Deposito_Origen_CHUCS',\n",
    "    'Deposito_Origen_CHUMA',\n",
    "    'Deposito_Origen_MVAMI',\n",
    "    'Deposito_Origen_MVAML',\n",
    "    #'Deposito_Origen_UNIUN',\n",
    "    'Tipo_Permiso_1.0',\n",
    "    'Tipo_Permiso_2.0',\n",
    "    #'Tipo_Permiso_3.0',\n",
    "    'Tipo_Servicio_EX',\n",
    "    #'Tipo_Servicio_FO',\n",
    "    'Ejes',\n",
    "    'Tipo_Viaje_AP',\n",
    "    'Tipo_Viaje_EX',\n",
    "    'Tipo_Viaje_LT',\n",
    "    'Tipo_Viaje_NO',\n",
    "    #'Tipo_Viaje_PM',\n",
    "    #'Tipo_Viaje_Vacio',\n",
    "    #'Salida_A√±o', Eliminada porque todos los datos son del 2024\n",
    "    'Salida_Mes',\n",
    "    'Salida_Dia'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a425b05",
   "metadata": {},
   "source": [
    "3. Funci√≥n de visualizaci√≥n para PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0952565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pc_dag(adj_matrix, columns, causal_strengths=None, title='Grafo Causal (CPDAG) Algoritmo PC'):\n",
    "    \"\"\"Visualiza el grafo de estructura (CPDAG) generado por el Algoritmo PC.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(columns)\n",
    "    edge_labels = {}\n",
    "    \n",
    "    # Mapear la matriz de adyacencia de PC a un grafo NetworkX\n",
    "    for i, col_i in enumerate(columns):\n",
    "        for j, col_j in enumerate(columns):\n",
    "            # El Algoritmo PC usa 1 para i -> j y 3 para i <- j (y 2 para i -- j)\n",
    "            if adj_matrix[i, j] == 1:\n",
    "                # i -> j\n",
    "                G.add_edge(col_i, col_j)\n",
    "                if causal_strengths and (col_i, col_j) in causal_strengths:\n",
    "                    weight = causal_strengths[(col_i, col_j)]\n",
    "                    edge_labels[(col_i, col_j)] = f\"{weight:.4f}\"\n",
    "            elif adj_matrix[i, j] == 2 and adj_matrix[j, i] == 2:\n",
    "                # i -- j (Borde incierto, bidireccional para visualizaci√≥n)\n",
    "                if not G.has_edge(col_j, col_i):  # Evitar duplicados\n",
    "                    G.add_edge(col_i, col_j, weight='?')\n",
    "    \n",
    "    # Configuraci√≥n de la visualizaci√≥n\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    # Intentar usar Graphviz, pero con mejor manejo de excepciones\n",
    "    pos = None\n",
    "    try:\n",
    "        pos = nx.drawing.nx_pydot.graphviz_layout(G, prog='dot')\n",
    "        print(\"‚úì Layout con Graphviz (dot) exitoso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  No se pudo usar Graphviz: {type(e).__name__}. Usando spring_layout...\")\n",
    "        pos = nx.spring_layout(G, k=2.5, iterations=100, seed=42)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightcoral', node_size=1500, edgecolors='gray')\n",
    "    \n",
    "    # Dibuja aristas dirigidas (flechas)\n",
    "    directed_edges = [(u, v) for u, v in G.edges() if G.edges[u, v].get('weight') != '?']\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=directed_edges, arrowstyle='->', \n",
    "                          arrowsize=20, edge_color='black', width=1.5)\n",
    "    \n",
    "    # Dibuja aristas inciertas (baja opacidad)\n",
    "    undirected_edges = [(u, v) for u, v in G.edges() if G.edges[u, v].get('weight') == '?']\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=undirected_edges, arrowstyle='-', \n",
    "                          style='dashed', edge_color='gray', width=1.0, alpha=0.5)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')\n",
    "    \n",
    "    # Dibuja las etiquetas de fuerza causal\n",
    "    if edge_labels:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, \n",
    "                                    font_color='red', font_size=8, label_pos=0.5)\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66356316",
   "metadata": {},
   "source": [
    "4. Bloque de ejecuci√≥n principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Carga, Filtrado y Preprocesamiento de Datos ---\n",
    "print(\"--- 1. Carga, Filtrado y Preprocesamiento de Datos ---\")\n",
    "try:\n",
    "    data = pd.read_csv(FILE_PATH)\n",
    "    data = data[SELECTED_COLUMNS]\n",
    "    data = data.dropna()\n",
    "    \n",
    "    for col in data.columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    \n",
    "    data = data.dropna().astype(float)\n",
    "    print(f\"‚úÖ Datos cargados y filtrados. Dimensiones finales: {data.shape}\")\n",
    "    print(f\"   Columnas: {list(data.columns)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Archivo no encontrado en {FILE_PATH}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR durante la carga o limpieza de datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Estandarizaci√≥n de Datos ---\n",
    "scaler = StandardScaler()\n",
    "data_scaled_np = scaler.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled_np, columns=data.columns)\n",
    "print(\"\\n[Estandarizaci√≥n de Datos]\")\n",
    "print(\"‚úÖ Datos escalados con StandardScaler.\")\n",
    "\n",
    "# --- 3. DESCUBRIMIENTO CAUSAL (Algoritmo PC) ---\n",
    "print(\"\\n--- 3. Descubrimiento Causal (Algoritmo PC) ---\")\n",
    "try:\n",
    "    data_matrix = data_scaled.values\n",
    "    \n",
    "    # Ejecutar el Algoritmo PC\n",
    "    graph_pc = pc(data_matrix, alpha=0.05, indep_test=fisherz)\n",
    "    \n",
    "    # CORRECCI√ìN: Acceso correcto a la matriz de adyacencia\n",
    "    adj_matrix = graph_pc.G.graph\n",
    "    \n",
    "    print(f\"‚úÖ Descubrimiento de Estructura exitoso con Algoritmo PC.\")\n",
    "    print(f\"   Variables analizadas: {len(data.columns)}\")\n",
    "    print(f\"\\nMatriz de Adyacencia (forma: {adj_matrix.shape}):\")\n",
    "    print(adj_matrix)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR CR√çTICO en el Descubrimiento Causal PC: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    exit()\n",
    "\n",
    "# --- 4. C√ÅLCULO DE FUERZA CAUSAL (Regresi√≥n OLS) ---\n",
    "print(\"\\n--- 4. C√ÅLCULO DE FUERZA CAUSAL (Regresi√≥n OLS) ---\")\n",
    "causal_strengths = {}\n",
    "\n",
    "# Iterar sobre cada variable como TARGET\n",
    "for target_name in data_scaled.columns:\n",
    "    target_index = data_scaled.columns.get_loc(target_name)\n",
    "    \n",
    "    # Identificar las variables PARENTES (causas) con flechas dirigidas al TARGET\n",
    "    parents_indices = [i for i, col in enumerate(data_scaled.columns) \n",
    "                        if adj_matrix[i, target_index] == 1]\n",
    "    parent_names = [data_scaled.columns[i] for i in parents_indices]\n",
    "    \n",
    "    if parent_names:\n",
    "        # Construir el modelo de regresi√≥n\n",
    "        Y = data_scaled[target_name]\n",
    "        X = data_scaled[parent_names]\n",
    "        X = add_constant(X)\n",
    "        \n",
    "        try:\n",
    "            model = OLS(Y, X).fit()\n",
    "            print(f\"\\nüìä Modelo para la variable DEPENDIENTE: {target_name}\")\n",
    "            print(f\"   R-squared: {model.rsquared:.4f}\")\n",
    "            \n",
    "            # Recorrer los resultados para obtener los coeficientes\n",
    "            for parent in parent_names:\n",
    "                coefficient = model.params.get(parent, 0.0)\n",
    "                p_value = model.pvalues.get(parent, 1.0)\n",
    "                \n",
    "                # Usar el coeficiente solo si es estad√≠sticamente significativo\n",
    "                if p_value < 0.05: \n",
    "                    causal_strengths[(parent, target_name)] = coefficient\n",
    "                    print(f\"  ‚úì {parent} ‚Üí {target_name} | CS: {coefficient:.4f} (p={p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  ‚úó {parent} ‚Üí {target_name} | CS: {coefficient:.4f} (p={p_value:.4f}) [NO SIG]\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error en regresi√≥n para {target_name}: {e}\")\n",
    "\n",
    "if not causal_strengths:\n",
    "    print(\"\\n‚ö†Ô∏è  ADVERTENCIA: No se encontraron relaciones causales significativas.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Total de relaciones causales significativas encontradas: {len(causal_strengths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306f001",
   "metadata": {},
   "source": [
    "Visualizaci√≥n del DAG con Fuerza Causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e468b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 5. Visualizaci√≥n del Grafo Causal ---\")\n",
    "plot_pc_dag(\n",
    "    adj_matrix, \n",
    "    data_scaled.columns.tolist(), \n",
    "    causal_strengths=causal_strengths,\n",
    "    title=f'Grafo Causal PC + Fuerza OLS (Alpha=0.05)'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvDAGV8 (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
